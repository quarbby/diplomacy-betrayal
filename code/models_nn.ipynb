{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Embedding, Flatten, Conv1D, MaxPooling1D, Attention, Permute, Reshape, Lambda, RepeatVector, multiply, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.00001)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 220\n",
    "\n",
    "######################################\n",
    "## Model Options ##\n",
    "######################################\n",
    "# options: lstm, cnn, lstm-attn, bilstm\n",
    "MODEL_NAME = 'bilstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    Inp = Input(name='inputs', shape=[max_len])\n",
    "    x = Embedding(max_words, 50, input_length=max_len)(Inp)\n",
    "    x = LSTM(64, name='LSTM_01')(x)\n",
    "    x = Dropout(0.5, name='Dropout')(x)\n",
    "    x = Dense(128, activation='relu',name='Dense_01')(x)\n",
    "    # x = Dropout(0.5,name='Dropout')(x)\n",
    "    out = Dense(1,activation='sigmoid', name='output')(x)\n",
    "    model = Model(inputs=Inp, outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    model_CNN = Sequential(name=\"CNN_with_embeddings\")\n",
    "    model_CNN.add(Embedding(max_words, 50, input_length=max_len))\n",
    "    model_CNN.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model_CNN.add(MaxPooling1D(pool_size=2))\n",
    "    model_CNN.add(Flatten())\n",
    "    model_CNN.add(Dropout(0.5))\n",
    "    model_CNN.add(Dense(10, activation='relu'))\n",
    "    model_CNN.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bilstm():\n",
    "    Inp = Input(name='inputs',shape=[max_len])\n",
    "    x = Embedding(max_words,50,input_length=max_len)(Inp)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dropout(0.5,name='Dropout')(x)\n",
    "    x = Dense(128,activation='relu',name='Dense_01')(x)\n",
    "    out = Dense(1,activation='sigmoid', name='output')(x)\n",
    "    model = Model(inputs=Inp,outputs=out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_ATTENTION_VECTOR = False\n",
    "APPLY_ATTENTION_BEFORE_LSTM = False\n",
    "INPUT_DIM = 50\n",
    "TIME_STEPS = max_len\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) \n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "def create_lstm_attn():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    lstm_units = 64\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(layer)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model([inputs], output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model():\n",
    "    global MODEL_NAME\n",
    "    MODEL_NAME = MODEL_NAME.lower()\n",
    "    if MODEL_NAME == 'lstm':\n",
    "        model = create_lstm()\n",
    "        return model\n",
    "    \n",
    "    elif MODEL_NAME == 'cnn':\n",
    "        model = create_cnn()\n",
    "        return model\n",
    "    \n",
    "    elif MODEL_NAME == 'lstm-attn':\n",
    "        model = create_lstm_attn()\n",
    "        return model\n",
    "    \n",
    "    elif MODEL_NAME == 'bilstm':\n",
    "        model = create_bilstm()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_nn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_joint_model(df):\n",
    "    inputB = Input(shape=(df.shape[1],))\n",
    "    c = Dense(2, activation='relu')(inputB)\n",
    "    c = Dense(4, activation='relu')(c)\n",
    "    c = Dense(1, activation='sigmoid')(c)\n",
    "    full_model = Model(inputs=inputB, outputs=c)\n",
    "\n",
    "    full_model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "                          metrics=['acc', f1_m, precision_m, recall_m])\n",
    "    \n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
