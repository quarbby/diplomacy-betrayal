{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from metadata_options.ipynb\n",
      "importing Jupyter notebook from models_nn.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Input, InputLayer, Dropout, Dense, Flatten, Embedding, Add, Concatenate\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "## Own code \n",
    "import import_ipynb\n",
    "import metadata_options\n",
    "import models_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_text(dataframe, input_text_col_name, num_words_col_name, size_of_output, seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    min_num_words = dataframe[num_words_col_name].min()\n",
    "    max_num_words = dataframe[num_words_col_name].max()\n",
    "    \n",
    "    def choose_random_word(dataframe, input_text_col_name, is_start, is_end):\n",
    "        random_index = np.random.randint(0, dataframe.shape[0] - 1)\n",
    "        df_random_sentence = dataframe[input_text_col_name].iloc[random_index]\n",
    "        while df_random_sentence == \"#NAME?\":\n",
    "            random_index = np.random.randint(0, dataframe.shape[0] - 1)\n",
    "            df_random_sentence = dataframe[input_text_col_name].iloc[random_index]\n",
    "        if is_start:\n",
    "            return df_random_sentence.split(' ')[0]  \n",
    "        if is_end:\n",
    "            return df_random_sentence.split(' ')[-1]\n",
    "        index_words = df_random_sentence.split(' ')[1:-1]\n",
    "        return np.random.choice(index_words)\n",
    "    \n",
    "    store_sentences = []\n",
    "    for i in range(size_of_output):\n",
    "        sentence_num_words = np.random.randint(min_num_words, max_num_words)\n",
    "        sentence = choose_random_word(dataframe, input_text_col_name, True, False) # Choose first word\n",
    "        for j in range(sentence_num_words - 1 - 1): # -1 for first word already chosen, -1 for ending word\n",
    "            sentence += ' ' + choose_random_word(dataframe, input_text_col_name, False, False)\n",
    "        sentence += ' ' + choose_random_word(dataframe, input_text_col_name, False, True) #Choose last word\n",
    "        store_sentences.append(sentence)\n",
    "    \n",
    "    return pd.DataFrame({input_text_col_name: store_sentences})\n",
    "\n",
    "def create_stratified_label_df_column(original_column, column_name, out_df_length):\n",
    "    df = original_column.copy()\n",
    "    return df.groupby(column_name, group_keys=False).apply(lambda x: x.sample(int(np.rint(out_df_length*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def create_metadata_df_column(original_column, meta_name, out_df_length):\n",
    "    df = original_column.copy()\n",
    "    df = df.squeeze()\n",
    "    test = pd.DataFrame(np.random.choice(df,size=(out_df_length, 1)), columns=[meta_name])\n",
    "    return test\n",
    "    \n",
    "def create_random_df(dataframe, input_text_col_name, num_words_col_name, size_of_output, seed, label_columns_list, meta_data_columns_list):\n",
    "    # Start with input text column\n",
    "    random_df = create_random_text(dataframe, input_text_col_name, num_words_col_name, size_of_output, seed)\n",
    "    \n",
    "    # Add the columns of label columns\n",
    "    for label in label_columns_list:\n",
    "        label_column = create_stratified_label_df_column(dataframe[label].to_frame(), label, size_of_output)\n",
    "        random_df = random_df.join(label_column)\n",
    "    \n",
    "    # Add the columns of metadata columns\n",
    "    for meta in meta_data_columns_list:\n",
    "        meta_column = create_metadata_df_column(dataframe[meta].to_frame(), meta, size_of_output)\n",
    "        random_df = random_df.join(meta_column)\n",
    "    return random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with Throughput & WorkTime\n",
    "df = pd.read_csv('./data/kokil dec 6 reprepare/conf_pc_worker_sem.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set if want use random_df or not\n",
    "is_random = True\n",
    "random_df_size = df.shape[0]\n",
    "seed = 160121\n",
    "columns_list = ['Input.deception_quadrant', 'Answer.3rapport.yes_label',\n",
    "                'Answer.4shareinformation.yes_label', 'Answer.2reasoning.yes_label',\n",
    "                'Answer.1gamemove.yes_label', ]\n",
    "meta_list = ['Throughput.1', 'Throughput.2', 'Throughput.3', 'Throughput.4', 'Throughput.5',\n",
    "             'WorkTime.1', 'WorkTime.2', 'WorkTime.3', 'WorkTime.4', 'WorkTime.5',\n",
    "             'Answer.1gamemove.yes_pc_agree', 'Answer.2reasoning.yes_pc_agree', 'Answer.4shareinformation.yes_pc_agree', 'Answer.3rapport.yes_pc_agree',\n",
    "             'Input.num_words', 'Input.num_characters']\n",
    "\n",
    "if is_random:\n",
    "    random_df = create_random_df(df, 'Input.full_text', 'Input.num_words', random_df_size, seed, columns_list, meta_list)\n",
    "    drop_list = ['Input.full_text', 'Input.deception_quadrant', 'Answer.3rapport.yes_label', 'Answer.4shareinformation.yes_label', 'Answer.2reasoning.yes_label', 'Answer.1gamemove.yes_label']\n",
    "    random_df = random_df.drop(drop_list, axis=1)\n",
    "    list_to_replace = list(random_df)\n",
    "    df.drop(labels=list_to_replace, axis=\"columns\", inplace=True)\n",
    "    df[list_to_replace] = random_df[list_to_replace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP2: weighted by 1 linear variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "Plot below: old throughput (x-axis) vs new throughput (y-axis)\n",
      "WT1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "PC1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "TL2: weighted by 1 normalised number of words per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "SP1: weighted by average of TP1 and TP2 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######################################\n",
    "## Weighted Onehot Encoding options ##\n",
    "######################################\n",
    "\n",
    "##############\n",
    "# Throughput #\n",
    "##############\n",
    "# TP1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# TP2: weighted by 1 linear variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# TP3 + k: weighted by 1 inverted k-power U-shaped variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# TP4 + k: weighted by 1 upright k-power U-shaped variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# (For TP3 & TP4, k=1 results in V-shaped variance, and as k>1 increases, sides will curve into U-shaped variance)\n",
    "\n",
    "############\n",
    "# Worktime #\n",
    "############\n",
    "# WT1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# WT2: weighted by 1 linear variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "\n",
    "################\n",
    "# PC agreement #\n",
    "################\n",
    "# PC1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# PC2: weighted by 1 linear variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# PC3: weighted by 1 PC agreement weight per annotation in each OHE, i.e. (a, b, c, d) -> (w1*a, w2*b, w3*c, w4*d)\n",
    "\n",
    "#####################\n",
    "# Input text length #\n",
    "#####################\n",
    "# TL1: weighted by 1 normalised number of characters per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# TL2: weighted by 1 normalised number of words per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "\n",
    "###################\n",
    "# Special Options #\n",
    "###################\n",
    "# SP1: weighted by average of TP1 and TP2 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# SP2: weighted by average of WT1 and WT2 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# SP3: weighted by average of PC1 and PC2 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# RAND_UNI: weighted by 1 uniformly distributed random number between 0 to 1 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# RAND_NORM: weighted by 1 normally distributed random number between 0 to 1 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "\n",
    "# Select 1 option from each of the few variants above, e.g. TP2, WT1, PC3, TL1, SP3, and input into function\n",
    "# set_OHE_pipeline_options. If not selecting TP3 or TP4, input k (option_k) will be ignored. After\n",
    "# editing the options, run the entire notebook for results accordingly.\n",
    "\n",
    "# Edit option choices here\n",
    "throughput_option = 'TP2'\n",
    "worktime_option = 'WT1'\n",
    "pc_agreement_option = 'PC1'\n",
    "textlength_option = 'TL2'\n",
    "special_option = 'SP1'\n",
    "k_option_for_tp = 3\n",
    "\n",
    "df_throughput, df_worktime, df_agreement, df_textlength, df_special = metadata_options.set_OHE_pipeline_options(df, throughput_option, worktime_option, pc_agreement_option, textlength_option, special_option, k_option_for_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "## Model Options ##\n",
    "######################################\n",
    "# options: lstm, cnn, lstm-attn\n",
    "\n",
    "model_name = 'cnn'\n",
    "models_nn.MODEL_NAME = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old train_test_split code\n",
    "# train, test, indices_train, indices_test = train_test_split(df, indices, test_size=0.2)\n",
    "\n",
    "# New train_test_split using Stratified Shaffled Splits\n",
    "y = df[\"Input.deception_quadrant\"].copy()\n",
    "X = df.drop([\"Input.deception_quadrant\"], axis=1)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "splits_generator = sss.split(X, y)\n",
    "\n",
    "for train_idx, test_idx in splits_generator:\n",
    "    indices_train = train_idx\n",
    "    indices_test = test_idx\n",
    "\n",
    "train = df.take(indices_train)\n",
    "test = df.take(indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11366, 863) (9092, 863) (2274, 863)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape, train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 0.23.2\n"
     ]
    }
   ],
   "source": [
    "# check scikit-learn version\n",
    "import sklearn\n",
    "print('sklearn: %s' % sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiyuan/anaconda3/envs/TF2/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1], y=[1 1 1 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/kaiyuan/anaconda3/envs/TF2/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0. 1.], y=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/kaiyuan/anaconda3/envs/TF2/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0. 1.], y=[1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/kaiyuan/anaconda3/envs/TF2/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0. 1.], y=[0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/kaiyuan/anaconda3/envs/TF2/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0. 1.], y=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "new_deception_train = train[\"Input.deception_quadrant\"].copy()\n",
    "new_deception_train['Input.deception_quadrant'] = train[\"Input.deception_quadrant\"].apply(lambda x : 1 if x == \"Straightforward\" else 0)\n",
    "y_train_deception = new_deception_train['Input.deception_quadrant'].to_numpy()\n",
    "deception_class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_deception),\n",
    "                                                 y_train_deception)\n",
    "deception_class_weight_dict = dict(enumerate(deception_class_weights))\n",
    "\n",
    "y_train_rapport = train['Answer.3rapport.yes_label'].tolist()\n",
    "rapport_class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_rapport),\n",
    "                                                 y_train_rapport)\n",
    "rapport_class_weight_dict = dict(enumerate(rapport_class_weights))\n",
    "\n",
    "y_train_share_information = train['Answer.4shareinformation.yes_label'].tolist()\n",
    "share_info_class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_share_information),\n",
    "                                                 y_train_share_information)\n",
    "share_info_class_weight_dict = dict(enumerate(share_info_class_weights))\n",
    "\n",
    "y_train_reasoning = train['Answer.2reasoning.yes_label'].tolist()\n",
    "reasoning_class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_reasoning),\n",
    "                                                 y_train_reasoning)\n",
    "reasoning_class_weight_dict = dict(enumerate(reasoning_class_weights))\n",
    "\n",
    "y_train_gamemove = train['Answer.1gamemove.yes_label'].tolist()\n",
    "gamemove_class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_gamemove),\n",
    "                                                 y_train_gamemove)\n",
    "gamemove_class_weight_dict = dict(enumerate(gamemove_class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights\n",
      "Deception: {0: 10.192825112107624, 1: 0.525792273883877} \n",
      "Rapport: {0: 3.582348305752561, 1: 0.5811069922024799} \n",
      "Share Information: {0: 3.117969821673525, 1: 0.59549384333246} \n",
      "Reasoning: {0: 2.9596354166666665, 1: 0.6016410799364743} \n",
      "Gamemove: {0: 7.048062015503876, 1: 0.5381792352314431}\n"
     ]
    }
   ],
   "source": [
    "print(\"Class weights\")\n",
    "print(\"Deception: {} \\nRapport: {} \\nShare Information: {} \\nReasoning: {} \\nGamemove: {}\".format(deception_class_weight_dict,\n",
    "                                                                                                  rapport_class_weight_dict,\n",
    "                                                                                                  share_info_class_weight_dict,\n",
    "                                                                                                  reasoning_class_weight_dict,\n",
    "                                                                                                  gamemove_class_weight_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_col = train['Input.full_text']\n",
    "\n",
    "new_deception_test = test[\"Input.deception_quadrant\"].copy()\n",
    "new_deception_test['Input.deception_quadrant'] = test[\"Input.deception_quadrant\"].apply(lambda x : 1 if x == \"Straightforward\" else 0)\n",
    "y_test_deception = new_deception_test['Input.deception_quadrant'].tolist()\n",
    "y_test_rapport = test['Answer.3rapport.yes_label'].tolist()\n",
    "y_test_share_information = test['Answer.4shareinformation.yes_label'].tolist()\n",
    "y_test_reasoning = test['Answer.2reasoning.yes_label'].tolist()\n",
    "y_test_gamemove = test['Answer.1gamemove.yes_label'].tolist()\n",
    "\n",
    "X_test_col = test['Input.full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiyuan/anaconda3/envs/TF2/lib/python3.6/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "# def label_preprocessing(y_data, label_encoder):\n",
    "#     out = label_encoder.fit_transform(y_data).reshape(-1,1)\n",
    "#     return out\n",
    "\n",
    "y_train_deception = le.fit_transform(y_train_deception)\n",
    "y_train_deception = y_train_deception.reshape(-1,1)\n",
    "\n",
    "y_train_rapport = le.fit_transform(y_train_rapport)\n",
    "y_train_rapport = y_train_rapport.reshape(-1,1)\n",
    "\n",
    "y_train_share_information = le.fit_transform(y_train_share_information)\n",
    "y_train_share_information = y_train_share_information.reshape(-1,1)\n",
    "\n",
    "y_train_reasoning = le.fit_transform(y_train_reasoning)\n",
    "y_train_reasoning = y_train_reasoning.reshape(-1,1)\n",
    "\n",
    "y_train_gamemove = le.fit_transform(y_train_gamemove)\n",
    "y_train_gamemove = y_train_gamemove.reshape(-1,1)\n",
    "\n",
    "y_train_deception = le.fit_transform(y_train_deception)\n",
    "y_train_deception = y_train_deception.reshape(-1,1)\n",
    "\n",
    "y_test_rapport = le.fit_transform(y_test_rapport)\n",
    "y_test_rapport = y_test_rapport.reshape(-1,1)\n",
    "\n",
    "y_test_share_information = le.fit_transform(y_test_share_information)\n",
    "y_test_share_information = y_test_share_information.reshape(-1,1)\n",
    "\n",
    "y_test_reasoning = le.fit_transform(y_test_reasoning)\n",
    "y_test_reasoning = y_test_reasoning.reshape(-1,1)\n",
    "\n",
    "y_test_gamemove = le.fit_transform(y_test_gamemove)\n",
    "y_test_gamemove = y_test_gamemove.reshape(-1,1)\n",
    "\n",
    "y_test_deception = le.fit_transform(y_test_deception)\n",
    "y_test_deception = y_test_deception.reshape(-1,1)\n",
    "\n",
    "y_test_rapport = np.asarray(y_test_rapport)\n",
    "y_train_rapport = np.asarray(y_train_rapport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 220\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "\n",
    "tok.fit_on_texts(X_train_col)\n",
    "X_train_sequences = tok.texts_to_sequences(X_train_col)\n",
    "X_train = pad_sequences(X_train_sequences, maxlen=max_len)\n",
    "\n",
    "X_test_sequences = tok.texts_to_sequences(X_test_col)\n",
    "X_test = pad_sequences(X_test_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct individual  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.6949 - accuracy: 0.5606 - f1_m: 0.6409 - recall_m: 0.5825 - precision_m: 0.8375 - val_loss: 0.6774 - val_accuracy: 0.7792 - val_f1_m: 0.8726 - val_recall_m: 0.8749 - val_precision_m: 0.8712\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.6887 - accuracy: 0.5307 - f1_m: 0.6068 - recall_m: 0.5223 - precision_m: 0.8890 - val_loss: 0.7030 - val_accuracy: 0.4204 - val_f1_m: 0.5382 - val_recall_m: 0.3898 - val_precision_m: 0.8761\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6742 - accuracy: 0.5719 - f1_m: 0.6710 - recall_m: 0.5672 - precision_m: 0.9077 - val_loss: 0.6039 - val_accuracy: 0.7498 - val_f1_m: 0.8526 - val_recall_m: 0.8366 - val_precision_m: 0.8702\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6407 - accuracy: 0.6389 - f1_m: 0.7484 - recall_m: 0.6408 - precision_m: 0.9252 - val_loss: 0.7248 - val_accuracy: 0.4393 - val_f1_m: 0.5608 - val_recall_m: 0.4138 - val_precision_m: 0.8785\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.5951 - accuracy: 0.6715 - f1_m: 0.7653 - recall_m: 0.6548 - precision_m: 0.9443 - val_loss: 0.5257 - val_accuracy: 0.7766 - val_f1_m: 0.8711 - val_recall_m: 0.8707 - val_precision_m: 0.8723\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.5385 - accuracy: 0.7249 - f1_m: 0.8172 - recall_m: 0.7239 - precision_m: 0.9504 - val_loss: 0.5362 - val_accuracy: 0.7410 - val_f1_m: 0.8457 - val_recall_m: 0.8189 - val_precision_m: 0.8755\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.4886 - accuracy: 0.7560 - f1_m: 0.8397 - recall_m: 0.7523 - precision_m: 0.9564 - val_loss: 0.5361 - val_accuracy: 0.7401 - val_f1_m: 0.8450 - val_recall_m: 0.8168 - val_precision_m: 0.8760\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.4346 - accuracy: 0.7855 - f1_m: 0.8602 - recall_m: 0.7772 - precision_m: 0.9681 - val_loss: 0.5211 - val_accuracy: 0.7665 - val_f1_m: 0.8640 - val_recall_m: 0.8549 - val_precision_m: 0.8740\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.3858 - accuracy: 0.8163 - f1_m: 0.8796 - recall_m: 0.8057 - precision_m: 0.9729 - val_loss: 0.5236 - val_accuracy: 0.7889 - val_f1_m: 0.8792 - val_recall_m: 0.8852 - val_precision_m: 0.8739\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.3451 - accuracy: 0.8364 - f1_m: 0.8978 - recall_m: 0.8341 - precision_m: 0.9751 - val_loss: 0.7063 - val_accuracy: 0.6332 - val_f1_m: 0.7617 - val_recall_m: 0.6757 - val_precision_m: 0.8745\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.3044 - accuracy: 0.8570 - f1_m: 0.9070 - recall_m: 0.8448 - precision_m: 0.9822 - val_loss: 0.6573 - val_accuracy: 0.7018 - val_f1_m: 0.8172 - val_recall_m: 0.7682 - val_precision_m: 0.8740\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.2782 - accuracy: 0.8705 - f1_m: 0.9161 - recall_m: 0.8604 - precision_m: 0.9827 - val_loss: 0.6534 - val_accuracy: 0.7203 - val_f1_m: 0.8312 - val_recall_m: 0.7938 - val_precision_m: 0.8736\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.2500 - accuracy: 0.8876 - f1_m: 0.9270 - recall_m: 0.8777 - precision_m: 0.9852 - val_loss: 0.7896 - val_accuracy: 0.6491 - val_f1_m: 0.7753 - val_recall_m: 0.6995 - val_precision_m: 0.8709\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.2294 - accuracy: 0.8939 - f1_m: 0.9355 - recall_m: 0.8902 - precision_m: 0.9870 - val_loss: 0.8086 - val_accuracy: 0.6671 - val_f1_m: 0.7904 - val_recall_m: 0.7228 - val_precision_m: 0.8731\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.2051 - accuracy: 0.9089 - f1_m: 0.9407 - recall_m: 0.8994 - precision_m: 0.9886 - val_loss: 0.7634 - val_accuracy: 0.7335 - val_f1_m: 0.8406 - val_recall_m: 0.8106 - val_precision_m: 0.8738\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.1917 - accuracy: 0.9156 - f1_m: 0.9494 - recall_m: 0.9132 - precision_m: 0.9895 - val_loss: 0.8232 - val_accuracy: 0.7155 - val_f1_m: 0.8270 - val_recall_m: 0.7846 - val_precision_m: 0.8752\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1686 - accuracy: 0.9259 - f1_m: 0.9538 - recall_m: 0.9190 - precision_m: 0.9923 - val_loss: 0.7866 - val_accuracy: 0.7586 - val_f1_m: 0.8586 - val_recall_m: 0.8467 - val_precision_m: 0.8716\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1544 - accuracy: 0.9333 - f1_m: 0.9603 - recall_m: 0.9307 - precision_m: 0.9927 - val_loss: 0.9204 - val_accuracy: 0.6983 - val_f1_m: 0.8139 - val_recall_m: 0.7618 - val_precision_m: 0.8750\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1449 - accuracy: 0.9352 - f1_m: 0.9615 - recall_m: 0.9323 - precision_m: 0.9930 - val_loss: 0.8938 - val_accuracy: 0.7331 - val_f1_m: 0.8400 - val_recall_m: 0.8096 - val_precision_m: 0.8737\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.1381 - accuracy: 0.9402 - f1_m: 0.9644 - recall_m: 0.9384 - precision_m: 0.9926 - val_loss: 0.9084 - val_accuracy: 0.7555 - val_f1_m: 0.8560 - val_recall_m: 0.8404 - val_precision_m: 0.8730\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1248 - accuracy: 0.9487 - f1_m: 0.9677 - recall_m: 0.9434 - precision_m: 0.9942 - val_loss: 0.9756 - val_accuracy: 0.7520 - val_f1_m: 0.8543 - val_recall_m: 0.8396 - val_precision_m: 0.8705\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1132 - accuracy: 0.9562 - f1_m: 0.9742 - recall_m: 0.9540 - precision_m: 0.9956 - val_loss: 1.1200 - val_accuracy: 0.6816 - val_f1_m: 0.8012 - val_recall_m: 0.7406 - val_precision_m: 0.8739\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1098 - accuracy: 0.9518 - f1_m: 0.9696 - recall_m: 0.9461 - precision_m: 0.9952 - val_loss: 1.0265 - val_accuracy: 0.7524 - val_f1_m: 0.8539 - val_recall_m: 0.8365 - val_precision_m: 0.8730\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1025 - accuracy: 0.9566 - f1_m: 0.9744 - recall_m: 0.9546 - precision_m: 0.9954 - val_loss: 1.1674 - val_accuracy: 0.6944 - val_f1_m: 0.8106 - val_recall_m: 0.7554 - val_precision_m: 0.8756\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0926 - accuracy: 0.9606 - f1_m: 0.9769 - recall_m: 0.9590 - precision_m: 0.9959 - val_loss: 1.1643 - val_accuracy: 0.7282 - val_f1_m: 0.8371 - val_recall_m: 0.8055 - val_precision_m: 0.8722\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0907 - accuracy: 0.9610 - f1_m: 0.9771 - recall_m: 0.9597 - precision_m: 0.9954 - val_loss: 1.1289 - val_accuracy: 0.7647 - val_f1_m: 0.8621 - val_recall_m: 0.8517 - val_precision_m: 0.8735\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0876 - accuracy: 0.9648 - f1_m: 0.9774 - recall_m: 0.9638 - precision_m: 0.9924 - val_loss: 1.3324 - val_accuracy: 0.6750 - val_f1_m: 0.7957 - val_recall_m: 0.7317 - val_precision_m: 0.8735\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0732 - accuracy: 0.9689 - f1_m: 0.9819 - recall_m: 0.9675 - precision_m: 0.9969 - val_loss: 1.2895 - val_accuracy: 0.7353 - val_f1_m: 0.8415 - val_recall_m: 0.8110 - val_precision_m: 0.8755\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0755 - accuracy: 0.9711 - f1_m: 0.9804 - recall_m: 0.9655 - precision_m: 0.9967 - val_loss: 1.2037 - val_accuracy: 0.7700 - val_f1_m: 0.8663 - val_recall_m: 0.8628 - val_precision_m: 0.8707\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0675 - accuracy: 0.9737 - f1_m: 0.9819 - recall_m: 0.9681 - precision_m: 0.9972 - val_loss: 1.2700 - val_accuracy: 0.7621 - val_f1_m: 0.8608 - val_recall_m: 0.8507 - val_precision_m: 0.8721\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0654 - accuracy: 0.9739 - f1_m: 0.9848 - recall_m: 0.9729 - precision_m: 0.9972 - val_loss: 1.3505 - val_accuracy: 0.7410 - val_f1_m: 0.8460 - val_recall_m: 0.8216 - val_precision_m: 0.8730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0594 - accuracy: 0.9747 - f1_m: 0.9853 - recall_m: 0.9744 - precision_m: 0.9966 - val_loss: 1.4267 - val_accuracy: 0.7573 - val_f1_m: 0.8574 - val_recall_m: 0.8440 - val_precision_m: 0.8721\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0607 - accuracy: 0.9759 - f1_m: 0.9859 - recall_m: 0.9754 - precision_m: 0.9968 - val_loss: 1.4444 - val_accuracy: 0.7296 - val_f1_m: 0.8371 - val_recall_m: 0.8036 - val_precision_m: 0.8746\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0567 - accuracy: 0.9776 - f1_m: 0.9869 - recall_m: 0.9761 - precision_m: 0.9981 - val_loss: 1.4736 - val_accuracy: 0.7634 - val_f1_m: 0.8614 - val_recall_m: 0.8508 - val_precision_m: 0.8730\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0525 - accuracy: 0.9781 - f1_m: 0.9872 - recall_m: 0.9770 - precision_m: 0.9979 - val_loss: 1.5411 - val_accuracy: 0.7502 - val_f1_m: 0.8522 - val_recall_m: 0.8331 - val_precision_m: 0.8731\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0518 - accuracy: 0.9781 - f1_m: 0.9872 - recall_m: 0.9772 - precision_m: 0.9975 - val_loss: 1.5721 - val_accuracy: 0.7559 - val_f1_m: 0.8564 - val_recall_m: 0.8412 - val_precision_m: 0.8730\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0560 - accuracy: 0.9783 - f1_m: 0.9874 - recall_m: 0.9776 - precision_m: 0.9974 - val_loss: 1.5683 - val_accuracy: 0.7480 - val_f1_m: 0.8504 - val_recall_m: 0.8284 - val_precision_m: 0.8747\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0470 - accuracy: 0.9804 - f1_m: 0.9886 - recall_m: 0.9800 - precision_m: 0.9974 - val_loss: 1.6488 - val_accuracy: 0.7419 - val_f1_m: 0.8460 - val_recall_m: 0.8200 - val_precision_m: 0.8745\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0492 - accuracy: 0.9821 - f1_m: 0.9895 - recall_m: 0.9820 - precision_m: 0.9973 - val_loss: 1.7102 - val_accuracy: 0.7199 - val_f1_m: 0.8302 - val_recall_m: 0.7923 - val_precision_m: 0.8734\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0452 - accuracy: 0.9814 - f1_m: 0.9891 - recall_m: 0.9805 - precision_m: 0.9981 - val_loss: 1.6939 - val_accuracy: 0.7480 - val_f1_m: 0.8510 - val_recall_m: 0.8328 - val_precision_m: 0.8708\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0399 - accuracy: 0.9852 - f1_m: 0.9913 - recall_m: 0.9846 - precision_m: 0.9982 - val_loss: 1.8141 - val_accuracy: 0.7335 - val_f1_m: 0.8405 - val_recall_m: 0.8116 - val_precision_m: 0.8725\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0450 - accuracy: 0.9823 - f1_m: 0.9898 - recall_m: 0.9823 - precision_m: 0.9975 - val_loss: 1.7941 - val_accuracy: 0.7274 - val_f1_m: 0.8360 - val_recall_m: 0.8022 - val_precision_m: 0.8738\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0460 - accuracy: 0.9823 - f1_m: 0.9897 - recall_m: 0.9819 - precision_m: 0.9977 - val_loss: 1.7892 - val_accuracy: 0.7625 - val_f1_m: 0.8609 - val_recall_m: 0.8512 - val_precision_m: 0.8715\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0455 - accuracy: 0.9825 - f1_m: 0.9899 - recall_m: 0.9823 - precision_m: 0.9976 - val_loss: 1.8064 - val_accuracy: 0.7533 - val_f1_m: 0.8545 - val_recall_m: 0.8393 - val_precision_m: 0.8713\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0427 - accuracy: 0.9837 - f1_m: 0.9906 - recall_m: 0.9829 - precision_m: 0.9985 - val_loss: 1.8759 - val_accuracy: 0.7445 - val_f1_m: 0.8483 - val_recall_m: 0.8268 - val_precision_m: 0.8720\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0420 - accuracy: 0.9856 - f1_m: 0.9890 - recall_m: 0.9859 - precision_m: 0.9931 - val_loss: 1.9123 - val_accuracy: 0.6939 - val_f1_m: 0.8114 - val_recall_m: 0.7601 - val_precision_m: 0.8712\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0409 - accuracy: 0.9831 - f1_m: 0.9902 - recall_m: 0.9824 - precision_m: 0.9982 - val_loss: 1.8260 - val_accuracy: 0.7476 - val_f1_m: 0.8507 - val_recall_m: 0.8312 - val_precision_m: 0.8722\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0430 - accuracy: 0.9859 - f1_m: 0.9918 - recall_m: 0.9859 - precision_m: 0.9979 - val_loss: 1.8508 - val_accuracy: 0.7682 - val_f1_m: 0.8649 - val_recall_m: 0.8576 - val_precision_m: 0.8729\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0323 - accuracy: 0.9883 - f1_m: 0.9933 - recall_m: 0.9885 - precision_m: 0.9981 - val_loss: 2.0262 - val_accuracy: 0.7542 - val_f1_m: 0.8554 - val_recall_m: 0.8398 - val_precision_m: 0.8725\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0429 - accuracy: 0.9828 - f1_m: 0.9901 - recall_m: 0.9825 - precision_m: 0.9978 - val_loss: 1.9469 - val_accuracy: 0.7410 - val_f1_m: 0.8454 - val_recall_m: 0.8199 - val_precision_m: 0.8734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f546ef15630>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rapport model\n",
    "rapport_model = models_nn.create_nn_model()\n",
    "rapport_model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', models_nn.f1_m, \n",
    "                                                                              models_nn.recall_m, models_nn.precision_m])\n",
    "rapport_model.fit(X_train,y_train_rapport,\n",
    "                  batch_size=128,\n",
    "                  epochs=50,\n",
    "                  validation_data=(X_test, y_test_rapport), \n",
    "#                   callbacks=[models_nn.early_stop],\n",
    "                  class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5083528128408914, 0.5110813823473166, 0.5067784607895088, None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rapport_pred = rapport_model.predict(X_train)\n",
    "rapport_pred_test = rapport_model.predict(X_test)\n",
    "\n",
    "rapport_pred_test_round = rapport_pred_test.round()\n",
    "precision_recall_fscore_support(y_test_rapport, rapport_pred_test_round, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.6939 - accuracy: 0.7471 - f1_m: 0.8159 - recall_m: 0.7917 - precision_m: 0.8771 - val_loss: 0.6774 - val_accuracy: 0.6253 - val_f1_m: 0.7597 - val_recall_m: 0.6348 - val_precision_m: 0.9483\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6894 - accuracy: 0.6537 - f1_m: 0.7655 - recall_m: 0.6676 - precision_m: 0.9394 - val_loss: 0.6581 - val_accuracy: 0.5862 - val_f1_m: 0.7265 - val_recall_m: 0.5887 - val_precision_m: 0.9511\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6760 - accuracy: 0.5974 - f1_m: 0.7097 - recall_m: 0.5971 - precision_m: 0.9480 - val_loss: 0.7300 - val_accuracy: 0.3443 - val_f1_m: 0.4733 - val_recall_m: 0.3168 - val_precision_m: 0.9532\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6493 - accuracy: 0.5488 - f1_m: 0.6835 - recall_m: 0.5388 - precision_m: 0.9644 - val_loss: 0.5162 - val_accuracy: 0.8056 - val_f1_m: 0.8905 - val_recall_m: 0.8454 - val_precision_m: 0.9412\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6082 - accuracy: 0.6494 - f1_m: 0.7691 - recall_m: 0.6447 - precision_m: 0.9711 - val_loss: 0.5471 - val_accuracy: 0.7256 - val_f1_m: 0.8365 - val_recall_m: 0.7503 - val_precision_m: 0.9459\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.5497 - accuracy: 0.7046 - f1_m: 0.8123 - recall_m: 0.7000 - precision_m: 0.9783 - val_loss: 0.4562 - val_accuracy: 0.8118 - val_f1_m: 0.8945 - val_recall_m: 0.8528 - val_precision_m: 0.9411\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.4875 - accuracy: 0.7500 - f1_m: 0.8448 - recall_m: 0.7457 - precision_m: 0.9805 - val_loss: 0.5594 - val_accuracy: 0.6926 - val_f1_m: 0.8130 - val_recall_m: 0.7132 - val_precision_m: 0.9471\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4190 - accuracy: 0.7844 - f1_m: 0.8705 - recall_m: 0.7816 - precision_m: 0.9873 - val_loss: 0.3937 - val_accuracy: 0.8360 - val_f1_m: 0.9098 - val_recall_m: 0.8824 - val_precision_m: 0.9398\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.3705 - accuracy: 0.8141 - f1_m: 0.8822 - recall_m: 0.8035 - precision_m: 0.9870 - val_loss: 0.4137 - val_accuracy: 0.8197 - val_f1_m: 0.8998 - val_recall_m: 0.8637 - val_precision_m: 0.9400\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.3280 - accuracy: 0.8510 - f1_m: 0.9122 - recall_m: 0.8474 - precision_m: 0.9896 - val_loss: 0.3949 - val_accuracy: 0.8404 - val_f1_m: 0.9126 - val_recall_m: 0.8881 - val_precision_m: 0.9393\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.2797 - accuracy: 0.8665 - f1_m: 0.9211 - recall_m: 0.8610 - precision_m: 0.9918 - val_loss: 0.5095 - val_accuracy: 0.7568 - val_f1_m: 0.8596 - val_recall_m: 0.7946 - val_precision_m: 0.9373\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.2426 - accuracy: 0.8908 - f1_m: 0.9384 - recall_m: 0.8900 - precision_m: 0.9934 - val_loss: 0.6140 - val_accuracy: 0.7067 - val_f1_m: 0.8243 - val_recall_m: 0.7340 - val_precision_m: 0.9412\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.2117 - accuracy: 0.9019 - f1_m: 0.9429 - recall_m: 0.8974 - precision_m: 0.9944 - val_loss: 0.4445 - val_accuracy: 0.8245 - val_f1_m: 0.9029 - val_recall_m: 0.8700 - val_precision_m: 0.9391\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1866 - accuracy: 0.9183 - f1_m: 0.9525 - recall_m: 0.9135 - precision_m: 0.9960 - val_loss: 0.4330 - val_accuracy: 0.8575 - val_f1_m: 0.9228 - val_recall_m: 0.9080 - val_precision_m: 0.9387\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.1718 - accuracy: 0.9243 - f1_m: 0.9554 - recall_m: 0.9194 - precision_m: 0.9955 - val_loss: 0.5008 - val_accuracy: 0.8047 - val_f1_m: 0.8904 - val_recall_m: 0.8475 - val_precision_m: 0.9386\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.1484 - accuracy: 0.9353 - f1_m: 0.9624 - recall_m: 0.9305 - precision_m: 0.9973 - val_loss: 0.4750 - val_accuracy: 0.8412 - val_f1_m: 0.9131 - val_recall_m: 0.8901 - val_precision_m: 0.9381\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.1229 - accuracy: 0.9473 - f1_m: 0.9692 - recall_m: 0.9435 - precision_m: 0.9972 - val_loss: 0.5190 - val_accuracy: 0.8470 - val_f1_m: 0.9163 - val_recall_m: 0.8950 - val_precision_m: 0.9392\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.1269 - accuracy: 0.9449 - f1_m: 0.9678 - recall_m: 0.9410 - precision_m: 0.9970 - val_loss: 0.4899 - val_accuracy: 0.8624 - val_f1_m: 0.9254 - val_recall_m: 0.9133 - val_precision_m: 0.9385\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1053 - accuracy: 0.9579 - f1_m: 0.9770 - recall_m: 0.9564 - precision_m: 0.9988 - val_loss: 0.5560 - val_accuracy: 0.8399 - val_f1_m: 0.9122 - val_recall_m: 0.8884 - val_precision_m: 0.9380\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1022 - accuracy: 0.9569 - f1_m: 0.9765 - recall_m: 0.9566 - precision_m: 0.9976 - val_loss: 0.5409 - val_accuracy: 0.8588 - val_f1_m: 0.9235 - val_recall_m: 0.9096 - val_precision_m: 0.9384\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0814 - accuracy: 0.9668 - f1_m: 0.9819 - recall_m: 0.9659 - precision_m: 0.9988 - val_loss: 0.5902 - val_accuracy: 0.8470 - val_f1_m: 0.9165 - val_recall_m: 0.8967 - val_precision_m: 0.9375\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0802 - accuracy: 0.9659 - f1_m: 0.9815 - recall_m: 0.9658 - precision_m: 0.9978 - val_loss: 0.5979 - val_accuracy: 0.8575 - val_f1_m: 0.9225 - val_recall_m: 0.9073 - val_precision_m: 0.9387\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0728 - accuracy: 0.9727 - f1_m: 0.9853 - recall_m: 0.9726 - precision_m: 0.9984 - val_loss: 0.6431 - val_accuracy: 0.8434 - val_f1_m: 0.9141 - val_recall_m: 0.8922 - val_precision_m: 0.9376\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0638 - accuracy: 0.9749 - f1_m: 0.9864 - recall_m: 0.9750 - precision_m: 0.9983 - val_loss: 0.6837 - val_accuracy: 0.8333 - val_f1_m: 0.9081 - val_recall_m: 0.8806 - val_precision_m: 0.9378\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0645 - accuracy: 0.9754 - f1_m: 0.9867 - recall_m: 0.9755 - precision_m: 0.9983 - val_loss: 0.6649 - val_accuracy: 0.8610 - val_f1_m: 0.9244 - val_recall_m: 0.9114 - val_precision_m: 0.9383\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0535 - accuracy: 0.9801 - f1_m: 0.9893 - recall_m: 0.9795 - precision_m: 0.9993 - val_loss: 0.7287 - val_accuracy: 0.8448 - val_f1_m: 0.9150 - val_recall_m: 0.8935 - val_precision_m: 0.9383\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0518 - accuracy: 0.9809 - f1_m: 0.9897 - recall_m: 0.9810 - precision_m: 0.9987 - val_loss: 0.7405 - val_accuracy: 0.8179 - val_f1_m: 0.8988 - val_recall_m: 0.8639 - val_precision_m: 0.9373\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0491 - accuracy: 0.9791 - f1_m: 0.9887 - recall_m: 0.9788 - precision_m: 0.9989 - val_loss: 0.7790 - val_accuracy: 0.8399 - val_f1_m: 0.9121 - val_recall_m: 0.8877 - val_precision_m: 0.9383\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0448 - accuracy: 0.9842 - f1_m: 0.9915 - recall_m: 0.9839 - precision_m: 0.9993 - val_loss: 0.8066 - val_accuracy: 0.8201 - val_f1_m: 0.9001 - val_recall_m: 0.8658 - val_precision_m: 0.9379\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0415 - accuracy: 0.9830 - f1_m: 0.9908 - recall_m: 0.9827 - precision_m: 0.9992 - val_loss: 0.7799 - val_accuracy: 0.8588 - val_f1_m: 0.9233 - val_recall_m: 0.9093 - val_precision_m: 0.9385\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0400 - accuracy: 0.9842 - f1_m: 0.9915 - recall_m: 0.9841 - precision_m: 0.9990 - val_loss: 0.8203 - val_accuracy: 0.8641 - val_f1_m: 0.9266 - val_recall_m: 0.9156 - val_precision_m: 0.9384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0392 - accuracy: 0.9870 - f1_m: 0.9930 - recall_m: 0.9872 - precision_m: 0.9991 - val_loss: 0.8259 - val_accuracy: 0.8637 - val_f1_m: 0.9262 - val_recall_m: 0.9144 - val_precision_m: 0.9387\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0392 - accuracy: 0.9846 - f1_m: 0.9917 - recall_m: 0.9846 - precision_m: 0.9991 - val_loss: 0.7916 - val_accuracy: 0.8791 - val_f1_m: 0.9349 - val_recall_m: 0.9315 - val_precision_m: 0.9388\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0333 - accuracy: 0.9872 - f1_m: 0.9931 - recall_m: 0.9872 - precision_m: 0.9992 - val_loss: 0.8722 - val_accuracy: 0.8443 - val_f1_m: 0.9147 - val_recall_m: 0.8924 - val_precision_m: 0.9387\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0317 - accuracy: 0.9889 - f1_m: 0.9941 - recall_m: 0.9891 - precision_m: 0.9992 - val_loss: 0.8816 - val_accuracy: 0.8536 - val_f1_m: 0.9200 - val_recall_m: 0.9025 - val_precision_m: 0.9388\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0330 - accuracy: 0.9883 - f1_m: 0.9937 - recall_m: 0.9889 - precision_m: 0.9987 - val_loss: 0.8808 - val_accuracy: 0.8628 - val_f1_m: 0.9255 - val_recall_m: 0.9120 - val_precision_m: 0.9399\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0356 - accuracy: 0.9888 - f1_m: 0.9940 - recall_m: 0.9888 - precision_m: 0.9993 - val_loss: 0.8788 - val_accuracy: 0.8703 - val_f1_m: 0.9299 - val_recall_m: 0.9209 - val_precision_m: 0.9396\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0323 - accuracy: 0.9900 - f1_m: 0.9947 - recall_m: 0.9902 - precision_m: 0.9992 - val_loss: 0.9101 - val_accuracy: 0.8694 - val_f1_m: 0.9294 - val_recall_m: 0.9208 - val_precision_m: 0.9386\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0315 - accuracy: 0.9907 - f1_m: 0.9950 - recall_m: 0.9910 - precision_m: 0.9991 - val_loss: 1.0089 - val_accuracy: 0.8206 - val_f1_m: 0.8999 - val_recall_m: 0.8633 - val_precision_m: 0.9405\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0322 - accuracy: 0.9905 - f1_m: 0.9949 - recall_m: 0.9907 - precision_m: 0.9992 - val_loss: 0.9885 - val_accuracy: 0.8426 - val_f1_m: 0.9134 - val_recall_m: 0.8891 - val_precision_m: 0.9397\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0230 - accuracy: 0.9918 - f1_m: 0.9956 - recall_m: 0.9918 - precision_m: 0.9994 - val_loss: 1.0305 - val_accuracy: 0.8412 - val_f1_m: 0.9127 - val_recall_m: 0.8878 - val_precision_m: 0.9398\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0281 - accuracy: 0.9893 - f1_m: 0.9943 - recall_m: 0.9893 - precision_m: 0.9994 - val_loss: 1.0611 - val_accuracy: 0.8514 - val_f1_m: 0.9189 - val_recall_m: 0.9000 - val_precision_m: 0.9392\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0196 - accuracy: 0.9930 - f1_m: 0.9962 - recall_m: 0.9930 - precision_m: 0.9995 - val_loss: 1.0312 - val_accuracy: 0.8602 - val_f1_m: 0.9240 - val_recall_m: 0.9095 - val_precision_m: 0.9397\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0303 - accuracy: 0.9914 - f1_m: 0.9954 - recall_m: 0.9917 - precision_m: 0.9992 - val_loss: 1.0852 - val_accuracy: 0.8478 - val_f1_m: 0.9166 - val_recall_m: 0.8952 - val_precision_m: 0.9398\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0216 - accuracy: 0.9923 - f1_m: 0.9959 - recall_m: 0.9921 - precision_m: 0.9998 - val_loss: 1.0907 - val_accuracy: 0.8478 - val_f1_m: 0.9167 - val_recall_m: 0.8962 - val_precision_m: 0.9389\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0212 - accuracy: 0.9925 - f1_m: 0.9960 - recall_m: 0.9925 - precision_m: 0.9995 - val_loss: 1.0912 - val_accuracy: 0.8659 - val_f1_m: 0.9271 - val_recall_m: 0.9146 - val_precision_m: 0.9404\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0219 - accuracy: 0.9923 - f1_m: 0.9959 - recall_m: 0.9928 - precision_m: 0.9991 - val_loss: 1.1720 - val_accuracy: 0.8659 - val_f1_m: 0.9273 - val_recall_m: 0.9154 - val_precision_m: 0.9400\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0203 - accuracy: 0.9931 - f1_m: 0.9963 - recall_m: 0.9932 - precision_m: 0.9994 - val_loss: 1.1014 - val_accuracy: 0.8791 - val_f1_m: 0.9350 - val_recall_m: 0.9309 - val_precision_m: 0.9396\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0242 - accuracy: 0.9925 - f1_m: 0.9960 - recall_m: 0.9926 - precision_m: 0.9994 - val_loss: 1.1424 - val_accuracy: 0.8646 - val_f1_m: 0.9266 - val_recall_m: 0.9140 - val_precision_m: 0.9399\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0243 - accuracy: 0.9921 - f1_m: 0.9957 - recall_m: 0.9921 - precision_m: 0.9994 - val_loss: 1.2050 - val_accuracy: 0.8514 - val_f1_m: 0.9189 - val_recall_m: 0.9001 - val_precision_m: 0.9391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f54fa1633c8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Game move model\n",
    "gamemove_model = models_nn.create_nn_model()\n",
    "gamemove_model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', models_nn.f1_m, \n",
    "                                                                               models_nn.recall_m, models_nn.precision_m])\n",
    "gamemove_model.fit(X_train,\n",
    "                   y_train_gamemove,\n",
    "                   batch_size=128,\n",
    "                   epochs=50,\n",
    "                   validation_data=(X_test, y_test_gamemove), \n",
    "#                    callbacks=[models_nn.early_stop],\n",
    "                   class_weight=gamemove_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.508430713634368, 0.5131926644293529, 0.5076396024280688, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamemove_pred = gamemove_model.predict(X_train)\n",
    "gamemove_pred_test = gamemove_model.predict(X_test)\n",
    "\n",
    "gamemove_pred_test_round = gamemove_pred_test.round()\n",
    "precision_recall_fscore_support(y_test_gamemove, gamemove_pred_test_round, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.6937 - accuracy: 0.6493 - f1_m: 0.7226 - recall_m: 0.7152 - precision_m: 0.8327 - val_loss: 0.6768 - val_accuracy: 0.8219 - val_f1_m: 0.9011 - val_recall_m: 1.0000 - val_precision_m: 0.8212\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6909 - accuracy: 0.5646 - f1_m: 0.6589 - recall_m: 0.5778 - precision_m: 0.8378 - val_loss: 0.7108 - val_accuracy: 0.3320 - val_f1_m: 0.3580 - val_recall_m: 0.2277 - val_precision_m: 0.8531\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.6823 - accuracy: 0.5110 - f1_m: 0.6052 - recall_m: 0.4855 - precision_m: 0.8797 - val_loss: 0.6835 - val_accuracy: 0.5101 - val_f1_m: 0.6260 - val_recall_m: 0.5009 - val_precision_m: 0.8394\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.6593 - accuracy: 0.6094 - f1_m: 0.7063 - recall_m: 0.6017 - precision_m: 0.9026 - val_loss: 0.7388 - val_accuracy: 0.3734 - val_f1_m: 0.4320 - val_recall_m: 0.2911 - val_precision_m: 0.8459\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6132 - accuracy: 0.6654 - f1_m: 0.7494 - recall_m: 0.6454 - precision_m: 0.9290 - val_loss: 0.5577 - val_accuracy: 0.7696 - val_f1_m: 0.8655 - val_recall_m: 0.9110 - val_precision_m: 0.8260\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.5613 - accuracy: 0.7226 - f1_m: 0.8090 - recall_m: 0.7201 - precision_m: 0.9340 - val_loss: 0.5897 - val_accuracy: 0.6957 - val_f1_m: 0.8095 - val_recall_m: 0.7953 - val_precision_m: 0.8260\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.5044 - accuracy: 0.7514 - f1_m: 0.8283 - recall_m: 0.7388 - precision_m: 0.9487 - val_loss: 0.5476 - val_accuracy: 0.7507 - val_f1_m: 0.8523 - val_recall_m: 0.8838 - val_precision_m: 0.8250\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.4510 - accuracy: 0.7918 - f1_m: 0.8628 - recall_m: 0.7902 - precision_m: 0.9557 - val_loss: 0.8629 - val_accuracy: 0.4569 - val_f1_m: 0.5662 - val_recall_m: 0.4326 - val_precision_m: 0.8240\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.4048 - accuracy: 0.8040 - f1_m: 0.8706 - recall_m: 0.7948 - precision_m: 0.9668 - val_loss: 0.7377 - val_accuracy: 0.5871 - val_f1_m: 0.7115 - val_recall_m: 0.6268 - val_precision_m: 0.8251\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.3619 - accuracy: 0.8350 - f1_m: 0.8916 - recall_m: 0.8280 - precision_m: 0.9684 - val_loss: 0.6683 - val_accuracy: 0.6671 - val_f1_m: 0.7860 - val_recall_m: 0.7527 - val_precision_m: 0.8239\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.3228 - accuracy: 0.8546 - f1_m: 0.9076 - recall_m: 0.8514 - precision_m: 0.9731 - val_loss: 0.6657 - val_accuracy: 0.7159 - val_f1_m: 0.8267 - val_recall_m: 0.8349 - val_precision_m: 0.8208\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.2907 - accuracy: 0.8668 - f1_m: 0.9133 - recall_m: 0.8589 - precision_m: 0.9772 - val_loss: 0.7061 - val_accuracy: 0.7093 - val_f1_m: 0.8208 - val_recall_m: 0.8195 - val_precision_m: 0.8242\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.2711 - accuracy: 0.8813 - f1_m: 0.9235 - recall_m: 0.8765 - precision_m: 0.9773 - val_loss: 0.7380 - val_accuracy: 0.6847 - val_f1_m: 0.8029 - val_recall_m: 0.7894 - val_precision_m: 0.8189\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.2348 - accuracy: 0.8973 - f1_m: 0.9357 - recall_m: 0.8945 - precision_m: 0.9819 - val_loss: 0.7773 - val_accuracy: 0.7014 - val_f1_m: 0.8149 - val_recall_m: 0.8106 - val_precision_m: 0.8214\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.2156 - accuracy: 0.9049 - f1_m: 0.9408 - recall_m: 0.9020 - precision_m: 0.9839 - val_loss: 0.8218 - val_accuracy: 0.6926 - val_f1_m: 0.8078 - val_recall_m: 0.7972 - val_precision_m: 0.8205\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.1902 - accuracy: 0.9184 - f1_m: 0.9475 - recall_m: 0.9118 - precision_m: 0.9871 - val_loss: 0.8069 - val_accuracy: 0.7159 - val_f1_m: 0.8258 - val_recall_m: 0.8320 - val_precision_m: 0.8219\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1777 - accuracy: 0.9232 - f1_m: 0.9481 - recall_m: 0.9146 - precision_m: 0.9866 - val_loss: 0.9274 - val_accuracy: 0.6737 - val_f1_m: 0.7924 - val_recall_m: 0.7670 - val_precision_m: 0.8218\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.1601 - accuracy: 0.9298 - f1_m: 0.9547 - recall_m: 0.9234 - precision_m: 0.9891 - val_loss: 0.9141 - val_accuracy: 0.7164 - val_f1_m: 0.8255 - val_recall_m: 0.8296 - val_precision_m: 0.8233\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.1516 - accuracy: 0.9361 - f1_m: 0.9609 - recall_m: 0.9343 - precision_m: 0.9895 - val_loss: 0.9758 - val_accuracy: 0.6891 - val_f1_m: 0.8037 - val_recall_m: 0.7862 - val_precision_m: 0.8238\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.1403 - accuracy: 0.9418 - f1_m: 0.9644 - recall_m: 0.9393 - precision_m: 0.9914 - val_loss: 1.0341 - val_accuracy: 0.6988 - val_f1_m: 0.8116 - val_recall_m: 0.8006 - val_precision_m: 0.8250\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1376 - accuracy: 0.9424 - f1_m: 0.9646 - recall_m: 0.9397 - precision_m: 0.9913 - val_loss: 1.0345 - val_accuracy: 0.7159 - val_f1_m: 0.8269 - val_recall_m: 0.8351 - val_precision_m: 0.8209\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1194 - accuracy: 0.9496 - f1_m: 0.9693 - recall_m: 0.9474 - precision_m: 0.9926 - val_loss: 1.0809 - val_accuracy: 0.7071 - val_f1_m: 0.8197 - val_recall_m: 0.8212 - val_precision_m: 0.8206\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1104 - accuracy: 0.9534 - f1_m: 0.9716 - recall_m: 0.9524 - precision_m: 0.9920 - val_loss: 1.1536 - val_accuracy: 0.6847 - val_f1_m: 0.8012 - val_recall_m: 0.7825 - val_precision_m: 0.8230\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.1070 - accuracy: 0.9566 - f1_m: 0.9736 - recall_m: 0.9542 - precision_m: 0.9941 - val_loss: 1.2013 - val_accuracy: 0.6917 - val_f1_m: 0.8077 - val_recall_m: 0.7980 - val_precision_m: 0.8201\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0991 - accuracy: 0.9593 - f1_m: 0.9752 - recall_m: 0.9572 - precision_m: 0.9942 - val_loss: 1.2817 - val_accuracy: 0.6662 - val_f1_m: 0.7851 - val_recall_m: 0.7518 - val_precision_m: 0.8232\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0895 - accuracy: 0.9624 - f1_m: 0.9771 - recall_m: 0.9608 - precision_m: 0.9943 - val_loss: 1.2836 - val_accuracy: 0.6781 - val_f1_m: 0.7957 - val_recall_m: 0.7718 - val_precision_m: 0.8232\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0897 - accuracy: 0.9644 - f1_m: 0.9784 - recall_m: 0.9630 - precision_m: 0.9945 - val_loss: 1.3397 - val_accuracy: 0.6843 - val_f1_m: 0.8019 - val_recall_m: 0.7863 - val_precision_m: 0.8202\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0841 - accuracy: 0.9648 - f1_m: 0.9787 - recall_m: 0.9631 - precision_m: 0.9950 - val_loss: 1.2612 - val_accuracy: 0.6829 - val_f1_m: 0.7996 - val_recall_m: 0.7813 - val_precision_m: 0.8211\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0840 - accuracy: 0.9659 - f1_m: 0.9794 - recall_m: 0.9647 - precision_m: 0.9947 - val_loss: 1.3376 - val_accuracy: 0.7067 - val_f1_m: 0.8190 - val_recall_m: 0.8177 - val_precision_m: 0.8225\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.0728 - accuracy: 0.9685 - f1_m: 0.9810 - recall_m: 0.9678 - precision_m: 0.9948 - val_loss: 1.4149 - val_accuracy: 0.6931 - val_f1_m: 0.8081 - val_recall_m: 0.7976 - val_precision_m: 0.8208\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0683 - accuracy: 0.9713 - f1_m: 0.9826 - recall_m: 0.9698 - precision_m: 0.9959 - val_loss: 1.5027 - val_accuracy: 0.6869 - val_f1_m: 0.8024 - val_recall_m: 0.7838 - val_precision_m: 0.8239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0668 - accuracy: 0.9739 - f1_m: 0.9843 - recall_m: 0.9730 - precision_m: 0.9960 - val_loss: 1.5624 - val_accuracy: 0.6719 - val_f1_m: 0.7912 - val_recall_m: 0.7669 - val_precision_m: 0.8192\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0646 - accuracy: 0.9742 - f1_m: 0.9843 - recall_m: 0.9730 - precision_m: 0.9962 - val_loss: 1.6105 - val_accuracy: 0.6719 - val_f1_m: 0.7914 - val_recall_m: 0.7668 - val_precision_m: 0.8198\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0607 - accuracy: 0.9770 - f1_m: 0.9842 - recall_m: 0.9760 - precision_m: 0.9931 - val_loss: 1.6589 - val_accuracy: 0.6825 - val_f1_m: 0.7996 - val_recall_m: 0.7811 - val_precision_m: 0.8213\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0613 - accuracy: 0.9751 - f1_m: 0.9850 - recall_m: 0.9738 - precision_m: 0.9966 - val_loss: 1.6190 - val_accuracy: 0.6825 - val_f1_m: 0.8001 - val_recall_m: 0.7833 - val_precision_m: 0.8195\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0594 - accuracy: 0.9766 - f1_m: 0.9859 - recall_m: 0.9757 - precision_m: 0.9964 - val_loss: 1.5752 - val_accuracy: 0.6900 - val_f1_m: 0.8062 - val_recall_m: 0.7936 - val_precision_m: 0.8214\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0578 - accuracy: 0.9775 - f1_m: 0.9864 - recall_m: 0.9769 - precision_m: 0.9963 - val_loss: 1.6685 - val_accuracy: 0.6759 - val_f1_m: 0.7956 - val_recall_m: 0.7770 - val_precision_m: 0.8170\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0540 - accuracy: 0.9798 - f1_m: 0.9878 - recall_m: 0.9795 - precision_m: 0.9963 - val_loss: 1.5703 - val_accuracy: 0.6974 - val_f1_m: 0.8118 - val_recall_m: 0.8026 - val_precision_m: 0.8229\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0603 - accuracy: 0.9781 - f1_m: 0.9868 - recall_m: 0.9781 - precision_m: 0.9958 - val_loss: 1.7315 - val_accuracy: 0.7027 - val_f1_m: 0.8156 - val_recall_m: 0.8100 - val_precision_m: 0.8232\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0523 - accuracy: 0.9812 - f1_m: 0.9887 - recall_m: 0.9805 - precision_m: 0.9972 - val_loss: 1.7005 - val_accuracy: 0.6851 - val_f1_m: 0.8012 - val_recall_m: 0.7816 - val_precision_m: 0.8240\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0501 - accuracy: 0.9789 - f1_m: 0.9872 - recall_m: 0.9778 - precision_m: 0.9969 - val_loss: 1.7540 - val_accuracy: 0.7032 - val_f1_m: 0.8152 - val_recall_m: 0.8071 - val_precision_m: 0.8254\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0466 - accuracy: 0.9815 - f1_m: 0.9888 - recall_m: 0.9815 - precision_m: 0.9964 - val_loss: 1.7469 - val_accuracy: 0.6834 - val_f1_m: 0.8003 - val_recall_m: 0.7815 - val_precision_m: 0.8222\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0457 - accuracy: 0.9824 - f1_m: 0.9894 - recall_m: 0.9825 - precision_m: 0.9966 - val_loss: 1.8435 - val_accuracy: 0.6900 - val_f1_m: 0.8051 - val_recall_m: 0.7889 - val_precision_m: 0.8246\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0452 - accuracy: 0.9835 - f1_m: 0.9901 - recall_m: 0.9832 - precision_m: 0.9972 - val_loss: 1.9877 - val_accuracy: 0.6917 - val_f1_m: 0.8064 - val_recall_m: 0.7912 - val_precision_m: 0.8243\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0447 - accuracy: 0.9827 - f1_m: 0.9877 - recall_m: 0.9791 - precision_m: 0.9970 - val_loss: 1.9169 - val_accuracy: 0.7155 - val_f1_m: 0.8256 - val_recall_m: 0.8305 - val_precision_m: 0.8225\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0471 - accuracy: 0.9834 - f1_m: 0.9900 - recall_m: 0.9835 - precision_m: 0.9967 - val_loss: 1.8157 - val_accuracy: 0.7036 - val_f1_m: 0.8160 - val_recall_m: 0.8136 - val_precision_m: 0.8204\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0410 - accuracy: 0.9836 - f1_m: 0.9901 - recall_m: 0.9832 - precision_m: 0.9972 - val_loss: 1.9185 - val_accuracy: 0.6887 - val_f1_m: 0.8044 - val_recall_m: 0.7894 - val_precision_m: 0.8221\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0418 - accuracy: 0.9825 - f1_m: 0.9895 - recall_m: 0.9819 - precision_m: 0.9974 - val_loss: 1.9324 - val_accuracy: 0.6922 - val_f1_m: 0.8069 - val_recall_m: 0.7924 - val_precision_m: 0.8242\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0412 - accuracy: 0.9834 - f1_m: 0.9900 - recall_m: 0.9833 - precision_m: 0.9969 - val_loss: 1.9517 - val_accuracy: 0.6935 - val_f1_m: 0.8079 - val_recall_m: 0.7946 - val_precision_m: 0.8236\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0348 - accuracy: 0.9870 - f1_m: 0.9922 - recall_m: 0.9867 - precision_m: 0.9979 - val_loss: 2.0562 - val_accuracy: 0.7106 - val_f1_m: 0.8205 - val_recall_m: 0.8176 - val_precision_m: 0.8260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f546ef15080>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reasoning model\n",
    "reasoning_model = models_nn.create_nn_model()\n",
    "reasoning_model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', models_nn.f1_m, models_nn.recall_m, models_nn.precision_m])\n",
    "reasoning_model.fit(X_train,\n",
    "                    y_train_reasoning,\n",
    "                    batch_size=128,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_test, y_test_reasoning), \n",
    "#                     callbacks=[models_nn.early_stop],\n",
    "                    class_weight=reasoning_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.514033050948117, 0.5145155856766344, 0.5142161446158641, None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_pred = reasoning_model.predict(X_train)\n",
    "reasoning_pred_test = reasoning_model.predict(X_test)\n",
    "\n",
    "reasoning_pred_test_round = reasoning_pred_test.round()\n",
    "precision_recall_fscore_support(y_test_reasoning, reasoning_pred_test_round, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.6937 - accuracy: 0.5980 - f1_m: 0.6515 - recall_m: 0.6424 - precision_m: 0.8372 - val_loss: 0.7151 - val_accuracy: 0.1772 - val_f1_m: 0.0594 - val_recall_m: 0.0310 - val_precision_m: 0.8083\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.6916 - accuracy: 0.4965 - f1_m: 0.5690 - recall_m: 0.4830 - precision_m: 0.8708 - val_loss: 0.6539 - val_accuracy: 0.8047 - val_f1_m: 0.8898 - val_recall_m: 0.9378 - val_precision_m: 0.8478\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.6819 - accuracy: 0.5940 - f1_m: 0.6931 - recall_m: 0.6146 - precision_m: 0.8808 - val_loss: 0.6137 - val_accuracy: 0.7823 - val_f1_m: 0.8747 - val_recall_m: 0.9039 - val_precision_m: 0.8492\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6598 - accuracy: 0.6350 - f1_m: 0.7377 - recall_m: 0.6460 - precision_m: 0.8969 - val_loss: 0.7010 - val_accuracy: 0.4631 - val_f1_m: 0.5779 - val_recall_m: 0.4379 - val_precision_m: 0.8575\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6203 - accuracy: 0.6511 - f1_m: 0.7477 - recall_m: 0.6418 - precision_m: 0.9166 - val_loss: 0.7763 - val_accuracy: 0.3931 - val_f1_m: 0.4801 - val_recall_m: 0.3332 - val_precision_m: 0.8679\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.5761 - accuracy: 0.6771 - f1_m: 0.7672 - recall_m: 0.6593 - precision_m: 0.9361 - val_loss: 0.6728 - val_accuracy: 0.5554 - val_f1_m: 0.6825 - val_recall_m: 0.5698 - val_precision_m: 0.8543\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.5319 - accuracy: 0.7301 - f1_m: 0.8130 - recall_m: 0.7164 - precision_m: 0.9482 - val_loss: 0.5571 - val_accuracy: 0.7181 - val_f1_m: 0.8281 - val_recall_m: 0.8093 - val_precision_m: 0.8499\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.4844 - accuracy: 0.7602 - f1_m: 0.8408 - recall_m: 0.7536 - precision_m: 0.9556 - val_loss: 0.6063 - val_accuracy: 0.6618 - val_f1_m: 0.7817 - val_recall_m: 0.7212 - val_precision_m: 0.8560\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4320 - accuracy: 0.7899 - f1_m: 0.8556 - recall_m: 0.7749 - precision_m: 0.9624 - val_loss: 0.5686 - val_accuracy: 0.7335 - val_f1_m: 0.8389 - val_recall_m: 0.8260 - val_precision_m: 0.8539\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.3912 - accuracy: 0.8102 - f1_m: 0.8722 - recall_m: 0.8041 - precision_m: 0.9607 - val_loss: 0.7375 - val_accuracy: 0.5888 - val_f1_m: 0.7139 - val_recall_m: 0.6112 - val_precision_m: 0.8602\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.3640 - accuracy: 0.8244 - f1_m: 0.8870 - recall_m: 0.8191 - precision_m: 0.9704 - val_loss: 0.7179 - val_accuracy: 0.6099 - val_f1_m: 0.7363 - val_recall_m: 0.6484 - val_precision_m: 0.8536\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.3271 - accuracy: 0.8465 - f1_m: 0.9026 - recall_m: 0.8424 - precision_m: 0.9740 - val_loss: 0.7338 - val_accuracy: 0.6310 - val_f1_m: 0.7548 - val_recall_m: 0.6763 - val_precision_m: 0.8553\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.2952 - accuracy: 0.8656 - f1_m: 0.9105 - recall_m: 0.8533 - precision_m: 0.9793 - val_loss: 0.8063 - val_accuracy: 0.6060 - val_f1_m: 0.7312 - val_recall_m: 0.6391 - val_precision_m: 0.8562\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.2768 - accuracy: 0.8729 - f1_m: 0.9201 - recall_m: 0.8678 - precision_m: 0.9807 - val_loss: 0.7487 - val_accuracy: 0.6640 - val_f1_m: 0.7835 - val_recall_m: 0.7242 - val_precision_m: 0.8544\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.2478 - accuracy: 0.8877 - f1_m: 0.9299 - recall_m: 0.8834 - precision_m: 0.9828 - val_loss: 0.7370 - val_accuracy: 0.7049 - val_f1_m: 0.8173 - val_recall_m: 0.7865 - val_precision_m: 0.8518\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.2303 - accuracy: 0.8990 - f1_m: 0.9348 - recall_m: 0.8921 - precision_m: 0.9834 - val_loss: 0.7620 - val_accuracy: 0.7102 - val_f1_m: 0.8209 - val_recall_m: 0.7906 - val_precision_m: 0.8550\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.2143 - accuracy: 0.9060 - f1_m: 0.9418 - recall_m: 0.9024 - precision_m: 0.9859 - val_loss: 0.7822 - val_accuracy: 0.7133 - val_f1_m: 0.8233 - val_recall_m: 0.7968 - val_precision_m: 0.8528\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1990 - accuracy: 0.9131 - f1_m: 0.9466 - recall_m: 0.9106 - precision_m: 0.9865 - val_loss: 0.8195 - val_accuracy: 0.7282 - val_f1_m: 0.8350 - val_recall_m: 0.8219 - val_precision_m: 0.8494\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.1912 - accuracy: 0.9184 - f1_m: 0.9499 - recall_m: 0.9173 - precision_m: 0.9858 - val_loss: 0.8799 - val_accuracy: 0.6873 - val_f1_m: 0.8036 - val_recall_m: 0.7627 - val_precision_m: 0.8506\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1739 - accuracy: 0.9237 - f1_m: 0.9394 - recall_m: 0.9068 - precision_m: 0.9754 - val_loss: 0.8523 - val_accuracy: 0.7647 - val_f1_m: 0.8614 - val_recall_m: 0.8755 - val_precision_m: 0.8486\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1563 - accuracy: 0.9376 - f1_m: 0.9621 - recall_m: 0.9366 - precision_m: 0.9897 - val_loss: 0.9249 - val_accuracy: 0.7225 - val_f1_m: 0.8304 - val_recall_m: 0.8113 - val_precision_m: 0.8512\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1555 - accuracy: 0.9349 - f1_m: 0.9578 - recall_m: 0.9302 - precision_m: 0.9884 - val_loss: 0.9419 - val_accuracy: 0.7318 - val_f1_m: 0.8369 - val_recall_m: 0.8215 - val_precision_m: 0.8537\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1433 - accuracy: 0.9405 - f1_m: 0.9640 - recall_m: 0.9393 - precision_m: 0.9904 - val_loss: 1.0405 - val_accuracy: 0.6834 - val_f1_m: 0.7991 - val_recall_m: 0.7526 - val_precision_m: 0.8526\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1303 - accuracy: 0.9478 - f1_m: 0.9683 - recall_m: 0.9459 - precision_m: 0.9923 - val_loss: 1.0597 - val_accuracy: 0.6895 - val_f1_m: 0.8041 - val_recall_m: 0.7609 - val_precision_m: 0.8535\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1219 - accuracy: 0.9505 - f1_m: 0.9701 - recall_m: 0.9492 - precision_m: 0.9923 - val_loss: 1.0596 - val_accuracy: 0.7190 - val_f1_m: 0.8279 - val_recall_m: 0.8082 - val_precision_m: 0.8493\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1134 - accuracy: 0.9530 - f1_m: 0.9717 - recall_m: 0.9523 - precision_m: 0.9922 - val_loss: 1.0971 - val_accuracy: 0.7296 - val_f1_m: 0.8355 - val_recall_m: 0.8207 - val_precision_m: 0.8516\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1059 - accuracy: 0.9582 - f1_m: 0.9747 - recall_m: 0.9575 - precision_m: 0.9931 - val_loss: 1.1226 - val_accuracy: 0.7344 - val_f1_m: 0.8394 - val_recall_m: 0.8292 - val_precision_m: 0.8505\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1015 - accuracy: 0.9604 - f1_m: 0.9763 - recall_m: 0.9600 - precision_m: 0.9934 - val_loss: 1.2341 - val_accuracy: 0.6658 - val_f1_m: 0.7847 - val_recall_m: 0.7266 - val_precision_m: 0.8540\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1016 - accuracy: 0.9572 - f1_m: 0.9742 - recall_m: 0.9563 - precision_m: 0.9932 - val_loss: 1.1028 - val_accuracy: 0.7480 - val_f1_m: 0.8496 - val_recall_m: 0.8500 - val_precision_m: 0.8497\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0858 - accuracy: 0.9649 - f1_m: 0.9790 - recall_m: 0.9644 - precision_m: 0.9942 - val_loss: 1.2298 - val_accuracy: 0.7124 - val_f1_m: 0.8214 - val_recall_m: 0.7918 - val_precision_m: 0.8541\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0872 - accuracy: 0.9659 - f1_m: 0.9796 - recall_m: 0.9657 - precision_m: 0.9942 - val_loss: 1.2973 - val_accuracy: 0.6869 - val_f1_m: 0.8027 - val_recall_m: 0.7604 - val_precision_m: 0.8509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0858 - accuracy: 0.9645 - f1_m: 0.9786 - recall_m: 0.9634 - precision_m: 0.9946 - val_loss: 1.2777 - val_accuracy: 0.7071 - val_f1_m: 0.8176 - val_recall_m: 0.7854 - val_precision_m: 0.8533\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0874 - accuracy: 0.9666 - f1_m: 0.9799 - recall_m: 0.9655 - precision_m: 0.9951 - val_loss: 1.3165 - val_accuracy: 0.6931 - val_f1_m: 0.8063 - val_recall_m: 0.7642 - val_precision_m: 0.8544\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0761 - accuracy: 0.9691 - f1_m: 0.9815 - recall_m: 0.9684 - precision_m: 0.9952 - val_loss: 1.3472 - val_accuracy: 0.7401 - val_f1_m: 0.8430 - val_recall_m: 0.8355 - val_precision_m: 0.8516\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0686 - accuracy: 0.9738 - f1_m: 0.9844 - recall_m: 0.9730 - precision_m: 0.9963 - val_loss: 1.4381 - val_accuracy: 0.6970 - val_f1_m: 0.8101 - val_recall_m: 0.7732 - val_precision_m: 0.8518\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0687 - accuracy: 0.9732 - f1_m: 0.9840 - recall_m: 0.9731 - precision_m: 0.9953 - val_loss: 1.4270 - val_accuracy: 0.7243 - val_f1_m: 0.8314 - val_recall_m: 0.8121 - val_precision_m: 0.8526\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0706 - accuracy: 0.9725 - f1_m: 0.9836 - recall_m: 0.9720 - precision_m: 0.9957 - val_loss: 1.4549 - val_accuracy: 0.7419 - val_f1_m: 0.8448 - val_recall_m: 0.8384 - val_precision_m: 0.8521\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0600 - accuracy: 0.9769 - f1_m: 0.9863 - recall_m: 0.9764 - precision_m: 0.9965 - val_loss: 1.4575 - val_accuracy: 0.7265 - val_f1_m: 0.8338 - val_recall_m: 0.8202 - val_precision_m: 0.8487\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0702 - accuracy: 0.9725 - f1_m: 0.9836 - recall_m: 0.9726 - precision_m: 0.9950 - val_loss: 1.5193 - val_accuracy: 0.7322 - val_f1_m: 0.8375 - val_recall_m: 0.8266 - val_precision_m: 0.8495\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0562 - accuracy: 0.9781 - f1_m: 0.9870 - recall_m: 0.9777 - precision_m: 0.9966 - val_loss: 1.5881 - val_accuracy: 0.7300 - val_f1_m: 0.8363 - val_recall_m: 0.8244 - val_precision_m: 0.8493\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0583 - accuracy: 0.9758 - f1_m: 0.9856 - recall_m: 0.9759 - precision_m: 0.9956 - val_loss: 1.6225 - val_accuracy: 0.6909 - val_f1_m: 0.8056 - val_recall_m: 0.7650 - val_precision_m: 0.8518\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0632 - accuracy: 0.9761 - f1_m: 0.9857 - recall_m: 0.9755 - precision_m: 0.9963 - val_loss: 1.5842 - val_accuracy: 0.7269 - val_f1_m: 0.8335 - val_recall_m: 0.8186 - val_precision_m: 0.8498\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0595 - accuracy: 0.9782 - f1_m: 0.9844 - recall_m: 0.9742 - precision_m: 0.9958 - val_loss: 1.6163 - val_accuracy: 0.7379 - val_f1_m: 0.8420 - val_recall_m: 0.8352 - val_precision_m: 0.8498\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0547 - accuracy: 0.9816 - f1_m: 0.9891 - recall_m: 0.9820 - precision_m: 0.9963 - val_loss: 1.5968 - val_accuracy: 0.7419 - val_f1_m: 0.8445 - val_recall_m: 0.8408 - val_precision_m: 0.8491\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0561 - accuracy: 0.9789 - f1_m: 0.9874 - recall_m: 0.9791 - precision_m: 0.9960 - val_loss: 1.6689 - val_accuracy: 0.7142 - val_f1_m: 0.8237 - val_recall_m: 0.7986 - val_precision_m: 0.8513\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0528 - accuracy: 0.9812 - f1_m: 0.9889 - recall_m: 0.9807 - precision_m: 0.9973 - val_loss: 1.7239 - val_accuracy: 0.7106 - val_f1_m: 0.8206 - val_recall_m: 0.7908 - val_precision_m: 0.8536\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0548 - accuracy: 0.9787 - f1_m: 0.9872 - recall_m: 0.9775 - precision_m: 0.9972 - val_loss: 1.6983 - val_accuracy: 0.7076 - val_f1_m: 0.8186 - val_recall_m: 0.7889 - val_precision_m: 0.8518\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0468 - accuracy: 0.9812 - f1_m: 0.9889 - recall_m: 0.9805 - precision_m: 0.9975 - val_loss: 1.7152 - val_accuracy: 0.7423 - val_f1_m: 0.8454 - val_recall_m: 0.8418 - val_precision_m: 0.8497\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0448 - accuracy: 0.9835 - f1_m: 0.9902 - recall_m: 0.9832 - precision_m: 0.9974 - val_loss: 1.7941 - val_accuracy: 0.7067 - val_f1_m: 0.8178 - val_recall_m: 0.7865 - val_precision_m: 0.8528\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0485 - accuracy: 0.9817 - f1_m: 0.9892 - recall_m: 0.9816 - precision_m: 0.9970 - val_loss: 1.7972 - val_accuracy: 0.7260 - val_f1_m: 0.8327 - val_recall_m: 0.8153 - val_precision_m: 0.8519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f54f8451160>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Share Information model\n",
    "shareinfo_model = models_nn.create_nn_model()\n",
    "shareinfo_model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', models_nn.f1_m, models_nn.recall_m, models_nn.precision_m])\n",
    "shareinfo_model.fit(X_train,\n",
    "                    y_train_share_information,\n",
    "                    batch_size=128,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_test, y_test_share_information), \n",
    "#                     callbacks=[models_nn.early_stop],\n",
    "                    class_weight=share_info_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5235139312159788, 0.5278689971751412, 0.5242318354845913, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shareinfo_pred = shareinfo_model.predict(X_train)\n",
    "shareinfo_pred_test = shareinfo_model.predict(X_test)\n",
    "\n",
    "shareinfo_pred_test_round = shareinfo_pred_test.round()\n",
    "precision_recall_fscore_support(y_test_share_information, shareinfo_pred_test_round, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6970 - accuracy: 0.7425 - f1_m: 0.8100 - recall_m: 0.7717 - precision_m: 0.9475 - val_loss: 0.4715 - val_accuracy: 0.9507 - val_f1_m: 0.9747 - val_recall_m: 1.0000 - val_precision_m: 0.9514\n",
      "Epoch 2/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.7030 - accuracy: 0.5277 - f1_m: 0.6127 - recall_m: 0.5278 - precision_m: 0.9359 - val_loss: 0.7135 - val_accuracy: 0.0655 - val_f1_m: 0.0341 - val_recall_m: 0.0179 - val_precision_m: 0.4236\n",
      "Epoch 3/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6912 - accuracy: 0.5581 - f1_m: 0.6569 - recall_m: 0.5581 - precision_m: 0.9478 - val_loss: 0.6722 - val_accuracy: 0.2867 - val_f1_m: 0.4017 - val_recall_m: 0.2609 - val_precision_m: 0.9325\n",
      "Epoch 4/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6788 - accuracy: 0.5219 - f1_m: 0.6496 - recall_m: 0.5179 - precision_m: 0.9671 - val_loss: 0.5328 - val_accuracy: 0.9507 - val_f1_m: 0.9747 - val_recall_m: 0.9995 - val_precision_m: 0.9518\n",
      "Epoch 5/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6740 - accuracy: 0.5406 - f1_m: 0.6573 - recall_m: 0.5341 - precision_m: 0.9661 - val_loss: 0.6560 - val_accuracy: 0.3883 - val_f1_m: 0.5247 - val_recall_m: 0.3690 - val_precision_m: 0.9407\n",
      "Epoch 6/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6595 - accuracy: 0.5807 - f1_m: 0.7032 - recall_m: 0.5769 - precision_m: 0.9720 - val_loss: 0.7228 - val_accuracy: 0.2617 - val_f1_m: 0.3647 - val_recall_m: 0.2314 - val_precision_m: 0.9350\n",
      "Epoch 7/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6630 - accuracy: 0.5582 - f1_m: 0.6863 - recall_m: 0.5486 - precision_m: 0.9713 - val_loss: 0.6687 - val_accuracy: 0.3958 - val_f1_m: 0.5336 - val_recall_m: 0.3779 - val_precision_m: 0.9423\n",
      "Epoch 8/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6482 - accuracy: 0.5224 - f1_m: 0.6514 - recall_m: 0.5108 - precision_m: 0.9768 - val_loss: 0.7308 - val_accuracy: 0.3276 - val_f1_m: 0.4513 - val_recall_m: 0.3012 - val_precision_m: 0.9440\n",
      "Epoch 9/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.5452 - f1_m: 0.6751 - recall_m: 0.5338 - precision_m: 0.9782 - val_loss: 0.7508 - val_accuracy: 0.3061 - val_f1_m: 0.4239 - val_recall_m: 0.2781 - val_precision_m: 0.9454\n",
      "Epoch 10/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6480 - accuracy: 0.4911 - f1_m: 0.6251 - recall_m: 0.4741 - precision_m: 0.9817 - val_loss: 0.7373 - val_accuracy: 0.3584 - val_f1_m: 0.4895 - val_recall_m: 0.3352 - val_precision_m: 0.9442\n",
      "Epoch 11/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6429 - accuracy: 0.5637 - f1_m: 0.6942 - recall_m: 0.5523 - precision_m: 0.9832 - val_loss: 0.5308 - val_accuracy: 0.6799 - val_f1_m: 0.8003 - val_recall_m: 0.6930 - val_precision_m: 0.9551\n",
      "Epoch 12/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.5401 - f1_m: 0.6713 - recall_m: 0.5258 - precision_m: 0.9839 - val_loss: 0.6715 - val_accuracy: 0.5079 - val_f1_m: 0.6486 - val_recall_m: 0.4985 - val_precision_m: 0.9423\n",
      "Epoch 13/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.5449 - f1_m: 0.6751 - recall_m: 0.5302 - precision_m: 0.9833 - val_loss: 0.6633 - val_accuracy: 0.4815 - val_f1_m: 0.6211 - val_recall_m: 0.4674 - val_precision_m: 0.9478\n",
      "Epoch 14/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6094 - accuracy: 0.5538 - f1_m: 0.6878 - recall_m: 0.5402 - precision_m: 0.9857 - val_loss: 0.6805 - val_accuracy: 0.4292 - val_f1_m: 0.5691 - val_recall_m: 0.4115 - val_precision_m: 0.9470\n",
      "Epoch 15/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6015 - accuracy: 0.5409 - f1_m: 0.6763 - recall_m: 0.5264 - precision_m: 0.9852 - val_loss: 0.6889 - val_accuracy: 0.5255 - val_f1_m: 0.6666 - val_recall_m: 0.5209 - val_precision_m: 0.9382\n",
      "Epoch 16/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6130 - accuracy: 0.5405 - f1_m: 0.6764 - recall_m: 0.5258 - precision_m: 0.9842 - val_loss: 0.6795 - val_accuracy: 0.4565 - val_f1_m: 0.5988 - val_recall_m: 0.4452 - val_precision_m: 0.9390\n",
      "Epoch 17/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.5601 - f1_m: 0.6936 - recall_m: 0.5478 - precision_m: 0.9844 - val_loss: 0.6725 - val_accuracy: 0.4833 - val_f1_m: 0.6237 - val_recall_m: 0.4704 - val_precision_m: 0.9469\n",
      "Epoch 18/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5869 - accuracy: 0.5808 - f1_m: 0.7110 - recall_m: 0.5662 - precision_m: 0.9862 - val_loss: 0.6895 - val_accuracy: 0.4371 - val_f1_m: 0.5784 - val_recall_m: 0.4221 - val_precision_m: 0.9449\n",
      "Epoch 19/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5563 - accuracy: 0.6186 - f1_m: 0.7458 - recall_m: 0.6084 - precision_m: 0.9859 - val_loss: 0.7143 - val_accuracy: 0.4244 - val_f1_m: 0.5643 - val_recall_m: 0.4070 - val_precision_m: 0.9461\n",
      "Epoch 20/50\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.5493 - accuracy: 0.5965 - f1_m: 0.7239 - recall_m: 0.5840 - precision_m: 0.9839 - val_loss: 0.8318 - val_accuracy: 0.3210 - val_f1_m: 0.4442 - val_recall_m: 0.2948 - val_precision_m: 0.9434\n",
      "Epoch 21/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5516 - accuracy: 0.5785 - f1_m: 0.7100 - recall_m: 0.5652 - precision_m: 0.9859 - val_loss: 0.5405 - val_accuracy: 0.6434 - val_f1_m: 0.7730 - val_recall_m: 0.6520 - val_precision_m: 0.9575\n",
      "Epoch 22/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5292 - accuracy: 0.6453 - f1_m: 0.7669 - recall_m: 0.6358 - precision_m: 0.9866 - val_loss: 0.5903 - val_accuracy: 0.5858 - val_f1_m: 0.7278 - val_recall_m: 0.5927 - val_precision_m: 0.9543\n",
      "Epoch 23/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5271 - accuracy: 0.6400 - f1_m: 0.7630 - recall_m: 0.6308 - precision_m: 0.9867 - val_loss: 0.5767 - val_accuracy: 0.6073 - val_f1_m: 0.7452 - val_recall_m: 0.6153 - val_precision_m: 0.9548\n",
      "Epoch 24/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5021 - accuracy: 0.6707 - f1_m: 0.7881 - recall_m: 0.6628 - precision_m: 0.9870 - val_loss: 0.6850 - val_accuracy: 0.5268 - val_f1_m: 0.6668 - val_recall_m: 0.5208 - val_precision_m: 0.9420\n",
      "Epoch 25/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5053 - accuracy: 0.6491 - f1_m: 0.7708 - recall_m: 0.6391 - precision_m: 0.9881 - val_loss: 0.6792 - val_accuracy: 0.5233 - val_f1_m: 0.6622 - val_recall_m: 0.5147 - val_precision_m: 0.9453\n",
      "Epoch 26/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5038 - accuracy: 0.6661 - f1_m: 0.7836 - recall_m: 0.6560 - precision_m: 0.9880 - val_loss: 0.4767 - val_accuracy: 0.7590 - val_f1_m: 0.8572 - val_recall_m: 0.7827 - val_precision_m: 0.9529\n",
      "Epoch 27/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4887 - accuracy: 0.6883 - f1_m: 0.8021 - recall_m: 0.6820 - precision_m: 0.9871 - val_loss: 0.5536 - val_accuracy: 0.6464 - val_f1_m: 0.7770 - val_recall_m: 0.6603 - val_precision_m: 0.9517\n",
      "Epoch 28/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4873 - accuracy: 0.6931 - f1_m: 0.8049 - recall_m: 0.6863 - precision_m: 0.9875 - val_loss: 0.4861 - val_accuracy: 0.7577 - val_f1_m: 0.8561 - val_recall_m: 0.7814 - val_precision_m: 0.9527\n",
      "Epoch 29/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4681 - accuracy: 0.7209 - f1_m: 0.8261 - recall_m: 0.7156 - precision_m: 0.9882 - val_loss: 0.5408 - val_accuracy: 0.6702 - val_f1_m: 0.7946 - val_recall_m: 0.6854 - val_precision_m: 0.9524\n",
      "Epoch 30/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4695 - accuracy: 0.7209 - f1_m: 0.8259 - recall_m: 0.7146 - precision_m: 0.9890 - val_loss: 0.5513 - val_accuracy: 0.6812 - val_f1_m: 0.8023 - val_recall_m: 0.6978 - val_precision_m: 0.9525\n",
      "Epoch 31/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4640 - accuracy: 0.7242 - f1_m: 0.8288 - recall_m: 0.7185 - precision_m: 0.9892 - val_loss: 0.6509 - val_accuracy: 0.5923 - val_f1_m: 0.7331 - val_recall_m: 0.6006 - val_precision_m: 0.9526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4494 - accuracy: 0.7389 - f1_m: 0.8392 - recall_m: 0.7333 - precision_m: 0.9900 - val_loss: 0.6080 - val_accuracy: 0.6284 - val_f1_m: 0.7626 - val_recall_m: 0.6395 - val_precision_m: 0.9537\n",
      "Epoch 33/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4510 - accuracy: 0.7410 - f1_m: 0.8395 - recall_m: 0.7351 - precision_m: 0.9886 - val_loss: 0.5261 - val_accuracy: 0.7137 - val_f1_m: 0.8263 - val_recall_m: 0.7335 - val_precision_m: 0.9525\n",
      "Epoch 34/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4354 - accuracy: 0.7476 - f1_m: 0.8453 - recall_m: 0.7433 - precision_m: 0.9897 - val_loss: 0.5800 - val_accuracy: 0.6601 - val_f1_m: 0.7866 - val_recall_m: 0.6749 - val_precision_m: 0.9530\n",
      "Epoch 35/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4186 - accuracy: 0.7631 - f1_m: 0.8560 - recall_m: 0.7586 - precision_m: 0.9898 - val_loss: 0.6002 - val_accuracy: 0.6649 - val_f1_m: 0.7907 - val_recall_m: 0.6798 - val_precision_m: 0.9535\n",
      "Epoch 36/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4310 - accuracy: 0.7541 - f1_m: 0.8502 - recall_m: 0.7489 - precision_m: 0.9901 - val_loss: 0.5792 - val_accuracy: 0.6689 - val_f1_m: 0.7938 - val_recall_m: 0.6849 - val_precision_m: 0.9527\n",
      "Epoch 37/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4035 - accuracy: 0.7745 - f1_m: 0.8624 - recall_m: 0.7679 - precision_m: 0.9924 - val_loss: 0.6119 - val_accuracy: 0.6653 - val_f1_m: 0.7909 - val_recall_m: 0.6798 - val_precision_m: 0.9538\n",
      "Epoch 38/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4000 - accuracy: 0.7664 - f1_m: 0.8579 - recall_m: 0.7603 - precision_m: 0.9917 - val_loss: 0.5737 - val_accuracy: 0.7498 - val_f1_m: 0.8509 - val_recall_m: 0.7735 - val_precision_m: 0.9525\n",
      "Epoch 39/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.4073 - accuracy: 0.7664 - f1_m: 0.8586 - recall_m: 0.7618 - precision_m: 0.9918 - val_loss: 0.5830 - val_accuracy: 0.6944 - val_f1_m: 0.8126 - val_recall_m: 0.7128 - val_precision_m: 0.9528\n",
      "Epoch 40/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.3921 - accuracy: 0.7742 - f1_m: 0.8626 - recall_m: 0.7685 - precision_m: 0.9917 - val_loss: 0.6376 - val_accuracy: 0.6328 - val_f1_m: 0.7661 - val_recall_m: 0.6447 - val_precision_m: 0.9535\n",
      "Epoch 41/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.3957 - accuracy: 0.7672 - f1_m: 0.8588 - recall_m: 0.7626 - precision_m: 0.9913 - val_loss: 0.5479 - val_accuracy: 0.7353 - val_f1_m: 0.8408 - val_recall_m: 0.7572 - val_precision_m: 0.9525\n",
      "Epoch 42/50\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.4065 - accuracy: 0.7698 - f1_m: 0.8591 - recall_m: 0.7639 - precision_m: 0.9908 - val_loss: 0.6875 - val_accuracy: 0.6011 - val_f1_m: 0.7399 - val_recall_m: 0.6078 - val_precision_m: 0.9575\n",
      "Epoch 43/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.3731 - accuracy: 0.7859 - f1_m: 0.8708 - recall_m: 0.7803 - precision_m: 0.9926 - val_loss: 0.5536 - val_accuracy: 0.7203 - val_f1_m: 0.8310 - val_recall_m: 0.7417 - val_precision_m: 0.9524\n",
      "Epoch 44/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.3852 - accuracy: 0.7911 - f1_m: 0.8755 - recall_m: 0.7869 - precision_m: 0.9925 - val_loss: 0.5073 - val_accuracy: 0.7806 - val_f1_m: 0.8711 - val_recall_m: 0.8062 - val_precision_m: 0.9542\n",
      "Epoch 45/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.3652 - accuracy: 0.8032 - f1_m: 0.8802 - recall_m: 0.7966 - precision_m: 0.9890 - val_loss: 0.5390 - val_accuracy: 0.7814 - val_f1_m: 0.8717 - val_recall_m: 0.8075 - val_precision_m: 0.9537\n",
      "Epoch 46/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.3629 - accuracy: 0.8098 - f1_m: 0.8872 - recall_m: 0.8053 - precision_m: 0.9937 - val_loss: 0.5660 - val_accuracy: 0.7252 - val_f1_m: 0.8339 - val_recall_m: 0.7450 - val_precision_m: 0.9540\n",
      "Epoch 47/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.3557 - accuracy: 0.8026 - f1_m: 0.8824 - recall_m: 0.7971 - precision_m: 0.9942 - val_loss: 0.6055 - val_accuracy: 0.7234 - val_f1_m: 0.8327 - val_recall_m: 0.7439 - val_precision_m: 0.9525\n",
      "Epoch 48/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.3560 - accuracy: 0.8069 - f1_m: 0.8857 - recall_m: 0.8019 - precision_m: 0.9940 - val_loss: 0.5484 - val_accuracy: 0.8105 - val_f1_m: 0.8900 - val_recall_m: 0.8386 - val_precision_m: 0.9542\n",
      "Epoch 49/50\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.3508 - accuracy: 0.8134 - f1_m: 0.8890 - recall_m: 0.8090 - precision_m: 0.9925 - val_loss: 0.7046 - val_accuracy: 0.6856 - val_f1_m: 0.8062 - val_recall_m: 0.7043 - val_precision_m: 0.9518\n",
      "Epoch 50/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.3570 - accuracy: 0.8132 - f1_m: 0.8892 - recall_m: 0.8089 - precision_m: 0.9927 - val_loss: 0.4739 - val_accuracy: 0.8250 - val_f1_m: 0.8988 - val_recall_m: 0.8535 - val_precision_m: 0.9548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f54e67d80f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deception model\n",
    "deception_model = models_nn.create_nn_model()\n",
    "deception_model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', models_nn.f1_m, models_nn.recall_m, models_nn.precision_m])\n",
    "deception_model.fit(X_train,\n",
    "                    y_train_deception,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_test, y_test_deception), \n",
    "#                     callbacks=[models_nn.early_stop],\n",
    "                    class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5101851851851852, 0.5269839434386151, 0.5013080956463668, None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deception_pred = deception_model.predict(X_train)\n",
    "deception_pred_test = deception_model.predict(X_test)\n",
    "deception_pred_test_round = deception_pred_test.round()\n",
    "precision_recall_fscore_support(y_test_deception, deception_pred_test_round, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deception_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique, counts = np.unique(deception_pred_test_round, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deception_model.history.history['val_f1_m']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train encodings\n",
    "pred_df_arr_full = []\n",
    "pred_df_arr = []\n",
    "for i in range(0, len(gamemove_pred)):\n",
    "    pred_obj_1 = {}\n",
    "    pred_obj_1['gamemove'] = gamemove_pred[i][0]\n",
    "    pred_obj_1['reasoning'] = reasoning_pred[i][0]\n",
    "    pred_obj_1['shareinfo'] = shareinfo_pred[i][0]\n",
    "    pred_df_arr.append(pred_obj_1)\n",
    "    \n",
    "    pred_obj_2 = pred_obj_1.copy()\n",
    "    pred_obj_2['rapport'] = rapport_pred[i][0]\n",
    "    pred_df_arr_full.append(pred_obj_2)\n",
    "    \n",
    "pred_df_full = pd.DataFrame(pred_df_arr_full)\n",
    "pred_df = pd.DataFrame(pred_df_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encodings\n",
    "pred_test_df_arr_full = []\n",
    "pred_test_df_arr = []\n",
    "\n",
    "for i in range(0, len(gamemove_pred_test)):\n",
    "    pred_obj_1 = {}\n",
    "    pred_obj_1['gamemove'] = gamemove_pred_test[i][0]\n",
    "    pred_obj_1['reasoning'] = reasoning_pred_test[i][0]\n",
    "    pred_obj_1['shareinfo'] = shareinfo_pred_test[i][0]\n",
    "    pred_test_df_arr.append(pred_obj_1)\n",
    "    \n",
    "    pred_obj_2 = pred_obj_1.copy()\n",
    "    pred_obj_2['rapport'] = rapport_pred_test[i][0]\n",
    "    pred_test_df_arr_full.append(pred_obj_2)\n",
    "    \n",
    "pred_test_df_full = pd.DataFrame(pred_test_df_arr_full)\n",
    "pred_test_df = pd.DataFrame(pred_test_df_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model with one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting deception\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.7062 - acc: 0.8586 - f1_m: 0.9233 - precision_m: 0.9516 - recall_m: 0.8978 - val_loss: 0.6339 - val_acc: 0.7828 - val_f1_m: 0.8769 - val_precision_m: 0.9495 - val_recall_m: 0.8160\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6975 - acc: 0.8454 - f1_m: 0.9155 - precision_m: 0.9517 - recall_m: 0.8833 - val_loss: 0.6654 - val_acc: 0.7423 - val_f1_m: 0.8499 - val_precision_m: 0.9489 - val_recall_m: 0.7713\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6959 - acc: 0.8013 - f1_m: 0.8757 - precision_m: 0.9510 - recall_m: 0.8306 - val_loss: 0.6855 - val_acc: 0.3470 - val_f1_m: 0.4871 - val_precision_m: 0.9555 - val_recall_m: 0.3298\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6956 - acc: 0.2112 - f1_m: 0.2979 - precision_m: 0.9440 - recall_m: 0.1790 - val_loss: 0.6894 - val_acc: 0.3184 - val_f1_m: 0.4515 - val_precision_m: 0.9567 - val_recall_m: 0.2973\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6954 - acc: 0.2046 - f1_m: 0.2893 - precision_m: 0.9455 - recall_m: 0.1733 - val_loss: 0.6911 - val_acc: 0.3061 - val_f1_m: 0.4350 - val_precision_m: 0.9587 - val_recall_m: 0.2831\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6953 - acc: 0.2446 - f1_m: 0.3326 - precision_m: 0.9437 - recall_m: 0.2172 - val_loss: 0.6935 - val_acc: 0.2907 - val_f1_m: 0.4149 - val_precision_m: 0.9567 - val_recall_m: 0.2667\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6953 - acc: 0.3366 - f1_m: 0.4250 - precision_m: 0.9431 - recall_m: 0.3182 - val_loss: 0.6922 - val_acc: 0.3017 - val_f1_m: 0.4295 - val_precision_m: 0.9582 - val_recall_m: 0.2786\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6949 - acc: 0.2042 - f1_m: 0.2899 - precision_m: 0.9475 - recall_m: 0.1731 - val_loss: 0.6967 - val_acc: 0.2784 - val_f1_m: 0.3979 - val_precision_m: 0.9576 - val_recall_m: 0.2530\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6948 - acc: 0.4602 - f1_m: 0.5501 - precision_m: 0.9540 - recall_m: 0.4560 - val_loss: 0.6939 - val_acc: 0.2946 - val_f1_m: 0.4208 - val_precision_m: 0.9571 - val_recall_m: 0.2712\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6946 - acc: 0.2046 - f1_m: 0.2909 - precision_m: 0.9523 - recall_m: 0.1736 - val_loss: 0.6951 - val_acc: 0.2880 - val_f1_m: 0.4111 - val_precision_m: 0.9580 - val_recall_m: 0.2635\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6946 - acc: 0.2040 - f1_m: 0.2871 - precision_m: 0.9418 - recall_m: 0.1712 - val_loss: 0.6982 - val_acc: 0.2696 - val_f1_m: 0.3863 - val_precision_m: 0.9560 - val_recall_m: 0.2438\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6946 - acc: 0.2028 - f1_m: 0.2878 - precision_m: 0.9498 - recall_m: 0.1712 - val_loss: 0.6917 - val_acc: 0.3404 - val_f1_m: 0.4781 - val_precision_m: 0.9597 - val_recall_m: 0.3211\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6945 - acc: 0.3670 - f1_m: 0.4588 - precision_m: 0.9563 - recall_m: 0.3548 - val_loss: 0.6931 - val_acc: 0.3140 - val_f1_m: 0.4463 - val_precision_m: 0.9568 - val_recall_m: 0.2931\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6944 - acc: 0.2099 - f1_m: 0.2956 - precision_m: 0.9439 - recall_m: 0.1779 - val_loss: 0.6936 - val_acc: 0.3100 - val_f1_m: 0.4415 - val_precision_m: 0.9560 - val_recall_m: 0.2889\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6943 - acc: 0.6859 - f1_m: 0.7677 - precision_m: 0.9517 - recall_m: 0.7074 - val_loss: 0.6961 - val_acc: 0.2832 - val_f1_m: 0.4048 - val_precision_m: 0.9568 - val_recall_m: 0.2584\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6942 - acc: 0.2226 - f1_m: 0.3133 - precision_m: 0.9479 - recall_m: 0.1954 - val_loss: 0.6958 - val_acc: 0.2880 - val_f1_m: 0.4117 - val_precision_m: 0.9572 - val_recall_m: 0.2639\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6943 - acc: 0.1716 - f1_m: 0.2374 - precision_m: 0.9467 - recall_m: 0.1375 - val_loss: 0.7046 - val_acc: 0.2296 - val_f1_m: 0.3285 - val_precision_m: 0.9538 - val_recall_m: 0.2003\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6941 - acc: 0.1987 - f1_m: 0.2808 - precision_m: 0.9471 - recall_m: 0.1670 - val_loss: 0.7022 - val_acc: 0.2388 - val_f1_m: 0.3417 - val_precision_m: 0.9562 - val_recall_m: 0.2098\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6941 - acc: 0.2034 - f1_m: 0.2860 - precision_m: 0.9449 - recall_m: 0.1704 - val_loss: 0.6981 - val_acc: 0.2652 - val_f1_m: 0.3802 - val_precision_m: 0.9545 - val_recall_m: 0.2391\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6940 - acc: 0.5875 - f1_m: 0.6669 - precision_m: 0.9516 - recall_m: 0.5962 - val_loss: 0.6989 - val_acc: 0.2590 - val_f1_m: 0.3715 - val_precision_m: 0.9553 - val_recall_m: 0.2324\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6941 - acc: 0.2152 - f1_m: 0.3014 - precision_m: 0.9500 - recall_m: 0.1858 - val_loss: 0.7013 - val_acc: 0.2414 - val_f1_m: 0.3451 - val_precision_m: 0.9569 - val_recall_m: 0.2126\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6942 - acc: 0.5462 - f1_m: 0.6331 - precision_m: 0.9514 - recall_m: 0.5543 - val_loss: 0.6954 - val_acc: 0.3012 - val_f1_m: 0.4300 - val_precision_m: 0.9556 - val_recall_m: 0.2794\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6939 - acc: 0.5252 - f1_m: 0.6015 - precision_m: 0.9375 - recall_m: 0.5257 - val_loss: 0.6998 - val_acc: 0.2502 - val_f1_m: 0.3584 - val_precision_m: 0.9557 - val_recall_m: 0.2226\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6939 - acc: 0.3795 - f1_m: 0.4589 - precision_m: 0.9531 - recall_m: 0.3663 - val_loss: 0.6967 - val_acc: 0.2832 - val_f1_m: 0.4053 - val_precision_m: 0.9564 - val_recall_m: 0.2588\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6939 - acc: 0.2279 - f1_m: 0.3010 - precision_m: 0.9402 - recall_m: 0.1997 - val_loss: 0.6950 - val_acc: 0.3338 - val_f1_m: 0.4700 - val_precision_m: 0.9583 - val_recall_m: 0.3141\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6940 - acc: 0.7045 - f1_m: 0.7787 - precision_m: 0.9423 - recall_m: 0.7230 - val_loss: 0.6989 - val_acc: 0.2529 - val_f1_m: 0.3625 - val_precision_m: 0.9561 - val_recall_m: 0.2256\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6938 - acc: 0.2653 - f1_m: 0.3442 - precision_m: 0.9404 - recall_m: 0.2400 - val_loss: 0.6953 - val_acc: 0.3347 - val_f1_m: 0.4709 - val_precision_m: 0.9597 - val_recall_m: 0.3146\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6939 - acc: 0.1885 - f1_m: 0.2605 - precision_m: 0.9469 - recall_m: 0.1616 - val_loss: 0.6942 - val_acc: 0.6662 - val_f1_m: 0.7948 - val_precision_m: 0.9526 - val_recall_m: 0.6837\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6938 - acc: 0.5400 - f1_m: 0.6203 - precision_m: 0.9470 - recall_m: 0.5496 - val_loss: 0.6895 - val_acc: 0.7062 - val_f1_m: 0.8248 - val_precision_m: 0.9487 - val_recall_m: 0.7311\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6938 - acc: 0.7741 - f1_m: 0.8475 - precision_m: 0.9456 - recall_m: 0.7991 - val_loss: 0.6959 - val_acc: 0.3127 - val_f1_m: 0.4436 - val_precision_m: 0.9576 - val_recall_m: 0.2914\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6938 - acc: 0.3102 - f1_m: 0.3846 - precision_m: 0.9479 - recall_m: 0.2959 - val_loss: 0.6928 - val_acc: 0.6860 - val_f1_m: 0.8100 - val_precision_m: 0.9495 - val_recall_m: 0.7079\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6937 - acc: 0.1882 - f1_m: 0.2517 - precision_m: 0.9458 - recall_m: 0.1564 - val_loss: 0.6980 - val_acc: 0.2581 - val_f1_m: 0.3694 - val_precision_m: 0.9586 - val_recall_m: 0.2305\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_test_df_full,y_test_deception), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5044464206313917, 0.5167255847760011, 0.23358426763590795, None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_test_df_full)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [1755  519]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting rapport\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.7612 - acc: 0.1391 - f1_m: 0.0672 - precision_m: 0.4103 - recall_m: 0.0388 - val_loss: 0.7567 - val_acc: 0.2476 - val_f1_m: 0.2669 - val_precision_m: 0.8656 - val_recall_m: 0.1605\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.7123 - acc: 0.3710 - f1_m: 0.4249 - precision_m: 0.6732 - recall_m: 0.3872 - val_loss: 0.6684 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6935 - acc: 0.8604 - f1_m: 0.9248 - precision_m: 0.8613 - recall_m: 1.0000 - val_loss: 0.6698 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.8586 - f1_m: 0.9227 - precision_m: 0.8608 - recall_m: 0.9958 - val_loss: 0.6753 - val_acc: 0.8487 - val_f1_m: 0.9166 - val_precision_m: 0.8705 - val_recall_m: 0.9691\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6925 - acc: 0.8291 - f1_m: 0.9038 - precision_m: 0.8674 - recall_m: 0.9448 - val_loss: 0.6811 - val_acc: 0.8197 - val_f1_m: 0.8988 - val_precision_m: 0.8686 - val_recall_m: 0.9327\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6923 - acc: 0.8288 - f1_m: 0.9037 - precision_m: 0.8680 - recall_m: 0.9440 - val_loss: 0.6862 - val_acc: 0.8017 - val_f1_m: 0.8867 - val_precision_m: 0.8703 - val_recall_m: 0.9052\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6922 - acc: 0.8282 - f1_m: 0.9042 - precision_m: 0.8696 - recall_m: 0.9437 - val_loss: 0.6886 - val_acc: 0.7951 - val_f1_m: 0.8821 - val_precision_m: 0.8703 - val_recall_m: 0.8957\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6920 - acc: 0.8274 - f1_m: 0.9036 - precision_m: 0.8693 - recall_m: 0.9425 - val_loss: 0.6919 - val_acc: 0.7762 - val_f1_m: 0.8696 - val_precision_m: 0.8717 - val_recall_m: 0.8694\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6918 - acc: 0.8259 - f1_m: 0.9016 - precision_m: 0.8675 - recall_m: 0.9405 - val_loss: 0.6924 - val_acc: 0.7753 - val_f1_m: 0.8688 - val_precision_m: 0.8723 - val_recall_m: 0.8674\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6917 - acc: 0.8259 - f1_m: 0.9017 - precision_m: 0.8693 - recall_m: 0.9388 - val_loss: 0.6934 - val_acc: 0.7713 - val_f1_m: 0.8662 - val_precision_m: 0.8720 - val_recall_m: 0.8623\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6915 - acc: 0.8249 - f1_m: 0.9020 - precision_m: 0.8690 - recall_m: 0.9392 - val_loss: 0.6933 - val_acc: 0.7687 - val_f1_m: 0.8643 - val_precision_m: 0.8720 - val_recall_m: 0.8588\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6913 - acc: 0.8261 - f1_m: 0.9027 - precision_m: 0.8692 - recall_m: 0.9406 - val_loss: 0.6922 - val_acc: 0.7722 - val_f1_m: 0.8665 - val_precision_m: 0.8729 - val_recall_m: 0.8624\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6911 - acc: 0.8263 - f1_m: 0.9030 - precision_m: 0.8692 - recall_m: 0.9412 - val_loss: 0.6928 - val_acc: 0.7661 - val_f1_m: 0.8624 - val_precision_m: 0.8727 - val_recall_m: 0.8543\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6907 - acc: 0.8241 - f1_m: 0.8993 - precision_m: 0.8672 - recall_m: 0.9361 - val_loss: 0.6958 - val_acc: 0.7555 - val_f1_m: 0.8547 - val_precision_m: 0.8725 - val_recall_m: 0.8397\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6902 - acc: 0.8201 - f1_m: 0.8962 - precision_m: 0.8705 - recall_m: 0.9254 - val_loss: 0.6973 - val_acc: 0.7480 - val_f1_m: 0.8493 - val_precision_m: 0.8726 - val_recall_m: 0.8298\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6896 - acc: 0.8216 - f1_m: 0.8987 - precision_m: 0.8711 - recall_m: 0.9296 - val_loss: 0.6991 - val_acc: 0.7441 - val_f1_m: 0.8464 - val_precision_m: 0.8724 - val_recall_m: 0.8248\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6888 - acc: 0.8179 - f1_m: 0.8950 - precision_m: 0.8722 - recall_m: 0.9220 - val_loss: 0.7005 - val_acc: 0.7436 - val_f1_m: 0.8461 - val_precision_m: 0.8728 - val_recall_m: 0.8238\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6881 - acc: 0.8219 - f1_m: 0.8997 - precision_m: 0.8729 - recall_m: 0.9299 - val_loss: 0.6999 - val_acc: 0.7405 - val_f1_m: 0.8438 - val_precision_m: 0.8727 - val_recall_m: 0.8199\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6872 - acc: 0.8225 - f1_m: 0.8992 - precision_m: 0.8729 - recall_m: 0.9289 - val_loss: 0.6996 - val_acc: 0.7436 - val_f1_m: 0.8464 - val_precision_m: 0.8711 - val_recall_m: 0.8257\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6865 - acc: 0.8239 - f1_m: 0.9007 - precision_m: 0.8730 - recall_m: 0.9323 - val_loss: 0.6997 - val_acc: 0.7493 - val_f1_m: 0.8507 - val_precision_m: 0.8714 - val_recall_m: 0.8338\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6852 - acc: 0.8233 - f1_m: 0.9003 - precision_m: 0.8742 - recall_m: 0.9301 - val_loss: 0.7035 - val_acc: 0.7230 - val_f1_m: 0.8313 - val_precision_m: 0.8697 - val_recall_m: 0.7990\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6782 - acc: 0.8209 - f1_m: 0.8960 - precision_m: 0.8962 - recall_m: 0.8974 - val_loss: 0.7139 - val_acc: 0.7036 - val_f1_m: 0.8176 - val_precision_m: 0.8687 - val_recall_m: 0.7746\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6688 - acc: 0.8213 - f1_m: 0.8958 - precision_m: 0.8990 - recall_m: 0.8944 - val_loss: 0.7311 - val_acc: 0.6948 - val_f1_m: 0.8096 - val_precision_m: 0.8728 - val_recall_m: 0.7570\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6555 - acc: 0.8290 - f1_m: 0.8988 - precision_m: 0.9148 - recall_m: 0.8850 - val_loss: 0.7407 - val_acc: 0.6917 - val_f1_m: 0.8076 - val_precision_m: 0.8703 - val_recall_m: 0.7554\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6445 - acc: 0.8301 - f1_m: 0.8994 - precision_m: 0.9150 - recall_m: 0.8858 - val_loss: 0.7427 - val_acc: 0.6970 - val_f1_m: 0.8119 - val_precision_m: 0.8700 - val_recall_m: 0.7630\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6357 - acc: 0.8310 - f1_m: 0.9000 - precision_m: 0.9150 - recall_m: 0.8870 - val_loss: 0.7395 - val_acc: 0.7005 - val_f1_m: 0.8151 - val_precision_m: 0.8697 - val_recall_m: 0.7691\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6287 - acc: 0.8319 - f1_m: 0.8986 - precision_m: 0.9126 - recall_m: 0.8864 - val_loss: 0.7319 - val_acc: 0.7058 - val_f1_m: 0.8190 - val_precision_m: 0.8696 - val_recall_m: 0.7759\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6232 - acc: 0.8323 - f1_m: 0.9009 - precision_m: 0.9148 - recall_m: 0.8891 - val_loss: 0.7243 - val_acc: 0.7102 - val_f1_m: 0.8225 - val_precision_m: 0.8689 - val_recall_m: 0.7824\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6181 - acc: 0.8326 - f1_m: 0.9000 - precision_m: 0.9147 - recall_m: 0.8875 - val_loss: 0.7155 - val_acc: 0.7133 - val_f1_m: 0.8247 - val_precision_m: 0.8685 - val_recall_m: 0.7868\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6139 - acc: 0.8333 - f1_m: 0.9008 - precision_m: 0.9153 - recall_m: 0.8887 - val_loss: 0.7036 - val_acc: 0.7172 - val_f1_m: 0.8281 - val_precision_m: 0.8682 - val_recall_m: 0.7934\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6102 - acc: 0.8335 - f1_m: 0.9010 - precision_m: 0.9154 - recall_m: 0.8891 - val_loss: 0.6912 - val_acc: 0.7186 - val_f1_m: 0.8297 - val_precision_m: 0.8670 - val_recall_m: 0.7974\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6071 - acc: 0.8337 - f1_m: 0.9020 - precision_m: 0.9152 - recall_m: 0.8909 - val_loss: 0.6859 - val_acc: 0.7238 - val_f1_m: 0.8334 - val_precision_m: 0.8673 - val_recall_m: 0.8038\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_test_df,y_test_rapport), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.495683198686282, 0.49396743465654086, 0.49099810666545, None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_test_df)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [ 443 1831]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted against Throughput, WorkTime, PC Agreement & Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train weighted encodings\n",
    "pred_df_full_throughput, pred_df_throughput, pred_df_full_worktime, pred_df_worktime, pred_df_full_agreement, pred_df_agreement, pred_df_full_textlength, pred_df_textlength, pred_df_full_special, pred_df_special = metadata_options.construct_weighted_dataframe(indices_train, df_throughput, df_worktime, df_agreement, df_textlength, df_special, pred_df, pred_df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weighted encodings\n",
    "pred_df_full_throughput_test, pred_df_throughput_test, pred_df_full_worktime_test, pred_df_worktime_test, pred_df_full_agreement_test, pred_df_agreement_test, pred_df_full_textlength_test, pred_df_textlength_test, pred_df_full_special_test, pred_df_special_test = metadata_options.construct_weighted_dataframe(indices_test, df_throughput, df_worktime, df_agreement, df_textlength, df_special, pred_test_df, pred_test_df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model with one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting deception, weighted by throughput\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6937 - acc: 0.3471 - f1_m: 0.4404 - precision_m: 0.7046 - recall_m: 0.3328 - val_loss: 0.7066 - val_acc: 0.3135 - val_f1_m: 0.4449 - val_precision_m: 0.9532 - val_recall_m: 0.2932\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6935 - acc: 0.3069 - f1_m: 0.4073 - precision_m: 0.8079 - recall_m: 0.2864 - val_loss: 0.6993 - val_acc: 0.5756 - val_f1_m: 0.7215 - val_precision_m: 0.9532 - val_recall_m: 0.5839\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6935 - acc: 0.5930 - f1_m: 0.7226 - precision_m: 0.9477 - recall_m: 0.6051 - val_loss: 0.7015 - val_acc: 0.4591 - val_f1_m: 0.6126 - val_precision_m: 0.9513 - val_recall_m: 0.4551\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6934 - acc: 0.3852 - f1_m: 0.5277 - precision_m: 0.9540 - recall_m: 0.3749 - val_loss: 0.7016 - val_acc: 0.4406 - val_f1_m: 0.5934 - val_precision_m: 0.9510 - val_recall_m: 0.4343\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.6163 - f1_m: 0.7519 - precision_m: 0.9533 - recall_m: 0.6291 - val_loss: 0.6969 - val_acc: 0.5770 - val_f1_m: 0.7221 - val_precision_m: 0.9519 - val_recall_m: 0.5855\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.5895 - f1_m: 0.7294 - precision_m: 0.9523 - recall_m: 0.5968 - val_loss: 0.6981 - val_acc: 0.5409 - val_f1_m: 0.6902 - val_precision_m: 0.9526 - val_recall_m: 0.5447\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.6423 - f1_m: 0.7730 - precision_m: 0.9533 - recall_m: 0.6591 - val_loss: 0.6957 - val_acc: 0.6038 - val_f1_m: 0.7446 - val_precision_m: 0.9521 - val_recall_m: 0.6148\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.6381 - f1_m: 0.7705 - precision_m: 0.9523 - recall_m: 0.6557 - val_loss: 0.6966 - val_acc: 0.5576 - val_f1_m: 0.7045 - val_precision_m: 0.9523 - val_recall_m: 0.5629\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.4740 - f1_m: 0.6214 - precision_m: 0.9523 - recall_m: 0.4724 - val_loss: 0.6957 - val_acc: 0.5787 - val_f1_m: 0.7233 - val_precision_m: 0.9515 - val_recall_m: 0.5873\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.5010 - f1_m: 0.6521 - precision_m: 0.9537 - recall_m: 0.5029 - val_loss: 0.6977 - val_acc: 0.5462 - val_f1_m: 0.6946 - val_precision_m: 0.9522 - val_recall_m: 0.5506\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.5639 - f1_m: 0.7068 - precision_m: 0.9535 - recall_m: 0.5726 - val_loss: 0.6977 - val_acc: 0.5558 - val_f1_m: 0.7034 - val_precision_m: 0.9529 - val_recall_m: 0.5613\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.5312 - f1_m: 0.6723 - precision_m: 0.9538 - recall_m: 0.5342 - val_loss: 0.6971 - val_acc: 0.5536 - val_f1_m: 0.7015 - val_precision_m: 0.9530 - val_recall_m: 0.5587\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.6347 - f1_m: 0.7695 - precision_m: 0.9532 - recall_m: 0.6494 - val_loss: 0.6964 - val_acc: 0.5633 - val_f1_m: 0.7103 - val_precision_m: 0.9515 - val_recall_m: 0.5706\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.5729 - f1_m: 0.7030 - precision_m: 0.9556 - recall_m: 0.5777 - val_loss: 0.6999 - val_acc: 0.4626 - val_f1_m: 0.6159 - val_precision_m: 0.9520 - val_recall_m: 0.4584\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.6335 - f1_m: 0.7676 - precision_m: 0.9527 - recall_m: 0.6464 - val_loss: 0.6953 - val_acc: 0.5721 - val_f1_m: 0.7175 - val_precision_m: 0.9523 - val_recall_m: 0.5793\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.5861 - f1_m: 0.7282 - precision_m: 0.9538 - recall_m: 0.5940 - val_loss: 0.6944 - val_acc: 0.5954 - val_f1_m: 0.7383 - val_precision_m: 0.9510 - val_recall_m: 0.6070\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.7310 - f1_m: 0.8418 - precision_m: 0.9525 - recall_m: 0.7576 - val_loss: 0.6929 - val_acc: 0.6262 - val_f1_m: 0.7628 - val_precision_m: 0.9529 - val_recall_m: 0.6398\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.6364 - f1_m: 0.7650 - precision_m: 0.9533 - recall_m: 0.6491 - val_loss: 0.6983 - val_acc: 0.4943 - val_f1_m: 0.6480 - val_precision_m: 0.9502 - val_recall_m: 0.4952\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.5990 - f1_m: 0.7374 - precision_m: 0.9532 - recall_m: 0.6104 - val_loss: 0.6971 - val_acc: 0.5242 - val_f1_m: 0.6758 - val_precision_m: 0.9497 - val_recall_m: 0.5279\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.6402 - f1_m: 0.7683 - precision_m: 0.9527 - recall_m: 0.6529 - val_loss: 0.6942 - val_acc: 0.5880 - val_f1_m: 0.7312 - val_precision_m: 0.9527 - val_recall_m: 0.5969\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.5339 - f1_m: 0.6801 - precision_m: 0.9522 - recall_m: 0.5370 - val_loss: 0.6970 - val_acc: 0.5347 - val_f1_m: 0.6847 - val_precision_m: 0.9498 - val_recall_m: 0.5393\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.6016 - f1_m: 0.7404 - precision_m: 0.9521 - recall_m: 0.6119 - val_loss: 0.6939 - val_acc: 0.5954 - val_f1_m: 0.7377 - val_precision_m: 0.9524 - val_recall_m: 0.6054\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.6659 - f1_m: 0.7906 - precision_m: 0.9521 - recall_m: 0.6813 - val_loss: 0.6948 - val_acc: 0.5831 - val_f1_m: 0.7271 - val_precision_m: 0.9529 - val_recall_m: 0.5913\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.4431 - f1_m: 0.5943 - precision_m: 0.9546 - recall_m: 0.4360 - val_loss: 0.7000 - val_acc: 0.4688 - val_f1_m: 0.6228 - val_precision_m: 0.9497 - val_recall_m: 0.4663\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.5616 - f1_m: 0.7092 - precision_m: 0.9529 - recall_m: 0.5704 - val_loss: 0.6978 - val_acc: 0.5097 - val_f1_m: 0.6632 - val_precision_m: 0.9488 - val_recall_m: 0.5129\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.4343 - f1_m: 0.5877 - precision_m: 0.9532 - recall_m: 0.4290 - val_loss: 0.6989 - val_acc: 0.4982 - val_f1_m: 0.6521 - val_precision_m: 0.9504 - val_recall_m: 0.4991\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.5118 - f1_m: 0.6614 - precision_m: 0.9542 - recall_m: 0.5101 - val_loss: 0.6994 - val_acc: 0.4859 - val_f1_m: 0.6404 - val_precision_m: 0.9498 - val_recall_m: 0.4858\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.4976 - f1_m: 0.6506 - precision_m: 0.9535 - recall_m: 0.4986 - val_loss: 0.6977 - val_acc: 0.5189 - val_f1_m: 0.6717 - val_precision_m: 0.9501 - val_recall_m: 0.5226\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.5806 - f1_m: 0.7254 - precision_m: 0.9532 - recall_m: 0.5889 - val_loss: 0.6918 - val_acc: 0.6324 - val_f1_m: 0.7684 - val_precision_m: 0.9531 - val_recall_m: 0.6465\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.6495 - f1_m: 0.7815 - precision_m: 0.9528 - recall_m: 0.6655 - val_loss: 0.6920 - val_acc: 0.6341 - val_f1_m: 0.7697 - val_precision_m: 0.9532 - val_recall_m: 0.6487\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.6567 - f1_m: 0.7881 - precision_m: 0.9528 - recall_m: 0.6753 - val_loss: 0.6907 - val_acc: 0.6592 - val_f1_m: 0.7888 - val_precision_m: 0.9542 - val_recall_m: 0.6755\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.6850 - f1_m: 0.8053 - precision_m: 0.9519 - recall_m: 0.7019 - val_loss: 0.6917 - val_acc: 0.6394 - val_f1_m: 0.7740 - val_precision_m: 0.9535 - val_recall_m: 0.6546\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by throughput')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_throughput)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_throughput, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_throughput_test,y_test_deception), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5037672748315065, 0.5182825095810757, 0.43488586069076374, None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_full_throughput_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [ 794 1480]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rapport = np.asarray(y_test_rapport)\n",
    "y_train_rapport = np.asarray(y_train_rapport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting rapport, weighted by throughput\n",
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6877 - acc: 0.6453 - f1_m: 0.6931 - precision_m: 0.8118 - recall_m: 0.6755 - val_loss: 0.6503 - val_acc: 0.7010 - val_f1_m: 0.8169 - val_precision_m: 0.8647 - val_recall_m: 0.7764\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6703 - acc: 0.7432 - f1_m: 0.8391 - precision_m: 0.9049 - recall_m: 0.7848 - val_loss: 0.6336 - val_acc: 0.6781 - val_f1_m: 0.7991 - val_precision_m: 0.8655 - val_recall_m: 0.7446\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6605 - acc: 0.7198 - f1_m: 0.8189 - precision_m: 0.9150 - recall_m: 0.7437 - val_loss: 0.6339 - val_acc: 0.6350 - val_f1_m: 0.7648 - val_precision_m: 0.8635 - val_recall_m: 0.6896\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6527 - acc: 0.6884 - f1_m: 0.7918 - precision_m: 0.9168 - recall_m: 0.6999 - val_loss: 0.6399 - val_acc: 0.6069 - val_f1_m: 0.7409 - val_precision_m: 0.8626 - val_recall_m: 0.6526\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6474 - acc: 0.6615 - f1_m: 0.7688 - precision_m: 0.9211 - recall_m: 0.6625 - val_loss: 0.6440 - val_acc: 0.5923 - val_f1_m: 0.7278 - val_precision_m: 0.8618 - val_recall_m: 0.6330\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6434 - acc: 0.6615 - f1_m: 0.7700 - precision_m: 0.9211 - recall_m: 0.6648 - val_loss: 0.6537 - val_acc: 0.5761 - val_f1_m: 0.7118 - val_precision_m: 0.8623 - val_recall_m: 0.6096\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6397 - acc: 0.6577 - f1_m: 0.7645 - precision_m: 0.9208 - recall_m: 0.6571 - val_loss: 0.6607 - val_acc: 0.5655 - val_f1_m: 0.7015 - val_precision_m: 0.8650 - val_recall_m: 0.5937\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6354 - acc: 0.6538 - f1_m: 0.7612 - precision_m: 0.9208 - recall_m: 0.6520 - val_loss: 0.6768 - val_acc: 0.5541 - val_f1_m: 0.6907 - val_precision_m: 0.8650 - val_recall_m: 0.5788\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6267 - acc: 0.6526 - f1_m: 0.7626 - precision_m: 0.9239 - recall_m: 0.6522 - val_loss: 0.6895 - val_acc: 0.5660 - val_f1_m: 0.7016 - val_precision_m: 0.8670 - val_recall_m: 0.5928\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6226 - acc: 0.6668 - f1_m: 0.7748 - precision_m: 0.9251 - recall_m: 0.6696 - val_loss: 0.6908 - val_acc: 0.5748 - val_f1_m: 0.7100 - val_precision_m: 0.8667 - val_recall_m: 0.6048\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6206 - acc: 0.6706 - f1_m: 0.7758 - precision_m: 0.9239 - recall_m: 0.6719 - val_loss: 0.6626 - val_acc: 0.6042 - val_f1_m: 0.7381 - val_precision_m: 0.8654 - val_recall_m: 0.6471\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6189 - acc: 0.6827 - f1_m: 0.7859 - precision_m: 0.9249 - recall_m: 0.6861 - val_loss: 0.6910 - val_acc: 0.5871 - val_f1_m: 0.7216 - val_precision_m: 0.8673 - val_recall_m: 0.6212\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6181 - acc: 0.6775 - f1_m: 0.7797 - precision_m: 0.9252 - recall_m: 0.6777 - val_loss: 0.6668 - val_acc: 0.6091 - val_f1_m: 0.7421 - val_precision_m: 0.8662 - val_recall_m: 0.6526\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6173 - acc: 0.6896 - f1_m: 0.7934 - precision_m: 0.9251 - recall_m: 0.6979 - val_loss: 0.6888 - val_acc: 0.5950 - val_f1_m: 0.7297 - val_precision_m: 0.8658 - val_recall_m: 0.6343\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6165 - acc: 0.6861 - f1_m: 0.7909 - precision_m: 0.9245 - recall_m: 0.6943 - val_loss: 0.6690 - val_acc: 0.6139 - val_f1_m: 0.7467 - val_precision_m: 0.8653 - val_recall_m: 0.6601\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6160 - acc: 0.6930 - f1_m: 0.7961 - precision_m: 0.9235 - recall_m: 0.7031 - val_loss: 0.6768 - val_acc: 0.6104 - val_f1_m: 0.7431 - val_precision_m: 0.8669 - val_recall_m: 0.6536\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6156 - acc: 0.6884 - f1_m: 0.7918 - precision_m: 0.9248 - recall_m: 0.6951 - val_loss: 0.6730 - val_acc: 0.6161 - val_f1_m: 0.7483 - val_precision_m: 0.8662 - val_recall_m: 0.6621\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6145 - acc: 0.6930 - f1_m: 0.7944 - precision_m: 0.9212 - recall_m: 0.7025 - val_loss: 0.6882 - val_acc: 0.6003 - val_f1_m: 0.7337 - val_precision_m: 0.8679 - val_recall_m: 0.6388\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6140 - acc: 0.6849 - f1_m: 0.7873 - precision_m: 0.9246 - recall_m: 0.6907 - val_loss: 0.6919 - val_acc: 0.5963 - val_f1_m: 0.7300 - val_precision_m: 0.8681 - val_recall_m: 0.6331\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6131 - acc: 0.6819 - f1_m: 0.7869 - precision_m: 0.9289 - recall_m: 0.6854 - val_loss: 0.6850 - val_acc: 0.6007 - val_f1_m: 0.7344 - val_precision_m: 0.8670 - val_recall_m: 0.6401\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6120 - acc: 0.6866 - f1_m: 0.7887 - precision_m: 0.9286 - recall_m: 0.6887 - val_loss: 0.6775 - val_acc: 0.6060 - val_f1_m: 0.7390 - val_precision_m: 0.8671 - val_recall_m: 0.6471\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6117 - acc: 0.6896 - f1_m: 0.7912 - precision_m: 0.9279 - recall_m: 0.6927 - val_loss: 0.6792 - val_acc: 0.6069 - val_f1_m: 0.7395 - val_precision_m: 0.8682 - val_recall_m: 0.6470\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6110 - acc: 0.6870 - f1_m: 0.7897 - precision_m: 0.9294 - recall_m: 0.6888 - val_loss: 0.6803 - val_acc: 0.6025 - val_f1_m: 0.7358 - val_precision_m: 0.8679 - val_recall_m: 0.6415\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6105 - acc: 0.6881 - f1_m: 0.7915 - precision_m: 0.9295 - recall_m: 0.6920 - val_loss: 0.6919 - val_acc: 0.5906 - val_f1_m: 0.7251 - val_precision_m: 0.8680 - val_recall_m: 0.6255\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6102 - acc: 0.6838 - f1_m: 0.7865 - precision_m: 0.9299 - recall_m: 0.6836 - val_loss: 0.6813 - val_acc: 0.6003 - val_f1_m: 0.7338 - val_precision_m: 0.8680 - val_recall_m: 0.6385\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6095 - acc: 0.6876 - f1_m: 0.7889 - precision_m: 0.9306 - recall_m: 0.6872 - val_loss: 0.6981 - val_acc: 0.5818 - val_f1_m: 0.7171 - val_precision_m: 0.8661 - val_recall_m: 0.6149\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6094 - acc: 0.6811 - f1_m: 0.7816 - precision_m: 0.9326 - recall_m: 0.6772 - val_loss: 0.6789 - val_acc: 0.6007 - val_f1_m: 0.7347 - val_precision_m: 0.8665 - val_recall_m: 0.6405\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6088 - acc: 0.6847 - f1_m: 0.7861 - precision_m: 0.9321 - recall_m: 0.6825 - val_loss: 0.6796 - val_acc: 0.5981 - val_f1_m: 0.7316 - val_precision_m: 0.8673 - val_recall_m: 0.6354\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6088 - acc: 0.6855 - f1_m: 0.7876 - precision_m: 0.9298 - recall_m: 0.6868 - val_loss: 0.6819 - val_acc: 0.5959 - val_f1_m: 0.7299 - val_precision_m: 0.8670 - val_recall_m: 0.6329\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6084 - acc: 0.6806 - f1_m: 0.7837 - precision_m: 0.9339 - recall_m: 0.6779 - val_loss: 0.6847 - val_acc: 0.5945 - val_f1_m: 0.7290 - val_precision_m: 0.8662 - val_recall_m: 0.6320\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6079 - acc: 0.6854 - f1_m: 0.7844 - precision_m: 0.9334 - recall_m: 0.6802 - val_loss: 0.6799 - val_acc: 0.5959 - val_f1_m: 0.7304 - val_precision_m: 0.8653 - val_recall_m: 0.6344\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6075 - acc: 0.6830 - f1_m: 0.7867 - precision_m: 0.9341 - recall_m: 0.6823 - val_loss: 0.6702 - val_acc: 0.6033 - val_f1_m: 0.7367 - val_precision_m: 0.8657 - val_recall_m: 0.6438\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by throughput')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_throughput)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_throughput, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64,\n",
    "                               validation_data=(pred_df_throughput_test,y_test_rapport), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49377715958087154, 0.48733617833582854, 0.4578547504461605, None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_throughput_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [ 803 1471]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WorkTime only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model with one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting deception, weighted by worktime\n",
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.1421 - f1_m: 0.1672 - precision_m: 0.5142 - recall_m: 0.1029 - val_loss: 0.7042 - val_acc: 0.2823 - val_f1_m: 0.3982 - val_precision_m: 0.9627 - val_recall_m: 0.2547\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.5081 - f1_m: 0.6502 - precision_m: 0.9526 - recall_m: 0.5067 - val_loss: 0.6986 - val_acc: 0.4763 - val_f1_m: 0.6279 - val_precision_m: 0.9531 - val_recall_m: 0.4706\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.5304 - f1_m: 0.6662 - precision_m: 0.9532 - recall_m: 0.5321 - val_loss: 0.6967 - val_acc: 0.5440 - val_f1_m: 0.6907 - val_precision_m: 0.9535 - val_recall_m: 0.5439\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.5878 - f1_m: 0.7297 - precision_m: 0.9527 - recall_m: 0.5963 - val_loss: 0.6924 - val_acc: 0.6759 - val_f1_m: 0.8003 - val_precision_m: 0.9522 - val_recall_m: 0.6919\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.7742 - f1_m: 0.8657 - precision_m: 0.9535 - recall_m: 0.7988 - val_loss: 0.6918 - val_acc: 0.7155 - val_f1_m: 0.8287 - val_precision_m: 0.9519 - val_recall_m: 0.7354\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.7029 - f1_m: 0.8186 - precision_m: 0.9535 - recall_m: 0.7238 - val_loss: 0.6918 - val_acc: 0.7093 - val_f1_m: 0.8241 - val_precision_m: 0.9519 - val_recall_m: 0.7284\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.6979 - f1_m: 0.8138 - precision_m: 0.9516 - recall_m: 0.7164 - val_loss: 0.6927 - val_acc: 0.6724 - val_f1_m: 0.7975 - val_precision_m: 0.9524 - val_recall_m: 0.6878\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.7554 - f1_m: 0.8582 - precision_m: 0.9525 - recall_m: 0.7838 - val_loss: 0.6913 - val_acc: 0.7181 - val_f1_m: 0.8306 - val_precision_m: 0.9516 - val_recall_m: 0.7385\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.6634 - f1_m: 0.7894 - precision_m: 0.9527 - recall_m: 0.6831 - val_loss: 0.6906 - val_acc: 0.7208 - val_f1_m: 0.8324 - val_precision_m: 0.9517 - val_recall_m: 0.7413\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.6622 - f1_m: 0.7904 - precision_m: 0.9521 - recall_m: 0.6802 - val_loss: 0.6895 - val_acc: 0.7397 - val_f1_m: 0.8456 - val_precision_m: 0.9530 - val_recall_m: 0.7618\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.6830 - f1_m: 0.8077 - precision_m: 0.9518 - recall_m: 0.7049 - val_loss: 0.6898 - val_acc: 0.7436 - val_f1_m: 0.8482 - val_precision_m: 0.9532 - val_recall_m: 0.7659\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.7679 - f1_m: 0.8655 - precision_m: 0.9511 - recall_m: 0.7977 - val_loss: 0.6902 - val_acc: 0.7278 - val_f1_m: 0.8374 - val_precision_m: 0.9522 - val_recall_m: 0.7490\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.6425 - f1_m: 0.7735 - precision_m: 0.9523 - recall_m: 0.6606 - val_loss: 0.6974 - val_acc: 0.5651 - val_f1_m: 0.7097 - val_precision_m: 0.9533 - val_recall_m: 0.5676\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.3678 - f1_m: 0.4915 - precision_m: 0.9505 - recall_m: 0.3562 - val_loss: 0.7013 - val_acc: 0.4134 - val_f1_m: 0.5627 - val_precision_m: 0.9527 - val_recall_m: 0.4024\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.5432 - f1_m: 0.6801 - precision_m: 0.9524 - recall_m: 0.5457 - val_loss: 0.6924 - val_acc: 0.6829 - val_f1_m: 0.8054 - val_precision_m: 0.9527 - val_recall_m: 0.6993\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.4567 - f1_m: 0.5814 - precision_m: 0.9528 - recall_m: 0.4514 - val_loss: 0.6884 - val_acc: 0.7467 - val_f1_m: 0.8505 - val_precision_m: 0.9524 - val_recall_m: 0.7700\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.7500 - f1_m: 0.8533 - precision_m: 0.9532 - recall_m: 0.7750 - val_loss: 0.6896 - val_acc: 0.7419 - val_f1_m: 0.8470 - val_precision_m: 0.9531 - val_recall_m: 0.7640\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.7665 - f1_m: 0.8654 - precision_m: 0.9524 - recall_m: 0.7952 - val_loss: 0.6875 - val_acc: 0.7696 - val_f1_m: 0.8661 - val_precision_m: 0.9509 - val_recall_m: 0.7970\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.6064 - f1_m: 0.7413 - precision_m: 0.9549 - recall_m: 0.6180 - val_loss: 0.6859 - val_acc: 0.7960 - val_f1_m: 0.8832 - val_precision_m: 0.9502 - val_recall_m: 0.8266\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.7269 - f1_m: 0.8337 - precision_m: 0.9528 - recall_m: 0.7514 - val_loss: 0.6856 - val_acc: 0.8096 - val_f1_m: 0.8922 - val_precision_m: 0.9502 - val_recall_m: 0.8422\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.8192 - f1_m: 0.8979 - precision_m: 0.9529 - recall_m: 0.8508 - val_loss: 0.6862 - val_acc: 0.7788 - val_f1_m: 0.8722 - val_precision_m: 0.9506 - val_recall_m: 0.8074\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6927 - acc: 0.6555 - f1_m: 0.7844 - precision_m: 0.9537 - recall_m: 0.6698 - val_loss: 0.6941 - val_acc: 0.6592 - val_f1_m: 0.7875 - val_precision_m: 0.9528 - val_recall_m: 0.6728\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.6420 - f1_m: 0.7729 - precision_m: 0.9497 - recall_m: 0.6543 - val_loss: 0.6937 - val_acc: 0.6667 - val_f1_m: 0.7930 - val_precision_m: 0.9527 - val_recall_m: 0.6810\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.7062 - f1_m: 0.8239 - precision_m: 0.9532 - recall_m: 0.7296 - val_loss: 0.6946 - val_acc: 0.6675 - val_f1_m: 0.7936 - val_precision_m: 0.9527 - val_recall_m: 0.6819\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.3528 - f1_m: 0.4714 - precision_m: 0.9328 - recall_m: 0.3362 - val_loss: 0.7003 - val_acc: 0.5330 - val_f1_m: 0.6814 - val_precision_m: 0.9517 - val_recall_m: 0.5330\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.6314 - f1_m: 0.7637 - precision_m: 0.9527 - recall_m: 0.6425 - val_loss: 0.6967 - val_acc: 0.6227 - val_f1_m: 0.7586 - val_precision_m: 0.9537 - val_recall_m: 0.6318\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.5290 - f1_m: 0.6709 - precision_m: 0.9475 - recall_m: 0.5271 - val_loss: 0.6943 - val_acc: 0.6715 - val_f1_m: 0.7968 - val_precision_m: 0.9531 - val_recall_m: 0.6865\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6927 - acc: 0.6596 - f1_m: 0.7828 - precision_m: 0.9459 - recall_m: 0.6717 - val_loss: 0.6934 - val_acc: 0.6829 - val_f1_m: 0.8056 - val_precision_m: 0.9521 - val_recall_m: 0.6997\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.5320 - f1_m: 0.6675 - precision_m: 0.9535 - recall_m: 0.5366 - val_loss: 0.6962 - val_acc: 0.6618 - val_f1_m: 0.7891 - val_precision_m: 0.9530 - val_recall_m: 0.6756\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.6586 - f1_m: 0.7813 - precision_m: 0.9498 - recall_m: 0.6724 - val_loss: 0.6959 - val_acc: 0.6574 - val_f1_m: 0.7858 - val_precision_m: 0.9534 - val_recall_m: 0.6705\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.4317 - f1_m: 0.5686 - precision_m: 0.9515 - recall_m: 0.4269 - val_loss: 0.6960 - val_acc: 0.6834 - val_f1_m: 0.8053 - val_precision_m: 0.9528 - val_recall_m: 0.6993\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.6520 - f1_m: 0.7837 - precision_m: 0.9530 - recall_m: 0.6695 - val_loss: 0.6914 - val_acc: 0.7485 - val_f1_m: 0.8517 - val_precision_m: 0.9515 - val_recall_m: 0.7728\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by worktime')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_worktime)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_worktime, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_worktime_test,y_test_deception), \n",
    "#                                callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5043590205890889, 0.516374553984406, 0.47303531928340503, None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_full_worktime_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [ 518 1756]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting rapport, weighted by worktime\n",
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6815 - acc: 0.7901 - f1_m: 0.8736 - precision_m: 0.9038 - recall_m: 0.8499 - val_loss: 0.6508 - val_acc: 0.7867 - val_f1_m: 0.8770 - val_precision_m: 0.8694 - val_recall_m: 0.8861\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6783 - acc: 0.7802 - f1_m: 0.8648 - precision_m: 0.9058 - recall_m: 0.8332 - val_loss: 0.6552 - val_acc: 0.6473 - val_f1_m: 0.7744 - val_precision_m: 0.8663 - val_recall_m: 0.7023\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6755 - acc: 0.7166 - f1_m: 0.8117 - precision_m: 0.9039 - recall_m: 0.7397 - val_loss: 0.6577 - val_acc: 0.5897 - val_f1_m: 0.7248 - val_precision_m: 0.8639 - val_recall_m: 0.6275\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6734 - acc: 0.6586 - f1_m: 0.7679 - precision_m: 0.9175 - recall_m: 0.6635 - val_loss: 0.6593 - val_acc: 0.5611 - val_f1_m: 0.6982 - val_precision_m: 0.8641 - val_recall_m: 0.5883\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6716 - acc: 0.6524 - f1_m: 0.7640 - precision_m: 0.9183 - recall_m: 0.6573 - val_loss: 0.6614 - val_acc: 0.5383 - val_f1_m: 0.6750 - val_precision_m: 0.8641 - val_recall_m: 0.5565\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6706 - acc: 0.6315 - f1_m: 0.7421 - precision_m: 0.9203 - recall_m: 0.6262 - val_loss: 0.6589 - val_acc: 0.5431 - val_f1_m: 0.6798 - val_precision_m: 0.8642 - val_recall_m: 0.5629\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6695 - acc: 0.6145 - f1_m: 0.7287 - precision_m: 0.9226 - recall_m: 0.6062 - val_loss: 0.6615 - val_acc: 0.5295 - val_f1_m: 0.6656 - val_precision_m: 0.8655 - val_recall_m: 0.5436\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6684 - acc: 0.5902 - f1_m: 0.7047 - precision_m: 0.9205 - recall_m: 0.5752 - val_loss: 0.6600 - val_acc: 0.5317 - val_f1_m: 0.6680 - val_precision_m: 0.8657 - val_recall_m: 0.5466\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6680 - acc: 0.5941 - f1_m: 0.7015 - precision_m: 0.9139 - recall_m: 0.5738 - val_loss: 0.6741 - val_acc: 0.4828 - val_f1_m: 0.6156 - val_precision_m: 0.8654 - val_recall_m: 0.4805\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6675 - acc: 0.5583 - f1_m: 0.6696 - precision_m: 0.9260 - recall_m: 0.5282 - val_loss: 0.6703 - val_acc: 0.4965 - val_f1_m: 0.6308 - val_precision_m: 0.8645 - val_recall_m: 0.4994\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6671 - acc: 0.5542 - f1_m: 0.6689 - precision_m: 0.9244 - recall_m: 0.5275 - val_loss: 0.6706 - val_acc: 0.4960 - val_f1_m: 0.6305 - val_precision_m: 0.8644 - val_recall_m: 0.4989\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6669 - acc: 0.5763 - f1_m: 0.6881 - precision_m: 0.9238 - recall_m: 0.5521 - val_loss: 0.6790 - val_acc: 0.4697 - val_f1_m: 0.6004 - val_precision_m: 0.8675 - val_recall_m: 0.4620\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6664 - acc: 0.5552 - f1_m: 0.6666 - precision_m: 0.9214 - recall_m: 0.5250 - val_loss: 0.6788 - val_acc: 0.4710 - val_f1_m: 0.6020 - val_precision_m: 0.8668 - val_recall_m: 0.4640\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6662 - acc: 0.5566 - f1_m: 0.6680 - precision_m: 0.9230 - recall_m: 0.5277 - val_loss: 0.6822 - val_acc: 0.4639 - val_f1_m: 0.5942 - val_precision_m: 0.8662 - val_recall_m: 0.4551\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6659 - acc: 0.5570 - f1_m: 0.6646 - precision_m: 0.9189 - recall_m: 0.5241 - val_loss: 0.6843 - val_acc: 0.4578 - val_f1_m: 0.5873 - val_precision_m: 0.8650 - val_recall_m: 0.4477\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6660 - acc: 0.5510 - f1_m: 0.6653 - precision_m: 0.9252 - recall_m: 0.5227 - val_loss: 0.6902 - val_acc: 0.4468 - val_f1_m: 0.5742 - val_precision_m: 0.8656 - val_recall_m: 0.4323\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6657 - acc: 0.5570 - f1_m: 0.6683 - precision_m: 0.9232 - recall_m: 0.5273 - val_loss: 0.6722 - val_acc: 0.4890 - val_f1_m: 0.6230 - val_precision_m: 0.8637 - val_recall_m: 0.4898\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6654 - acc: 0.5460 - f1_m: 0.6593 - precision_m: 0.9265 - recall_m: 0.5157 - val_loss: 0.6766 - val_acc: 0.4754 - val_f1_m: 0.6067 - val_precision_m: 0.8672 - val_recall_m: 0.4694\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6652 - acc: 0.5706 - f1_m: 0.6818 - precision_m: 0.9237 - recall_m: 0.5448 - val_loss: 0.6887 - val_acc: 0.4525 - val_f1_m: 0.5814 - val_precision_m: 0.8640 - val_recall_m: 0.4412\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6650 - acc: 0.5318 - f1_m: 0.6451 - precision_m: 0.9261 - recall_m: 0.4991 - val_loss: 0.6774 - val_acc: 0.4745 - val_f1_m: 0.6057 - val_precision_m: 0.8668 - val_recall_m: 0.4684\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6648 - acc: 0.5594 - f1_m: 0.6701 - precision_m: 0.9254 - recall_m: 0.5294 - val_loss: 0.6853 - val_acc: 0.4604 - val_f1_m: 0.5903 - val_precision_m: 0.8646 - val_recall_m: 0.4511\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6646 - acc: 0.5502 - f1_m: 0.6637 - precision_m: 0.9252 - recall_m: 0.5204 - val_loss: 0.6785 - val_acc: 0.4732 - val_f1_m: 0.6043 - val_precision_m: 0.8662 - val_recall_m: 0.4668\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6645 - acc: 0.5444 - f1_m: 0.6557 - precision_m: 0.9242 - recall_m: 0.5132 - val_loss: 0.6767 - val_acc: 0.4776 - val_f1_m: 0.6087 - val_precision_m: 0.8674 - val_recall_m: 0.4718\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6643 - acc: 0.5492 - f1_m: 0.6602 - precision_m: 0.9232 - recall_m: 0.5207 - val_loss: 0.6774 - val_acc: 0.4754 - val_f1_m: 0.6066 - val_precision_m: 0.8669 - val_recall_m: 0.4694\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6642 - acc: 0.5397 - f1_m: 0.6541 - precision_m: 0.9265 - recall_m: 0.5097 - val_loss: 0.6945 - val_acc: 0.4358 - val_f1_m: 0.5610 - val_precision_m: 0.8625 - val_recall_m: 0.4186\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6643 - acc: 0.5328 - f1_m: 0.6425 - precision_m: 0.9268 - recall_m: 0.4960 - val_loss: 0.6962 - val_acc: 0.4340 - val_f1_m: 0.5587 - val_precision_m: 0.8628 - val_recall_m: 0.4161\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6640 - acc: 0.5359 - f1_m: 0.6465 - precision_m: 0.9266 - recall_m: 0.4998 - val_loss: 0.6901 - val_acc: 0.4481 - val_f1_m: 0.5766 - val_precision_m: 0.8620 - val_recall_m: 0.4361\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6639 - acc: 0.5476 - f1_m: 0.6593 - precision_m: 0.9272 - recall_m: 0.5152 - val_loss: 0.6895 - val_acc: 0.4499 - val_f1_m: 0.5784 - val_precision_m: 0.8625 - val_recall_m: 0.4381\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6635 - acc: 0.5724 - f1_m: 0.6791 - precision_m: 0.9166 - recall_m: 0.5449 - val_loss: 0.7170 - val_acc: 0.3975 - val_f1_m: 0.5104 - val_precision_m: 0.8659 - val_recall_m: 0.3647\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6637 - acc: 0.5250 - f1_m: 0.6363 - precision_m: 0.9258 - recall_m: 0.4886 - val_loss: 0.6696 - val_acc: 0.4960 - val_f1_m: 0.6300 - val_precision_m: 0.8628 - val_recall_m: 0.4991\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6633 - acc: 0.5496 - f1_m: 0.6620 - precision_m: 0.9249 - recall_m: 0.5197 - val_loss: 0.6778 - val_acc: 0.4776 - val_f1_m: 0.6085 - val_precision_m: 0.8664 - val_recall_m: 0.4717\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6630 - acc: 0.5350 - f1_m: 0.6455 - precision_m: 0.9263 - recall_m: 0.4984 - val_loss: 0.6715 - val_acc: 0.4952 - val_f1_m: 0.6283 - val_precision_m: 0.8641 - val_recall_m: 0.4966\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by worktime')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_worktime)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_worktime, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64,\n",
    "                               validation_data=(pred_df_worktime_test,y_test_rapport), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4964683895531474, 0.4921343204125196, 0.41539222246407775, None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_worktime_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [1141 1133]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC Agreement only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model with one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting deception, weighted by PC Agreement\n",
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.0648 - f1_m: 0.0188 - precision_m: 0.0194 - recall_m: 0.0183 - val_loss: 0.6938 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.2663 - f1_m: 0.2487 - precision_m: 0.2522 - recall_m: 0.2461 - val_loss: 0.6920 - val_acc: 0.9195 - val_f1_m: 0.9575 - val_precision_m: 0.9511 - val_recall_m: 0.9645\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.9437 - f1_m: 0.9700 - precision_m: 0.9499 - recall_m: 0.9917 - val_loss: 0.6912 - val_acc: 0.9283 - val_f1_m: 0.9622 - val_precision_m: 0.9508 - val_recall_m: 0.9746\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.4412 - f1_m: 0.4297 - precision_m: 0.4235 - recall_m: 0.4366 - val_loss: 0.6952 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.0491 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6961 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.5225 - f1_m: 0.5273 - precision_m: 0.5305 - recall_m: 0.5252 - val_loss: 0.6939 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.9073 - f1_m: 0.9346 - precision_m: 0.9179 - recall_m: 0.9527 - val_loss: 0.6914 - val_acc: 0.9292 - val_f1_m: 0.9627 - val_precision_m: 0.9508 - val_recall_m: 0.9755\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.2290 - f1_m: 0.2060 - precision_m: 0.2062 - recall_m: 0.2061 - val_loss: 0.6922 - val_acc: 0.9129 - val_f1_m: 0.9539 - val_precision_m: 0.9509 - val_recall_m: 0.9578\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.4100 - f1_m: 0.4083 - precision_m: 0.4124 - recall_m: 0.4054 - val_loss: 0.6917 - val_acc: 0.9182 - val_f1_m: 0.9568 - val_precision_m: 0.9511 - val_recall_m: 0.9632\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.9362 - f1_m: 0.9670 - precision_m: 0.9515 - recall_m: 0.9837 - val_loss: 0.6905 - val_acc: 0.9252 - val_f1_m: 0.9606 - val_precision_m: 0.9507 - val_recall_m: 0.9715\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.9309 - f1_m: 0.9640 - precision_m: 0.9518 - recall_m: 0.9775 - val_loss: 0.6921 - val_acc: 0.8461 - val_f1_m: 0.9157 - val_precision_m: 0.9509 - val_recall_m: 0.8844\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.8655 - f1_m: 0.9254 - precision_m: 0.9519 - recall_m: 0.9042 - val_loss: 0.6909 - val_acc: 0.9178 - val_f1_m: 0.9565 - val_precision_m: 0.9511 - val_recall_m: 0.9628\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.9346 - f1_m: 0.9661 - precision_m: 0.9521 - recall_m: 0.9809 - val_loss: 0.6916 - val_acc: 0.9041 - val_f1_m: 0.9489 - val_precision_m: 0.9512 - val_recall_m: 0.9473\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.9304 - f1_m: 0.9623 - precision_m: 0.9500 - recall_m: 0.9763 - val_loss: 0.6920 - val_acc: 0.7920 - val_f1_m: 0.8815 - val_precision_m: 0.9524 - val_recall_m: 0.8225\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.0647 - f1_m: 0.0186 - precision_m: 0.0198 - recall_m: 0.0176 - val_loss: 0.6984 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.0491 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.7004 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.0491 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6954 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.3047 - f1_m: 0.3076 - precision_m: 0.3385 - recall_m: 0.2830 - val_loss: 0.6938 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.8879 - f1_m: 0.9368 - precision_m: 0.9450 - recall_m: 0.9301 - val_loss: 0.6918 - val_acc: 0.8474 - val_f1_m: 0.9161 - val_precision_m: 0.9522 - val_recall_m: 0.8840\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.8626 - f1_m: 0.9243 - precision_m: 0.9516 - recall_m: 0.9012 - val_loss: 0.6927 - val_acc: 0.8175 - val_f1_m: 0.8976 - val_precision_m: 0.9524 - val_recall_m: 0.8504\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.6189 - f1_m: 0.6609 - precision_m: 0.6909 - recall_m: 0.6350 - val_loss: 0.6928 - val_acc: 0.8311 - val_f1_m: 0.9061 - val_precision_m: 0.9522 - val_recall_m: 0.8658\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.9005 - f1_m: 0.9471 - precision_m: 0.9520 - recall_m: 0.9438 - val_loss: 0.6916 - val_acc: 0.8342 - val_f1_m: 0.9083 - val_precision_m: 0.9524 - val_recall_m: 0.8694\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.8579 - f1_m: 0.9228 - precision_m: 0.9516 - recall_m: 0.8974 - val_loss: 0.6906 - val_acc: 0.8593 - val_f1_m: 0.9234 - val_precision_m: 0.9516 - val_recall_m: 0.8981\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.8020 - f1_m: 0.8876 - precision_m: 0.9512 - recall_m: 0.8342 - val_loss: 0.6905 - val_acc: 0.8769 - val_f1_m: 0.9336 - val_precision_m: 0.9512 - val_recall_m: 0.9176\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.8659 - f1_m: 0.9261 - precision_m: 0.9517 - recall_m: 0.9038 - val_loss: 0.6915 - val_acc: 0.8039 - val_f1_m: 0.8890 - val_precision_m: 0.9530 - val_recall_m: 0.8348\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.8458 - f1_m: 0.9147 - precision_m: 0.9504 - recall_m: 0.8835 - val_loss: 0.6908 - val_acc: 0.8364 - val_f1_m: 0.9096 - val_precision_m: 0.9521 - val_recall_m: 0.8721\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.3306 - f1_m: 0.3302 - precision_m: 0.3513 - recall_m: 0.3120 - val_loss: 0.6949 - val_acc: 0.7858 - val_f1_m: 0.8777 - val_precision_m: 0.9520 - val_recall_m: 0.8162\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.8075 - f1_m: 0.8920 - precision_m: 0.9518 - recall_m: 0.8420 - val_loss: 0.6926 - val_acc: 0.8527 - val_f1_m: 0.9195 - val_precision_m: 0.9513 - val_recall_m: 0.8913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.8686 - f1_m: 0.9271 - precision_m: 0.9519 - recall_m: 0.9066 - val_loss: 0.6935 - val_acc: 0.7845 - val_f1_m: 0.8768 - val_precision_m: 0.9524 - val_recall_m: 0.8143\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.8115 - f1_m: 0.8922 - precision_m: 0.9521 - recall_m: 0.8421 - val_loss: 0.6918 - val_acc: 0.8259 - val_f1_m: 0.9029 - val_precision_m: 0.9520 - val_recall_m: 0.8604\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.8371 - f1_m: 0.9099 - precision_m: 0.9522 - recall_m: 0.8742 - val_loss: 0.6900 - val_acc: 0.8580 - val_f1_m: 0.9226 - val_precision_m: 0.9515 - val_recall_m: 0.8967\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.8239 - f1_m: 0.9015 - precision_m: 0.9503 - recall_m: 0.8594 - val_loss: 0.6920 - val_acc: 0.8113 - val_f1_m: 0.8937 - val_precision_m: 0.9521 - val_recall_m: 0.8440\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by PC Agreement')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_agreement)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_agreement, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_agreement_test,y_test_deception), \n",
    "#                                callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49390423031727376, 0.49385114216455644, 0.49387589515796676, None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_full_throughput_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [ 296 1978]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rapport = np.asarray(y_test_rapport)\n",
    "y_train_rapport = np.asarray(y_train_rapport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting rapport, weighted by PC Agreement\n",
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6940 - acc: 0.5786 - f1_m: 0.6415 - precision_m: 0.8059 - recall_m: 0.6128 - val_loss: 0.6828 - val_acc: 0.8672 - val_f1_m: 0.9276 - val_precision_m: 0.8710 - val_recall_m: 0.9932\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6904 - acc: 0.8107 - f1_m: 0.8886 - precision_m: 0.8918 - recall_m: 0.8942 - val_loss: 0.6766 - val_acc: 0.7801 - val_f1_m: 0.8727 - val_precision_m: 0.8661 - val_recall_m: 0.8806\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6836 - acc: 0.8307 - f1_m: 0.8998 - precision_m: 0.8942 - recall_m: 0.9079 - val_loss: 0.6428 - val_acc: 0.8369 - val_f1_m: 0.9095 - val_precision_m: 0.8683 - val_recall_m: 0.9563\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6737 - acc: 0.8289 - f1_m: 0.8978 - precision_m: 0.9007 - recall_m: 0.9001 - val_loss: 0.6280 - val_acc: 0.7181 - val_f1_m: 0.8309 - val_precision_m: 0.8640 - val_recall_m: 0.8022\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6648 - acc: 0.7567 - f1_m: 0.8495 - precision_m: 0.9071 - recall_m: 0.8010 - val_loss: 0.6239 - val_acc: 0.6737 - val_f1_m: 0.7964 - val_precision_m: 0.8660 - val_recall_m: 0.7391\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6584 - acc: 0.7320 - f1_m: 0.8295 - precision_m: 0.9100 - recall_m: 0.7649 - val_loss: 0.6177 - val_acc: 0.6640 - val_f1_m: 0.7889 - val_precision_m: 0.8651 - val_recall_m: 0.7273\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6539 - acc: 0.7161 - f1_m: 0.8166 - precision_m: 0.9126 - recall_m: 0.7417 - val_loss: 0.6267 - val_acc: 0.6385 - val_f1_m: 0.7670 - val_precision_m: 0.8681 - val_recall_m: 0.6891\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6506 - acc: 0.6989 - f1_m: 0.8003 - precision_m: 0.9143 - recall_m: 0.7152 - val_loss: 0.6296 - val_acc: 0.6266 - val_f1_m: 0.7569 - val_precision_m: 0.8671 - val_recall_m: 0.6741\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6478 - acc: 0.6876 - f1_m: 0.7936 - precision_m: 0.9145 - recall_m: 0.7045 - val_loss: 0.6276 - val_acc: 0.6253 - val_f1_m: 0.7554 - val_precision_m: 0.8673 - val_recall_m: 0.6715\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6456 - acc: 0.6805 - f1_m: 0.7867 - precision_m: 0.9158 - recall_m: 0.6926 - val_loss: 0.6408 - val_acc: 0.5959 - val_f1_m: 0.7294 - val_precision_m: 0.8669 - val_recall_m: 0.6328\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6437 - acc: 0.6747 - f1_m: 0.7819 - precision_m: 0.9172 - recall_m: 0.6844 - val_loss: 0.6465 - val_acc: 0.5840 - val_f1_m: 0.7189 - val_precision_m: 0.8660 - val_recall_m: 0.6180\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6425 - acc: 0.6660 - f1_m: 0.7733 - precision_m: 0.9161 - recall_m: 0.6724 - val_loss: 0.6449 - val_acc: 0.5862 - val_f1_m: 0.7210 - val_precision_m: 0.8654 - val_recall_m: 0.6215\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6412 - acc: 0.6550 - f1_m: 0.7629 - precision_m: 0.9184 - recall_m: 0.6559 - val_loss: 0.6531 - val_acc: 0.5717 - val_f1_m: 0.7067 - val_precision_m: 0.8662 - val_recall_m: 0.6006\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6403 - acc: 0.6559 - f1_m: 0.7649 - precision_m: 0.9174 - recall_m: 0.6590 - val_loss: 0.6545 - val_acc: 0.5704 - val_f1_m: 0.7056 - val_precision_m: 0.8659 - val_recall_m: 0.5992\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6394 - acc: 0.6417 - f1_m: 0.7508 - precision_m: 0.9192 - recall_m: 0.6393 - val_loss: 0.6486 - val_acc: 0.5783 - val_f1_m: 0.7136 - val_precision_m: 0.8659 - val_recall_m: 0.6105\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6386 - acc: 0.6545 - f1_m: 0.7622 - precision_m: 0.9192 - recall_m: 0.6551 - val_loss: 0.6639 - val_acc: 0.5541 - val_f1_m: 0.6903 - val_precision_m: 0.8644 - val_recall_m: 0.5783\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6383 - acc: 0.6432 - f1_m: 0.7516 - precision_m: 0.9182 - recall_m: 0.6407 - val_loss: 0.6546 - val_acc: 0.5708 - val_f1_m: 0.7058 - val_precision_m: 0.8664 - val_recall_m: 0.5991\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6377 - acc: 0.6447 - f1_m: 0.7558 - precision_m: 0.9189 - recall_m: 0.6446 - val_loss: 0.6527 - val_acc: 0.5739 - val_f1_m: 0.7086 - val_precision_m: 0.8667 - val_recall_m: 0.6031\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6366 - acc: 0.6546 - f1_m: 0.7624 - precision_m: 0.9196 - recall_m: 0.6548 - val_loss: 0.6828 - val_acc: 0.5325 - val_f1_m: 0.6697 - val_precision_m: 0.8629 - val_recall_m: 0.5508\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6366 - acc: 0.6331 - f1_m: 0.7459 - precision_m: 0.9221 - recall_m: 0.6292 - val_loss: 0.6583 - val_acc: 0.5673 - val_f1_m: 0.7027 - val_precision_m: 0.8657 - val_recall_m: 0.5952\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6363 - acc: 0.6409 - f1_m: 0.7469 - precision_m: 0.9139 - recall_m: 0.6345 - val_loss: 0.6658 - val_acc: 0.5554 - val_f1_m: 0.6916 - val_precision_m: 0.8648 - val_recall_m: 0.5797\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6359 - acc: 0.6304 - f1_m: 0.7429 - precision_m: 0.9216 - recall_m: 0.6249 - val_loss: 0.6657 - val_acc: 0.5576 - val_f1_m: 0.6935 - val_precision_m: 0.8651 - val_recall_m: 0.5821\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6358 - acc: 0.6409 - f1_m: 0.7495 - precision_m: 0.9185 - recall_m: 0.6368 - val_loss: 0.6682 - val_acc: 0.5554 - val_f1_m: 0.6914 - val_precision_m: 0.8654 - val_recall_m: 0.5792\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6351 - acc: 0.6410 - f1_m: 0.7491 - precision_m: 0.9203 - recall_m: 0.6363 - val_loss: 0.6577 - val_acc: 0.5708 - val_f1_m: 0.7068 - val_precision_m: 0.8652 - val_recall_m: 0.6012\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6348 - acc: 0.6508 - f1_m: 0.7604 - precision_m: 0.9200 - recall_m: 0.6507 - val_loss: 0.6691 - val_acc: 0.5580 - val_f1_m: 0.6938 - val_precision_m: 0.8659 - val_recall_m: 0.5821\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6345 - acc: 0.6406 - f1_m: 0.7518 - precision_m: 0.9203 - recall_m: 0.6378 - val_loss: 0.6673 - val_acc: 0.5620 - val_f1_m: 0.6978 - val_precision_m: 0.8664 - val_recall_m: 0.5876\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6342 - acc: 0.6408 - f1_m: 0.7518 - precision_m: 0.9209 - recall_m: 0.6382 - val_loss: 0.6629 - val_acc: 0.5699 - val_f1_m: 0.7053 - val_precision_m: 0.8658 - val_recall_m: 0.5986\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6338 - acc: 0.6442 - f1_m: 0.7552 - precision_m: 0.9206 - recall_m: 0.6429 - val_loss: 0.6614 - val_acc: 0.5712 - val_f1_m: 0.7070 - val_precision_m: 0.8658 - val_recall_m: 0.6012\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6337 - acc: 0.6480 - f1_m: 0.7527 - precision_m: 0.9132 - recall_m: 0.6428 - val_loss: 0.6708 - val_acc: 0.5611 - val_f1_m: 0.6968 - val_precision_m: 0.8669 - val_recall_m: 0.5861\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6334 - acc: 0.6443 - f1_m: 0.7552 - precision_m: 0.9221 - recall_m: 0.6432 - val_loss: 0.6664 - val_acc: 0.5686 - val_f1_m: 0.7040 - val_precision_m: 0.8663 - val_recall_m: 0.5967\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6332 - acc: 0.6439 - f1_m: 0.7548 - precision_m: 0.9216 - recall_m: 0.6422 - val_loss: 0.6582 - val_acc: 0.5756 - val_f1_m: 0.7110 - val_precision_m: 0.8663 - val_recall_m: 0.6065\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6329 - acc: 0.6509 - f1_m: 0.7616 - precision_m: 0.9203 - recall_m: 0.6519 - val_loss: 0.6680 - val_acc: 0.5682 - val_f1_m: 0.7036 - val_precision_m: 0.8662 - val_recall_m: 0.5961\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by PC Agreement')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_agreement)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_agreement, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_agreement_test,y_test_rapport), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4915660107799208, 0.48388961344375664, 0.46521198001322056, None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_throughput_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [ 708 1566]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Length only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model with one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting deception, weighted by text length\n",
      "Model: \"functional_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6934 - acc: 0.6941 - f1_m: 0.7555 - precision_m: 0.9325 - recall_m: 0.7162 - val_loss: 0.6951 - val_acc: 0.1187 - val_f1_m: 0.1405 - val_precision_m: 0.9680 - val_recall_m: 0.0768\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.2082 - f1_m: 0.2511 - precision_m: 0.7732 - recall_m: 0.1759 - val_loss: 0.6959 - val_acc: 0.2168 - val_f1_m: 0.3088 - val_precision_m: 0.9454 - val_recall_m: 0.1869\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.3189 - f1_m: 0.3614 - precision_m: 0.6656 - recall_m: 0.2989 - val_loss: 0.6962 - val_acc: 0.0743 - val_f1_m: 0.0508 - val_precision_m: 0.7685 - val_recall_m: 0.0265\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.6216 - f1_m: 0.7293 - precision_m: 0.9493 - recall_m: 0.6370 - val_loss: 0.6934 - val_acc: 0.5435 - val_f1_m: 0.6935 - val_precision_m: 0.9572 - val_recall_m: 0.5459\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.7795 - f1_m: 0.8666 - precision_m: 0.9528 - recall_m: 0.8070 - val_loss: 0.6928 - val_acc: 0.6737 - val_f1_m: 0.8003 - val_precision_m: 0.9524 - val_recall_m: 0.6931\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.6830 - f1_m: 0.7990 - precision_m: 0.9545 - recall_m: 0.7046 - val_loss: 0.6904 - val_acc: 0.8347 - val_f1_m: 0.9090 - val_precision_m: 0.9536 - val_recall_m: 0.8699\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.5640 - f1_m: 0.6867 - precision_m: 0.9508 - recall_m: 0.5754 - val_loss: 0.6907 - val_acc: 0.8193 - val_f1_m: 0.8996 - val_precision_m: 0.9528 - val_recall_m: 0.8535\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.8884 - f1_m: 0.9402 - precision_m: 0.9510 - recall_m: 0.9319 - val_loss: 0.6892 - val_acc: 0.8848 - val_f1_m: 0.9386 - val_precision_m: 0.9510 - val_recall_m: 0.9274\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.7424 - f1_m: 0.8417 - precision_m: 0.9515 - recall_m: 0.7719 - val_loss: 0.6912 - val_acc: 0.7779 - val_f1_m: 0.8737 - val_precision_m: 0.9512 - val_recall_m: 0.8095\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.6138 - f1_m: 0.6817 - precision_m: 0.8028 - recall_m: 0.6277 - val_loss: 0.6951 - val_acc: 0.3012 - val_f1_m: 0.4328 - val_precision_m: 0.9462 - val_recall_m: 0.2826\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.4272 - f1_m: 0.5002 - precision_m: 0.6838 - recall_m: 0.4274 - val_loss: 0.6919 - val_acc: 0.8320 - val_f1_m: 0.9074 - val_precision_m: 0.9505 - val_recall_m: 0.8690\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.1368 - f1_m: 0.1360 - precision_m: 0.2644 - recall_m: 0.0987 - val_loss: 0.6989 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.4122 - f1_m: 0.4491 - precision_m: 0.5943 - recall_m: 0.4010 - val_loss: 0.6972 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.0492 - f1_m: 2.3310e-04 - precision_m: 0.0070 - recall_m: 1.1853e-04 - val_loss: 0.7017 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.0491 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.7036 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.0915 - f1_m: 0.0696 - precision_m: 0.1329 - recall_m: 0.0539 - val_loss: 0.6920 - val_acc: 0.8100 - val_f1_m: 0.8937 - val_precision_m: 0.9534 - val_recall_m: 0.8425\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6934 - acc: 0.5753 - f1_m: 0.6175 - precision_m: 0.6771 - recall_m: 0.5879 - val_loss: 0.6895 - val_acc: 0.8580 - val_f1_m: 0.9230 - val_precision_m: 0.9507 - val_recall_m: 0.8977\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.6923 - f1_m: 0.7600 - precision_m: 0.8515 - recall_m: 0.7175 - val_loss: 0.6919 - val_acc: 0.7823 - val_f1_m: 0.8764 - val_precision_m: 0.9523 - val_recall_m: 0.8132\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.8840 - f1_m: 0.9376 - precision_m: 0.9509 - recall_m: 0.9265 - val_loss: 0.6912 - val_acc: 0.7784 - val_f1_m: 0.8740 - val_precision_m: 0.9517 - val_recall_m: 0.8096\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.7023 - f1_m: 0.8127 - precision_m: 0.9535 - recall_m: 0.7249 - val_loss: 0.6926 - val_acc: 0.6491 - val_f1_m: 0.7823 - val_precision_m: 0.9501 - val_recall_m: 0.6677\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.5026 - f1_m: 0.5923 - precision_m: 0.9138 - recall_m: 0.5046 - val_loss: 0.6872 - val_acc: 0.8465 - val_f1_m: 0.9164 - val_precision_m: 0.9519 - val_recall_m: 0.8844\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.8542 - f1_m: 0.9185 - precision_m: 0.9506 - recall_m: 0.8912 - val_loss: 0.6913 - val_acc: 0.7208 - val_f1_m: 0.8351 - val_precision_m: 0.9505 - val_recall_m: 0.7468\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.8260 - f1_m: 0.9001 - precision_m: 0.9501 - recall_m: 0.8625 - val_loss: 0.6935 - val_acc: 0.6082 - val_f1_m: 0.7497 - val_precision_m: 0.9503 - val_recall_m: 0.6222\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.0962 - f1_m: 0.0787 - precision_m: 0.1948 - recall_m: 0.0535 - val_loss: 0.6955 - val_acc: 0.4639 - val_f1_m: 0.6169 - val_precision_m: 0.9524 - val_recall_m: 0.4592\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.3736 - f1_m: 0.4569 - precision_m: 0.7417 - recall_m: 0.3649 - val_loss: 0.6904 - val_acc: 0.7639 - val_f1_m: 0.8646 - val_precision_m: 0.9508 - val_recall_m: 0.7945\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.8295 - f1_m: 0.9045 - precision_m: 0.9515 - recall_m: 0.8653 - val_loss: 0.6901 - val_acc: 0.7643 - val_f1_m: 0.8649 - val_precision_m: 0.9510 - val_recall_m: 0.7950\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.7009 - f1_m: 0.8166 - precision_m: 0.9535 - recall_m: 0.7227 - val_loss: 0.6889 - val_acc: 0.8039 - val_f1_m: 0.8902 - val_precision_m: 0.9523 - val_recall_m: 0.8371\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.4449 - f1_m: 0.5393 - precision_m: 0.8365 - recall_m: 0.4405 - val_loss: 0.6909 - val_acc: 0.7331 - val_f1_m: 0.8436 - val_precision_m: 0.9493 - val_recall_m: 0.7613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.5381 - f1_m: 0.6457 - precision_m: 0.9468 - recall_m: 0.5390 - val_loss: 0.6950 - val_acc: 0.5220 - val_f1_m: 0.6741 - val_precision_m: 0.9535 - val_recall_m: 0.5244\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.3835 - f1_m: 0.4391 - precision_m: 0.6594 - recall_m: 0.3727 - val_loss: 0.6968 - val_acc: 0.3540 - val_f1_m: 0.4967 - val_precision_m: 0.9492 - val_recall_m: 0.3389\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6934 - acc: 0.4152 - f1_m: 0.4890 - precision_m: 0.7142 - recall_m: 0.4075 - val_loss: 0.6990 - val_acc: 0.1407 - val_f1_m: 0.1824 - val_precision_m: 0.9420 - val_recall_m: 0.1025\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.0625 - f1_m: 0.0273 - precision_m: 0.1551 - recall_m: 0.0162 - val_loss: 0.6965 - val_acc: 0.4024 - val_f1_m: 0.5520 - val_precision_m: 0.9497 - val_recall_m: 0.3915\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by text length')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_textlength)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_textlength, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_textlength_test,y_test_deception), \n",
    "#                                callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49906181556932067, 0.4952218514602881, 0.3224538298201207, None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_full_textlength_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [1381  893]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting rapport, weighted by text length\n",
      "Model: \"functional_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6952 - acc: 0.1736 - f1_m: 0.0851 - precision_m: 0.3478 - recall_m: 0.0543 - val_loss: 0.6972 - val_acc: 0.1517 - val_f1_m: 0.0597 - val_precision_m: 0.8222 - val_recall_m: 0.0313\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6923 - acc: 0.1637 - f1_m: 0.0604 - precision_m: 0.3900 - recall_m: 0.0388 - val_loss: 0.6951 - val_acc: 0.3905 - val_f1_m: 0.5010 - val_precision_m: 0.8649 - val_recall_m: 0.3553\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6906 - acc: 0.4253 - f1_m: 0.5054 - precision_m: 0.9206 - recall_m: 0.3627 - val_loss: 0.6998 - val_acc: 0.3223 - val_f1_m: 0.3937 - val_precision_m: 0.8841 - val_recall_m: 0.2561\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6886 - acc: 0.3294 - f1_m: 0.3770 - precision_m: 0.9247 - recall_m: 0.2408 - val_loss: 0.7042 - val_acc: 0.3237 - val_f1_m: 0.3964 - val_precision_m: 0.8817 - val_recall_m: 0.2584\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6863 - acc: 0.3471 - f1_m: 0.4011 - precision_m: 0.9235 - recall_m: 0.2596 - val_loss: 0.7054 - val_acc: 0.3553 - val_f1_m: 0.4467 - val_precision_m: 0.8738 - val_recall_m: 0.3031\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6837 - acc: 0.3708 - f1_m: 0.4401 - precision_m: 0.9296 - recall_m: 0.2935 - val_loss: 0.7024 - val_acc: 0.4006 - val_f1_m: 0.5169 - val_precision_m: 0.8600 - val_recall_m: 0.3718\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6807 - acc: 0.4398 - f1_m: 0.5344 - precision_m: 0.9254 - recall_m: 0.3790 - val_loss: 0.7132 - val_acc: 0.3668 - val_f1_m: 0.4635 - val_precision_m: 0.8728 - val_recall_m: 0.3184\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6776 - acc: 0.4237 - f1_m: 0.5151 - precision_m: 0.9268 - recall_m: 0.3613 - val_loss: 0.7178 - val_acc: 0.3720 - val_f1_m: 0.4712 - val_precision_m: 0.8721 - val_recall_m: 0.3258\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6727 - acc: 0.4551 - f1_m: 0.5552 - precision_m: 0.9262 - recall_m: 0.4003 - val_loss: 0.7287 - val_acc: 0.3909 - val_f1_m: 0.5006 - val_precision_m: 0.8670 - val_recall_m: 0.3542\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6669 - acc: 0.4745 - f1_m: 0.5801 - precision_m: 0.9236 - recall_m: 0.4283 - val_loss: 0.7068 - val_acc: 0.4771 - val_f1_m: 0.6080 - val_precision_m: 0.8690 - val_recall_m: 0.4705\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6625 - acc: 0.5069 - f1_m: 0.6169 - precision_m: 0.9222 - recall_m: 0.4672 - val_loss: 0.7096 - val_acc: 0.4842 - val_f1_m: 0.6159 - val_precision_m: 0.8686 - val_recall_m: 0.4805\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6582 - acc: 0.5571 - f1_m: 0.6665 - precision_m: 0.9146 - recall_m: 0.5275 - val_loss: 0.7140 - val_acc: 0.4859 - val_f1_m: 0.6183 - val_precision_m: 0.8687 - val_recall_m: 0.4835\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6544 - acc: 0.5553 - f1_m: 0.6708 - precision_m: 0.9220 - recall_m: 0.5313 - val_loss: 0.7030 - val_acc: 0.5150 - val_f1_m: 0.6515 - val_precision_m: 0.8638 - val_recall_m: 0.5258\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6509 - acc: 0.5867 - f1_m: 0.7008 - precision_m: 0.9212 - recall_m: 0.5691 - val_loss: 0.6955 - val_acc: 0.5361 - val_f1_m: 0.6734 - val_precision_m: 0.8631 - val_recall_m: 0.5546\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6476 - acc: 0.5967 - f1_m: 0.7082 - precision_m: 0.9218 - recall_m: 0.5790 - val_loss: 0.7124 - val_acc: 0.5123 - val_f1_m: 0.6489 - val_precision_m: 0.8630 - val_recall_m: 0.5228\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6441 - acc: 0.5980 - f1_m: 0.7095 - precision_m: 0.9211 - recall_m: 0.5815 - val_loss: 0.6923 - val_acc: 0.5611 - val_f1_m: 0.6977 - val_precision_m: 0.8648 - val_recall_m: 0.5876\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6409 - acc: 0.6204 - f1_m: 0.7335 - precision_m: 0.9209 - recall_m: 0.6124 - val_loss: 0.6811 - val_acc: 0.5814 - val_f1_m: 0.7183 - val_precision_m: 0.8623 - val_recall_m: 0.6185\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6382 - acc: 0.6346 - f1_m: 0.7469 - precision_m: 0.9203 - recall_m: 0.6316 - val_loss: 0.6626 - val_acc: 0.6157 - val_f1_m: 0.7486 - val_precision_m: 0.8633 - val_recall_m: 0.6638\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6363 - acc: 0.6636 - f1_m: 0.7658 - precision_m: 0.9133 - recall_m: 0.6625 - val_loss: 0.6987 - val_acc: 0.5686 - val_f1_m: 0.7054 - val_precision_m: 0.8654 - val_recall_m: 0.5987\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6345 - acc: 0.6639 - f1_m: 0.7714 - precision_m: 0.9197 - recall_m: 0.6681 - val_loss: 0.7011 - val_acc: 0.5704 - val_f1_m: 0.7076 - val_precision_m: 0.8641 - val_recall_m: 0.6022\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6325 - acc: 0.6650 - f1_m: 0.7743 - precision_m: 0.9196 - recall_m: 0.6721 - val_loss: 0.6846 - val_acc: 0.5923 - val_f1_m: 0.7285 - val_precision_m: 0.8619 - val_recall_m: 0.6340\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6313 - acc: 0.6661 - f1_m: 0.7730 - precision_m: 0.9177 - recall_m: 0.6704 - val_loss: 0.6928 - val_acc: 0.5853 - val_f1_m: 0.7220 - val_precision_m: 0.8617 - val_recall_m: 0.6245\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6300 - acc: 0.6774 - f1_m: 0.7838 - precision_m: 0.9200 - recall_m: 0.6858 - val_loss: 0.6807 - val_acc: 0.6025 - val_f1_m: 0.7374 - val_precision_m: 0.8626 - val_recall_m: 0.6470\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6289 - acc: 0.6798 - f1_m: 0.7829 - precision_m: 0.9211 - recall_m: 0.6857 - val_loss: 0.6685 - val_acc: 0.6196 - val_f1_m: 0.7525 - val_precision_m: 0.8627 - val_recall_m: 0.6704\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6284 - acc: 0.6892 - f1_m: 0.7883 - precision_m: 0.9119 - recall_m: 0.6970 - val_loss: 0.6721 - val_acc: 0.6179 - val_f1_m: 0.7508 - val_precision_m: 0.8634 - val_recall_m: 0.6675\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6272 - acc: 0.6940 - f1_m: 0.7989 - precision_m: 0.9191 - recall_m: 0.7095 - val_loss: 0.6963 - val_acc: 0.5928 - val_f1_m: 0.7289 - val_precision_m: 0.8615 - val_recall_m: 0.6350\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6269 - acc: 0.6894 - f1_m: 0.7937 - precision_m: 0.9187 - recall_m: 0.7013 - val_loss: 0.6913 - val_acc: 0.6003 - val_f1_m: 0.7354 - val_precision_m: 0.8625 - val_recall_m: 0.6441\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6257 - acc: 0.6906 - f1_m: 0.7958 - precision_m: 0.9198 - recall_m: 0.7039 - val_loss: 0.6573 - val_acc: 0.6363 - val_f1_m: 0.7661 - val_precision_m: 0.8625 - val_recall_m: 0.6924\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6257 - acc: 0.7005 - f1_m: 0.8043 - precision_m: 0.9169 - recall_m: 0.7189 - val_loss: 0.6659 - val_acc: 0.6266 - val_f1_m: 0.7581 - val_precision_m: 0.8616 - val_recall_m: 0.6804\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6247 - acc: 0.6940 - f1_m: 0.7990 - precision_m: 0.9195 - recall_m: 0.7095 - val_loss: 0.6627 - val_acc: 0.6315 - val_f1_m: 0.7621 - val_precision_m: 0.8620 - val_recall_m: 0.6864\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6246 - acc: 0.7035 - f1_m: 0.8067 - precision_m: 0.9182 - recall_m: 0.7224 - val_loss: 0.6605 - val_acc: 0.6363 - val_f1_m: 0.7662 - val_precision_m: 0.8621 - val_recall_m: 0.6929\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6238 - acc: 0.7047 - f1_m: 0.8074 - precision_m: 0.9180 - recall_m: 0.7233 - val_loss: 0.6991 - val_acc: 0.6047 - val_f1_m: 0.7391 - val_precision_m: 0.8623 - val_recall_m: 0.6500\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by text length')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_textlength)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_textlength, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_textlength_test,y_test_rapport), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4890395428055487, 0.4779147636333565, 0.45383431858128565, None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_textlength_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [ 786 1488]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other special options (either SP1, SP2, SP3, RAND_UNI, or RAND_NORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model with one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting deception, weighted by special option\n",
      "Model: \"functional_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.3528 - f1_m: 0.4182 - precision_m: 0.6315 - recall_m: 0.3373 - val_loss: 0.6949 - val_acc: 0.1768 - val_f1_m: 0.2424 - val_precision_m: 0.9694 - val_recall_m: 0.1398\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.6242 - f1_m: 0.6908 - precision_m: 0.8172 - recall_m: 0.6342 - val_loss: 0.6936 - val_acc: 0.6161 - val_f1_m: 0.7562 - val_precision_m: 0.9514 - val_recall_m: 0.6299\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.7901 - f1_m: 0.8767 - precision_m: 0.9508 - recall_m: 0.8236 - val_loss: 0.6921 - val_acc: 0.7726 - val_f1_m: 0.8702 - val_precision_m: 0.9494 - val_recall_m: 0.8047\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.2901 - f1_m: 0.3179 - precision_m: 0.4510 - recall_m: 0.2677 - val_loss: 0.6963 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.1785 - f1_m: 0.2093 - precision_m: 0.5300 - recall_m: 0.1444 - val_loss: 0.6961 - val_acc: 0.0704 - val_f1_m: 0.0469 - val_precision_m: 0.7593 - val_recall_m: 0.0244\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6932 - acc: 0.7317 - f1_m: 0.8295 - precision_m: 0.9456 - recall_m: 0.7574 - val_loss: 0.6936 - val_acc: 0.6434 - val_f1_m: 0.7778 - val_precision_m: 0.9515 - val_recall_m: 0.6601\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.6130 - f1_m: 0.7410 - precision_m: 0.9507 - recall_m: 0.6296 - val_loss: 0.6919 - val_acc: 0.7476 - val_f1_m: 0.8536 - val_precision_m: 0.9507 - val_recall_m: 0.7759\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.7633 - f1_m: 0.8594 - precision_m: 0.9536 - recall_m: 0.7909 - val_loss: 0.6924 - val_acc: 0.7150 - val_f1_m: 0.8309 - val_precision_m: 0.9514 - val_recall_m: 0.7393\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.7365 - f1_m: 0.8443 - precision_m: 0.9518 - recall_m: 0.7639 - val_loss: 0.6913 - val_acc: 0.7625 - val_f1_m: 0.8633 - val_precision_m: 0.9506 - val_recall_m: 0.7923\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.6262 - f1_m: 0.7600 - precision_m: 0.9526 - recall_m: 0.6398 - val_loss: 0.6926 - val_acc: 0.7018 - val_f1_m: 0.8214 - val_precision_m: 0.9522 - val_recall_m: 0.7239\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.6998 - f1_m: 0.8087 - precision_m: 0.9522 - recall_m: 0.7208 - val_loss: 0.6903 - val_acc: 0.8131 - val_f1_m: 0.8960 - val_precision_m: 0.9484 - val_recall_m: 0.8503\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.6306 - f1_m: 0.7205 - precision_m: 0.8235 - recall_m: 0.6483 - val_loss: 0.6907 - val_acc: 0.7898 - val_f1_m: 0.8810 - val_precision_m: 0.9494 - val_recall_m: 0.8234\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.6643 - f1_m: 0.7872 - precision_m: 0.9533 - recall_m: 0.6826 - val_loss: 0.6920 - val_acc: 0.7067 - val_f1_m: 0.8243 - val_precision_m: 0.9539 - val_recall_m: 0.7275\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.7812 - f1_m: 0.8723 - precision_m: 0.9515 - recall_m: 0.8100 - val_loss: 0.6904 - val_acc: 0.7551 - val_f1_m: 0.8584 - val_precision_m: 0.9506 - val_recall_m: 0.7841\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.7586 - f1_m: 0.8600 - precision_m: 0.9504 - recall_m: 0.7877 - val_loss: 0.6911 - val_acc: 0.7274 - val_f1_m: 0.8391 - val_precision_m: 0.9533 - val_recall_m: 0.7512\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6930 - acc: 0.3947 - f1_m: 0.5144 - precision_m: 0.8539 - recall_m: 0.3855 - val_loss: 0.6958 - val_acc: 0.5097 - val_f1_m: 0.6639 - val_precision_m: 0.9498 - val_recall_m: 0.5124\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.6533 - f1_m: 0.7806 - precision_m: 0.9532 - recall_m: 0.6677 - val_loss: 0.6936 - val_acc: 0.6438 - val_f1_m: 0.7780 - val_precision_m: 0.9518 - val_recall_m: 0.6601\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.5966 - f1_m: 0.7248 - precision_m: 0.9535 - recall_m: 0.6048 - val_loss: 0.6963 - val_acc: 0.5040 - val_f1_m: 0.6586 - val_precision_m: 0.9516 - val_recall_m: 0.5057\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.6213 - f1_m: 0.7548 - precision_m: 0.9511 - recall_m: 0.6330 - val_loss: 0.6952 - val_acc: 0.5730 - val_f1_m: 0.7207 - val_precision_m: 0.9492 - val_recall_m: 0.5832\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.6333 - f1_m: 0.7644 - precision_m: 0.9496 - recall_m: 0.6482 - val_loss: 0.6928 - val_acc: 0.6693 - val_f1_m: 0.7970 - val_precision_m: 0.9531 - val_recall_m: 0.6870\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.4539 - f1_m: 0.6010 - precision_m: 0.9561 - recall_m: 0.4485 - val_loss: 0.6980 - val_acc: 0.4380 - val_f1_m: 0.5924 - val_precision_m: 0.9502 - val_recall_m: 0.4324\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.6046 - f1_m: 0.7440 - precision_m: 0.9537 - recall_m: 0.6144 - val_loss: 0.6970 - val_acc: 0.5123 - val_f1_m: 0.6660 - val_precision_m: 0.9516 - val_recall_m: 0.5144\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.5546 - f1_m: 0.6929 - precision_m: 0.9518 - recall_m: 0.5630 - val_loss: 0.6953 - val_acc: 0.5893 - val_f1_m: 0.7346 - val_precision_m: 0.9492 - val_recall_m: 0.6019\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6929 - acc: 0.6636 - f1_m: 0.7904 - precision_m: 0.9520 - recall_m: 0.6826 - val_loss: 0.6947 - val_acc: 0.6157 - val_f1_m: 0.7558 - val_precision_m: 0.9516 - val_recall_m: 0.6295\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.6274 - f1_m: 0.7596 - precision_m: 0.9514 - recall_m: 0.6422 - val_loss: 0.6952 - val_acc: 0.6033 - val_f1_m: 0.7462 - val_precision_m: 0.9501 - val_recall_m: 0.6167\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.4729 - f1_m: 0.6214 - precision_m: 0.9522 - recall_m: 0.4739 - val_loss: 0.6934 - val_acc: 0.6618 - val_f1_m: 0.7915 - val_precision_m: 0.9531 - val_recall_m: 0.6793\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6927 - acc: 0.7449 - f1_m: 0.8478 - precision_m: 0.9522 - recall_m: 0.7674 - val_loss: 0.6910 - val_acc: 0.7058 - val_f1_m: 0.8238 - val_precision_m: 0.9535 - val_recall_m: 0.7273\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.6606 - f1_m: 0.7863 - precision_m: 0.9481 - recall_m: 0.6767 - val_loss: 0.6907 - val_acc: 0.7076 - val_f1_m: 0.8251 - val_precision_m: 0.9536 - val_recall_m: 0.7291\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.3363 - f1_m: 0.4566 - precision_m: 0.9542 - recall_m: 0.3184 - val_loss: 0.6987 - val_acc: 0.5611 - val_f1_m: 0.7104 - val_precision_m: 0.9504 - val_recall_m: 0.5698\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.5256 - f1_m: 0.6704 - precision_m: 0.9541 - recall_m: 0.5289 - val_loss: 0.6934 - val_acc: 0.6763 - val_f1_m: 0.8022 - val_precision_m: 0.9542 - val_recall_m: 0.6943\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6928 - acc: 0.6992 - f1_m: 0.8145 - precision_m: 0.9515 - recall_m: 0.7176 - val_loss: 0.6934 - val_acc: 0.6728 - val_f1_m: 0.7997 - val_precision_m: 0.9540 - val_recall_m: 0.6907\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6927 - acc: 0.6130 - f1_m: 0.7504 - precision_m: 0.9515 - recall_m: 0.6247 - val_loss: 0.6972 - val_acc: 0.5888 - val_f1_m: 0.7346 - val_precision_m: 0.9492 - val_recall_m: 0.6017\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by special option')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_special)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_special, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_special_test,y_test_deception), \n",
    "#                                callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49837763996179835, 0.49168676490022467, 0.4095666645469246, None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_full_special_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [ 909 1365]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rapport = np.asarray(y_test_rapport)\n",
    "y_train_rapport = np.asarray(y_train_rapport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting rapport, weighted by special option\n",
      "Model: \"functional_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6808 - acc: 0.7578 - f1_m: 0.8516 - precision_m: 0.8881 - recall_m: 0.8238 - val_loss: 0.6523 - val_acc: 0.6891 - val_f1_m: 0.8082 - val_precision_m: 0.8629 - val_recall_m: 0.7625\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6770 - acc: 0.7132 - f1_m: 0.8161 - precision_m: 0.9027 - recall_m: 0.7477 - val_loss: 0.6547 - val_acc: 0.5994 - val_f1_m: 0.7328 - val_precision_m: 0.8628 - val_recall_m: 0.6405\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6736 - acc: 0.6745 - f1_m: 0.7835 - precision_m: 0.9082 - recall_m: 0.6917 - val_loss: 0.6519 - val_acc: 0.5796 - val_f1_m: 0.7151 - val_precision_m: 0.8612 - val_recall_m: 0.6154\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6706 - acc: 0.6410 - f1_m: 0.7544 - precision_m: 0.9140 - recall_m: 0.6465 - val_loss: 0.6537 - val_acc: 0.5466 - val_f1_m: 0.6841 - val_precision_m: 0.8613 - val_recall_m: 0.5709\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6682 - acc: 0.6320 - f1_m: 0.7451 - precision_m: 0.9177 - recall_m: 0.6305 - val_loss: 0.6611 - val_acc: 0.5123 - val_f1_m: 0.6501 - val_precision_m: 0.8616 - val_recall_m: 0.5251\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6662 - acc: 0.5921 - f1_m: 0.7064 - precision_m: 0.9231 - recall_m: 0.5755 - val_loss: 0.6556 - val_acc: 0.5246 - val_f1_m: 0.6622 - val_precision_m: 0.8610 - val_recall_m: 0.5414\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6647 - acc: 0.5892 - f1_m: 0.7037 - precision_m: 0.9206 - recall_m: 0.5737 - val_loss: 0.6614 - val_acc: 0.5084 - val_f1_m: 0.6453 - val_precision_m: 0.8639 - val_recall_m: 0.5181\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6632 - acc: 0.5861 - f1_m: 0.7005 - precision_m: 0.9243 - recall_m: 0.5665 - val_loss: 0.6672 - val_acc: 0.4872 - val_f1_m: 0.6217 - val_precision_m: 0.8655 - val_recall_m: 0.4890\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6625 - acc: 0.5594 - f1_m: 0.6718 - precision_m: 0.9212 - recall_m: 0.5349 - val_loss: 0.6606 - val_acc: 0.5075 - val_f1_m: 0.6446 - val_precision_m: 0.8629 - val_recall_m: 0.5176\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6611 - acc: 0.5502 - f1_m: 0.6634 - precision_m: 0.9245 - recall_m: 0.5203 - val_loss: 0.6721 - val_acc: 0.4776 - val_f1_m: 0.6101 - val_precision_m: 0.8658 - val_recall_m: 0.4747\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6604 - acc: 0.5438 - f1_m: 0.6531 - precision_m: 0.9249 - recall_m: 0.5092 - val_loss: 0.6710 - val_acc: 0.4802 - val_f1_m: 0.6136 - val_precision_m: 0.8645 - val_recall_m: 0.4793\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6597 - acc: 0.5509 - f1_m: 0.6650 - precision_m: 0.9252 - recall_m: 0.5235 - val_loss: 0.6743 - val_acc: 0.4732 - val_f1_m: 0.6046 - val_precision_m: 0.8671 - val_recall_m: 0.4678\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6593 - acc: 0.5540 - f1_m: 0.6653 - precision_m: 0.9262 - recall_m: 0.5240 - val_loss: 0.6676 - val_acc: 0.4934 - val_f1_m: 0.6277 - val_precision_m: 0.8668 - val_recall_m: 0.4957\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6585 - acc: 0.5409 - f1_m: 0.6487 - precision_m: 0.9186 - recall_m: 0.5044 - val_loss: 0.6663 - val_acc: 0.4947 - val_f1_m: 0.6300 - val_precision_m: 0.8633 - val_recall_m: 0.4998\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6583 - acc: 0.5562 - f1_m: 0.6680 - precision_m: 0.9248 - recall_m: 0.5265 - val_loss: 0.6712 - val_acc: 0.4855 - val_f1_m: 0.6189 - val_precision_m: 0.8678 - val_recall_m: 0.4849\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6579 - acc: 0.5462 - f1_m: 0.6600 - precision_m: 0.9248 - recall_m: 0.5165 - val_loss: 0.6730 - val_acc: 0.4833 - val_f1_m: 0.6155 - val_precision_m: 0.8687 - val_recall_m: 0.4805\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6577 - acc: 0.5456 - f1_m: 0.6558 - precision_m: 0.9252 - recall_m: 0.5128 - val_loss: 0.6763 - val_acc: 0.4736 - val_f1_m: 0.6048 - val_precision_m: 0.8683 - val_recall_m: 0.4678\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6575 - acc: 0.5551 - f1_m: 0.6663 - precision_m: 0.9258 - recall_m: 0.5244 - val_loss: 0.6890 - val_acc: 0.4450 - val_f1_m: 0.5709 - val_precision_m: 0.8670 - val_recall_m: 0.4288\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6574 - acc: 0.5400 - f1_m: 0.6529 - precision_m: 0.9239 - recall_m: 0.5080 - val_loss: 0.6825 - val_acc: 0.4595 - val_f1_m: 0.5882 - val_precision_m: 0.8663 - val_recall_m: 0.4488\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6571 - acc: 0.5253 - f1_m: 0.6349 - precision_m: 0.9257 - recall_m: 0.4872 - val_loss: 0.6644 - val_acc: 0.5004 - val_f1_m: 0.6366 - val_precision_m: 0.8637 - val_recall_m: 0.5079\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6570 - acc: 0.5536 - f1_m: 0.6627 - precision_m: 0.9250 - recall_m: 0.5220 - val_loss: 0.6806 - val_acc: 0.4675 - val_f1_m: 0.5972 - val_precision_m: 0.8678 - val_recall_m: 0.4589\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6568 - acc: 0.5518 - f1_m: 0.6603 - precision_m: 0.9187 - recall_m: 0.5184 - val_loss: 0.6835 - val_acc: 0.4626 - val_f1_m: 0.5914 - val_precision_m: 0.8683 - val_recall_m: 0.4519\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6566 - acc: 0.5287 - f1_m: 0.6408 - precision_m: 0.9272 - recall_m: 0.4931 - val_loss: 0.6647 - val_acc: 0.5009 - val_f1_m: 0.6370 - val_precision_m: 0.8637 - val_recall_m: 0.5084\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6565 - acc: 0.5480 - f1_m: 0.6580 - precision_m: 0.9216 - recall_m: 0.5161 - val_loss: 0.6807 - val_acc: 0.4683 - val_f1_m: 0.5988 - val_precision_m: 0.8686 - val_recall_m: 0.4605\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6564 - acc: 0.5230 - f1_m: 0.6327 - precision_m: 0.9269 - recall_m: 0.4848 - val_loss: 0.6659 - val_acc: 0.4965 - val_f1_m: 0.6322 - val_precision_m: 0.8637 - val_recall_m: 0.5024\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6561 - acc: 0.5492 - f1_m: 0.6567 - precision_m: 0.9191 - recall_m: 0.5147 - val_loss: 0.6811 - val_acc: 0.4679 - val_f1_m: 0.5981 - val_precision_m: 0.8682 - val_recall_m: 0.4600\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6560 - acc: 0.5298 - f1_m: 0.6412 - precision_m: 0.9245 - recall_m: 0.4939 - val_loss: 0.6807 - val_acc: 0.4697 - val_f1_m: 0.5999 - val_precision_m: 0.8687 - val_recall_m: 0.4619\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6558 - acc: 0.5389 - f1_m: 0.6524 - precision_m: 0.9261 - recall_m: 0.5075 - val_loss: 0.6766 - val_acc: 0.4798 - val_f1_m: 0.6115 - val_precision_m: 0.8697 - val_recall_m: 0.4755\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6556 - acc: 0.5258 - f1_m: 0.6344 - precision_m: 0.9250 - recall_m: 0.4873 - val_loss: 0.6733 - val_acc: 0.4894 - val_f1_m: 0.6227 - val_precision_m: 0.8693 - val_recall_m: 0.4890\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6557 - acc: 0.5267 - f1_m: 0.6358 - precision_m: 0.9257 - recall_m: 0.4884 - val_loss: 0.6771 - val_acc: 0.4776 - val_f1_m: 0.6093 - val_precision_m: 0.8692 - val_recall_m: 0.4731\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6553 - acc: 0.5392 - f1_m: 0.6506 - precision_m: 0.9273 - recall_m: 0.5045 - val_loss: 0.6796 - val_acc: 0.4727 - val_f1_m: 0.6031 - val_precision_m: 0.8698 - val_recall_m: 0.4655\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.6553 - acc: 0.5268 - f1_m: 0.6379 - precision_m: 0.9273 - recall_m: 0.4896 - val_loss: 0.6748 - val_acc: 0.4850 - val_f1_m: 0.6177 - val_precision_m: 0.8697 - val_recall_m: 0.4831\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by special option')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_special)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_special, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64,\n",
    "                               validation_data=(pred_df_special_test,y_test_rapport), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49908359929352714, 0.4979618663997395, 0.4121053764509132, None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_special_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [1180 1094]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
