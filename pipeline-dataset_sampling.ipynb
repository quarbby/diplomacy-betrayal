{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from metadata_options.ipynb\n",
      "importing Jupyter notebook from models_nn.ipynb\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 16)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\lynne\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3417\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-2-45f13bd404fa>\"\u001b[0m, line \u001b[0;32m29\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    import models_nn\n",
      "  File \u001b[0;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[0;32m983\u001b[0m, in \u001b[0;35m_find_and_load\u001b[0m\n",
      "  File \u001b[0;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[0;32m967\u001b[0m, in \u001b[0;35m_find_and_load_unlocked\u001b[0m\n",
      "  File \u001b[0;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[0;32m668\u001b[0m, in \u001b[0;35m_load_unlocked\u001b[0m\n",
      "  File \u001b[0;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[0;32m638\u001b[0m, in \u001b[0;35m_load_backward_compatible\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\lynne\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\import_ipynb.py\"\u001b[1;36m, line \u001b[1;32m61\u001b[1;36m, in \u001b[1;35mload_module\u001b[1;36m\u001b[0m\n\u001b[1;33m    exec(code, mod.__dict__)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<string>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    elif MODEL_NAME = 'bilstm':\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Input, InputLayer, Dropout, Dense, Flatten, Embedding, Add, Concatenate\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "## Own code \n",
    "import import_ipynb\n",
    "import metadata_options\n",
    "import models_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with Throughput & WorkTime\n",
    "df = pd.read_csv('./data/kokil dec 6 reprepare/conf_pc_worker_sem.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "## Model Options ##\n",
    "######################################\n",
    "# options: lstm, cnn, lstm-attn, bilstm\n",
    "\n",
    "model_name = 'bilstm'\n",
    "models_nn.MODEL_NAME = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# pipeline-onehot Function blocks #\n",
    "###################################\n",
    "\n",
    "def sss_train_test_split(dataframe, class_name, n_splits, test_size, random_state):\n",
    "    y = dataframe[class_name].copy()\n",
    "    X = dataframe.drop([class_name], axis=1)\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    splits_generator = sss.split(X, y)\n",
    "\n",
    "    for train_idx, test_idx in splits_generator:\n",
    "        indices_train = train_idx\n",
    "        indices_test = test_idx\n",
    "\n",
    "    train = df.take(indices_train)\n",
    "    test = df.take(indices_test)\n",
    "    \n",
    "    return indices_train, indices_test, train, test\n",
    "\n",
    "def generate_class_weights(train_data, class_name, annotation_name):\n",
    "    # Check if first class label is numeric or alphabetic\n",
    "    if class_name == annotation_name:\n",
    "        ## Convert to int type\n",
    "        tmp_y_train = train_data[annotation_name].copy()\n",
    "        tmp_y_train[annotation_name] = train_data[annotation_name].apply(lambda x : 1 if x == \"Straightforward\" else 0)\n",
    "        y_train = tmp_y_train[annotation_name].to_numpy()\n",
    "    else:\n",
    "        y_train = train_data[annotation_name].to_numpy()\n",
    "    \n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    return y_train, class_weight_dict\n",
    "\n",
    "def label_preprocessing(y_data, label_encoder):\n",
    "    out = label_encoder.fit_transform(y_data).reshape(-1,1)\n",
    "    return out\n",
    "\n",
    "def individual_model(annot_name, x_train_data, y_train_data, x_val_data, y_val_data, class_weight_dict, indiv_batch_size, indiv_epochs):\n",
    "    model = models_nn.create_nn_model()\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = RMSprop(),\n",
    "                  metrics = ['accuracy', models_nn.f1_m, models_nn.recall_m, models_nn.precision_m])\n",
    "    history = model.fit(x_train_data,y_train_data,\n",
    "                        batch_size=indiv_batch_size,\n",
    "                        epochs=indiv_epochs,\n",
    "                        validation_data=(x_val_data, y_val_data), \n",
    "#                         callbacks=[models_nn.early_stop],\n",
    "                        class_weight=class_weight_dict,\n",
    "                        verbose=0)\n",
    "    \n",
    "    pred = model.predict(x_train_data)\n",
    "    pred_test = model.predict(x_val_data)\n",
    "\n",
    "    pred_test_round = pred_test.round()\n",
    "    \n",
    "    validation_metrics_dict = history.history\n",
    "    val_f1_list = history.history['val_f1_m']\n",
    "    best_val_f1 = max(val_f1_list)\n",
    "    best_val_prec = history.history['val_precision_m'][val_f1_list.index(best_val_f1)]\n",
    "    best_val_recall = history.history['val_recall_m'][val_f1_list.index(best_val_f1)]\n",
    "    macro_scores = precision_recall_fscore_support(y_val_data, pred_test_round, average='macro')\n",
    "    print(\"#############################################################\")\n",
    "    print(\"Metrics for {} individual model:\".format(annot_name))\n",
    "    print(\"Best validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(best_val_f1,\n",
    "                                                                                 best_val_prec,\n",
    "                                                                                 best_val_recall))\n",
    "    print(\"Macro validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(macro_scores[2],\n",
    "                                                                                  macro_scores[0],\n",
    "                                                                                  macro_scores[1]))\n",
    "    return pred, pred_test\n",
    "\n",
    "# (HARD-CODED)\n",
    "def generate_encodings(gamemove_pred, reasoning_pred, shareinfo_pred, rapport_pred):\n",
    "    pred_df_arr_full = []\n",
    "    pred_df_arr = []\n",
    "    for i in range(0, len(gamemove_pred)):\n",
    "        pred_obj_1 = {}\n",
    "        pred_obj_1['gamemove'] = gamemove_pred[i][0]\n",
    "        pred_obj_1['reasoning'] = reasoning_pred[i][0]\n",
    "        pred_obj_1['shareinfo'] = shareinfo_pred[i][0]\n",
    "        pred_df_arr.append(pred_obj_1)\n",
    "\n",
    "        pred_obj_2 = pred_obj_1.copy()\n",
    "        pred_obj_2['rapport'] = rapport_pred[i][0]\n",
    "        pred_df_arr_full.append(pred_obj_2)\n",
    "\n",
    "    pred_df_full = pd.DataFrame(pred_df_arr_full)\n",
    "    pred_df = pd.DataFrame(pred_df_arr)\n",
    "    return pred_df_full, pred_df\n",
    "\n",
    "def joint_model(is_max, weights_name, pred_df_full, y_train_1, pred_df_full_test, y_test_1,\n",
    "                pred_df, y_train_2, pred_df_test, y_test_2,\n",
    "                class_weight_dict_1, class_weight_dict_2, joint_batch_size, joint_epochs):\n",
    "    def helper(predict_name, pred_df, y_train, pred_df_test, y_test, class_weight_dict_1, joint_batch_size, joint_epochs):\n",
    "        joint_full_model_1 = models_nn.create_joint_model(pred_df_full)\n",
    "        history_1 = joint_full_model_1.fit(x=pred_df_full, \n",
    "                                           y=y_train_1, \n",
    "                                           epochs=joint_epochs, \n",
    "                                           batch_size=joint_batch_size, \n",
    "                                           validation_data=(pred_df_full_test,y_test_1), \n",
    "                                           #callbacks=[models_nn.callback], \n",
    "                                           class_weight=class_weight_dict_1,\n",
    "                                           verbose=0)\n",
    "        joint_predict_1 = joint_full_model_1.predict(pred_df_full_test)\n",
    "        # joint_predict_round = []\n",
    "        # for a in joint_predict:\n",
    "        #     joint_predict_round.append(np.argmax(a))\n",
    "        joint_predict_round_1 = joint_predict_1.round()\n",
    "        out1 = precision_recall_fscore_support(y_test_1, np.array(joint_predict_round_1), average='macro')\n",
    "\n",
    "        val_f1_list = history_1.history['val_f1_m']\n",
    "        \n",
    "        # if is_max, then take max f1 from history, otherwise take last value\n",
    "        if is_max == True:\n",
    "            best_val_f1 = max(val_f1_list)\n",
    "        else:\n",
    "            best_val_f1 = val_f1_list[-1]\n",
    "        \n",
    "        best_val_prec = history_1.history['val_precision_m'][val_f1_list.index(best_val_f1)]\n",
    "        best_val_recall = history_1.history['val_recall_m'][val_f1_list.index(best_val_f1)]\n",
    "        macro_scores = out1\n",
    "        if weights_name == None:\n",
    "            print(\"Metrics for {} joint model w/o weights:\".format(predict_name))\n",
    "        else:\n",
    "            print(\"Metrics for {} joint model weighted by {}\".format(predict_name, weights_name))\n",
    "        print(\"Best validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(best_val_f1,\n",
    "                                                                                     best_val_prec,\n",
    "                                                                                     best_val_recall))\n",
    "        print(\"Macro validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(macro_scores[2],\n",
    "                                                                                      macro_scores[0],\n",
    "                                                                                      macro_scores[1]))\n",
    "        return [best_val_f1, best_val_prec, best_val_recall], macro_scores\n",
    "    \n",
    "    print(\"#############################################################\")\n",
    "    decep_1, decep_2 = helper(\"Deception\", pred_df_full, y_train_1, pred_df_full_test, y_test_1, class_weight_dict_1, joint_batch_size, joint_epochs)\n",
    "    rapport_1, rapport_2 = helper(\"Rapport\", pred_df, y_train_2, pred_df_test, y_test_2, class_weight_dict_2, joint_batch_size, joint_epochs)\n",
    "\n",
    "    \n",
    "    return decep_1, decep_2, rapport_1, rapport_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Main function for dataset sampling experiments #\n",
    "##################################################\n",
    "\n",
    "# Currently only supports annotations with 2 classes, i.e. binary\n",
    "\n",
    "def dataset_sampling(dataframe, class_name, sampling_size_list, metadata_options_list, model_name, is_drop_rapport):\n",
    "    \n",
    "    # Misc variables\n",
    "    results = {}\n",
    "    \n",
    "    # Model settings (for individual annotation models)\n",
    "    models_nn.MODEL_NAME = model_name\n",
    "    \n",
    "    # Full dataframe proportions\n",
    "    full_size = dataframe.shape[0]\n",
    "    full_counts = dataframe[class_name].value_counts()\n",
    "    print(\"Full dataset proportions w.r.t. {}\".format(class_name))\n",
    "    print(full_counts)\n",
    "    full_counts_dict = full_counts.to_dict()\n",
    "    full_counts_list = list(full_counts_dict.values())\n",
    "    \n",
    "    ## class_proportions is a list of class proportions, first item corresponding to first class, etc\n",
    "    class_proportions = []\n",
    "    for each_class_counts in full_counts_list:\n",
    "        class_proportions.append(each_class_counts / full_size)\n",
    "\n",
    "    # Looping through sample_size_list\n",
    "    for each_sample_size in sampling_size_list:\n",
    "        \n",
    "        print(\"#################################\")\n",
    "        print(\"Sample size: {}\".format(each_sample_size))\n",
    "        print(\"#################################\")\n",
    "        \n",
    "        ## Counting number of datapoints per class proportionate to main dataset\n",
    "        class_sizes = [round(each_sample_size * class_proportions[0])]\n",
    "        class_sizes.append(each_sample_size - class_sizes[0])\n",
    "\n",
    "        ## Creating sub dataframe\n",
    "        s0 = dataframe.loc[dataframe[class_name] == list(full_counts_dict.keys())[0]].sample(class_sizes[0]).index\n",
    "        s1 = dataframe.loc[dataframe[class_name] == list(full_counts_dict.keys())[1]].sample(class_sizes[1]).index\n",
    "        sub_df = dataframe.loc[s0.union(s1)]\n",
    "\n",
    "        # Metadata settings\n",
    "        print(\"#############################################################\")\n",
    "        print(\"Metadata options for current sample\")\n",
    "        df_throughput, df_worktime, df_agreement, df_textlength, df_special = metadata_options.set_OHE_pipeline_options(sub_df, *metadata_options_list)\n",
    "  \n",
    "        ## Train_test_split using SSS\n",
    "        indices_train, indices_test, train, test = sss_train_test_split(sub_df, class_name, n_splits, test_size, random_state)\n",
    "        \n",
    "        ## Generate class weights dict and y_train data (HARD-CODED)\n",
    "        y_train_deception, deception_class_weight_dict = generate_class_weights(train, class_name, \"Input.deception_quadrant\")\n",
    "        y_train_rapport, rapport_class_weight_dict = generate_class_weights(train, class_name, 'Answer.3rapport.yes_label')\n",
    "        y_train_share_information, share_info_class_weight_dict = generate_class_weights(train, class_name, 'Answer.4shareinformation.yes_label')\n",
    "        y_train_reasoning, reasoning_class_weight_dict = generate_class_weights(train, class_name, 'Answer.2reasoning.yes_label')\n",
    "        y_train_gamemove, gamemove_class_weight_dict = generate_class_weights(train, class_name, 'Answer.1gamemove.yes_label')\n",
    "        print(\"#############################################################\")\n",
    "        print(\"Class weights generated\")\n",
    "        print(\"Deception: {} \\nRapport: {} \\nShare Information: {} \\nReasoning: {} \\nGamemove: {}\".format(deception_class_weight_dict,\n",
    "                                                                                                          rapport_class_weight_dict,\n",
    "                                                                                                          share_info_class_weight_dict,\n",
    "                                                                                                          reasoning_class_weight_dict,\n",
    "                                                                                                          gamemove_class_weight_dict))\n",
    "        \n",
    "        ## Train and test data preparation (HARD-CODED)\n",
    "        X_train_col = train['Input.full_text']\n",
    "        \n",
    "        new_deception_test = test[\"Input.deception_quadrant\"].copy()\n",
    "        new_deception_test['Input.deception_quadrant'] = test[\"Input.deception_quadrant\"].apply(lambda x : 1 if x == \"Straightforward\" else 0)\n",
    "        y_test_deception = new_deception_test['Input.deception_quadrant'].tolist()\n",
    "        y_test_rapport = test['Answer.3rapport.yes_label'].tolist()\n",
    "        y_test_share_information = test['Answer.4shareinformation.yes_label'].tolist()\n",
    "        y_test_reasoning = test['Answer.2reasoning.yes_label'].tolist()\n",
    "        y_test_gamemove = test['Answer.1gamemove.yes_label'].tolist()\n",
    "        X_test_col = test['Input.full_text']\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "\n",
    "        y_train_deception = label_preprocessing(y_train_deception, le)\n",
    "        y_train_rapport = label_preprocessing(y_train_rapport, le)\n",
    "        y_train_share_information = label_preprocessing(y_train_share_information, le)\n",
    "        y_train_reasoning = label_preprocessing(y_train_reasoning, le)\n",
    "        y_train_gamemove = label_preprocessing(y_train_gamemove, le)\n",
    "        \n",
    "        y_test_deception = label_preprocessing(y_test_deception, le)\n",
    "        y_test_rapport = label_preprocessing(y_test_rapport, le)\n",
    "        y_test_share_information = label_preprocessing(y_test_share_information, le)\n",
    "        y_test_reasoning = label_preprocessing(y_test_reasoning, le)\n",
    "        y_test_gamemove = label_preprocessing(y_test_gamemove, le)\n",
    "        \n",
    "        ## Tokenizer settings\n",
    "        max_words = 1000\n",
    "        max_len = 220\n",
    "\n",
    "        tok = Tokenizer(num_words=max_words)\n",
    "\n",
    "        tok.fit_on_texts(X_train_col)\n",
    "        X_train_sequences = tok.texts_to_sequences(X_train_col)\n",
    "        X_train = pad_sequences(X_train_sequences, maxlen=max_len)\n",
    "\n",
    "        X_test_sequences = tok.texts_to_sequences(X_test_col)\n",
    "        X_test = pad_sequences(X_test_sequences, maxlen=max_len)\n",
    "        \n",
    "        ## Individual Models (HARD-CODED)\n",
    "        ### Deception pred and pred_test not needed\n",
    "        _, _ = individual_model('Deception', X_train, y_train_deception, X_test, y_test_deception, deception_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        rapport_pred, rapport_pred_test = individual_model('Rapport', X_train, y_train_rapport, X_test, y_test_rapport, rapport_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        shareinfo_pred, shareinfo_pred_test = individual_model('Share Info', X_train, y_train_share_information, X_test, y_test_share_information, share_info_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        reasoning_pred, reasoning_pred_test = individual_model('Reasoning', X_train, y_train_reasoning, X_test, y_test_reasoning, reasoning_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        gamemove_pred, gamemove_pred_test = individual_model('Gamemove', X_train, y_train_gamemove, X_test, y_test_gamemove, gamemove_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        \n",
    "        ## Generate one-hot encodings (HARD-CODED)\n",
    "        pred_df_full, pred_df = generate_encodings(gamemove_pred, reasoning_pred, shareinfo_pred, rapport_pred)\n",
    "        pred_test_df_full, pred_test_df = generate_encodings(gamemove_pred_test, reasoning_pred_test, shareinfo_pred_test, rapport_pred_test)\n",
    "        \n",
    "        if is_drop_rapport:\n",
    "            pred_df_full, pred_test_df_full = pred_df.copy(), pred_test_df.copy()\n",
    "            \n",
    "        ## Generate weighted one-hot encodings (HARD-CODED)\n",
    "        pred_df_full_throughput, pred_df_throughput, pred_df_full_worktime, pred_df_worktime, pred_df_full_agreement, pred_df_agreement, pred_df_full_textlength, pred_df_textlength, pred_df_full_special, pred_df_special = metadata_options.construct_weighted_dataframe(indices_train, df_throughput, df_worktime, df_agreement, df_textlength, df_special, pred_df, pred_df_full)\n",
    "        pred_df_full_throughput_test, pred_df_throughput_test, pred_df_full_worktime_test, pred_df_worktime_test, pred_df_full_agreement_test, pred_df_agreement_test, pred_df_full_textlength_test, pred_df_textlength_test, pred_df_full_special_test, pred_df_special_test = metadata_options.construct_weighted_dataframe(indices_test, df_throughput, df_worktime, df_agreement, df_textlength, df_special, pred_test_df, pred_test_df_full)\n",
    "        print(\"#############################################################\")\n",
    "        print(\"Weighted one-hot encodings generated\")\n",
    "        \n",
    "        ## Joint model w/o weights\n",
    "        out1_wo_weights, _, out2_wo_weights, _ = joint_model(False, None, pred_df_full, y_train_deception, pred_test_df_full, y_test_deception,\n",
    "                                                             pred_df, y_train_rapport, pred_test_df, y_test_rapport,\n",
    "                                                             deception_class_weight_dict, rapport_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by Throughput\n",
    "        out1_tp, _, out2_tp, _ = joint_model(False, 'Throughput', pred_df_full_throughput, y_train_deception, pred_df_full_throughput_test, y_test_deception,\n",
    "                                       pred_df_throughput, y_train_rapport, pred_df_throughput_test, y_test_rapport,\n",
    "                                       deception_class_weight_dict, rapport_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by Worktime\n",
    "        out1_wt, _, out2_wt, _ = joint_model(False, 'Worktime', pred_df_full_worktime, y_train_deception, pred_df_full_worktime_test, y_test_deception,\n",
    "                                       pred_df_worktime, y_train_rapport, pred_df_worktime_test, y_test_rapport,\n",
    "                                       deception_class_weight_dict, rapport_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by PC Agreement\n",
    "        out1_pc, _, out2_pc, _ = joint_model(False, 'PC Agreement', pred_df_full_agreement, y_train_deception, pred_df_full_agreement_test, y_test_deception,\n",
    "                                       pred_df_agreement, y_train_rapport, pred_df_agreement_test, y_test_rapport,\n",
    "                                       deception_class_weight_dict, rapport_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by Text Length\n",
    "        out1_tl, _, out2_tl, _ = joint_model(False, 'Text Length', pred_df_full_textlength, y_train_deception, pred_df_full_textlength_test, y_test_deception,\n",
    "                                       pred_df_textlength, y_train_rapport, pred_df_textlength_test, y_test_rapport,\n",
    "                                       deception_class_weight_dict, rapport_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by Special options\n",
    "        out1_sp, _, out2_sp, _ = joint_model(False, 'Special', pred_df_full_special, y_train_deception, pred_df_full_special_test, y_test_deception,\n",
    "                                       pred_df_special, y_train_rapport, pred_df_special_test, y_test_rapport,\n",
    "                                       deception_class_weight_dict, rapport_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        results['run_' + str(each_sample_size)] = [out1_wo_weights, out2_wo_weights, out1_tp, out2_tp, out1_wt, out2_wt, \n",
    "                                                   out1_pc, out2_pc, out1_tl, out2_tl, out1_sp, out2_sp]\n",
    "    print(\"Done\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Arguments for current experiment #\n",
    "####################################\n",
    "\n",
    "# Metadata options\n",
    "throughput_option = 'TP3'\n",
    "worktime_option = 'WT2'\n",
    "pc_agreement_option = 'PC2'\n",
    "textlength_option = 'TL2'\n",
    "special_option = 'SP3'\n",
    "k_option_for_tp = 3\n",
    "metadata_options_choices = [throughput_option, worktime_option, pc_agreement_option, textlength_option, special_option, k_option_for_tp]\n",
    "\n",
    "# Train_test_split SSS options\n",
    "n_splits = 1\n",
    "test_size = 0.2\n",
    "random_state = 0\n",
    "\n",
    "# Individual model options\n",
    "model_name = 'lstm-attn'\n",
    "indiv_batch_size = 128\n",
    "indiv_epochs = 50\n",
    "\n",
    "# Joint model options\n",
    "joint_batch_size = 64\n",
    "joint_epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampling_sizes = [250, 500, 750, \n",
    "                  1000, 1250, 1500, 1750, \n",
    "                  2000, 2250, 2500, 2750,\n",
    "                  3000, 3500, \n",
    "                  4000, 4500, \n",
    "                  5000, 5500, \n",
    "                  6000, 6500, \n",
    "                  7000, 7500, \n",
    "                  8000, 8500, \n",
    "                  9000, 9500, \n",
    "                  10000, 10500, 11000, 11366]\n",
    "# sampling_sizes = [11366]\n",
    "\n",
    "results_dict = dataset_sampling(dataframe=df, \n",
    "                                class_name=\"Input.deception_quadrant\", \n",
    "                                sampling_size_list=sampling_sizes, \n",
    "                                metadata_options_list=metadata_options_choices, \n",
    "                                model_name=model_name,\n",
    "                                is_drop_rapport=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_into_pandas(results_dictionary, metric_of_focus):\n",
    "    def helper(metric_of_focus_number):\n",
    "        new_dict = {}\n",
    "        for each_key, each_values_list in results_dictionary.items():\n",
    "            new_dict[each_key[4:]] = []\n",
    "            for each in each_values_list:\n",
    "                new_dict[each_key[4:]].append(each[metric_of_focus_number])\n",
    "        out_df = pd.DataFrame.from_dict(new_dict)\n",
    "        return out_df\n",
    "    \n",
    "    if metric_of_focus == 'F1':\n",
    "        metric_of_focus_number = 0\n",
    "    elif metric_of_focus == 'Precision':\n",
    "        metric_of_focus_number = 1\n",
    "    elif metric_of_focus == 'Recall':\n",
    "        metric_of_focus_number = 2\n",
    "        \n",
    "    return helper(metric_of_focus_number)\n",
    "\n",
    "metric_of_focus = ['F1', 'Precision', 'Recall']\n",
    "\n",
    "for metric in metric_of_focus:\n",
    "    experiment_df = translate_into_pandas(results_dict, metric)\n",
    "    results_name = \"./output/dataset_sampling_\" + str(len(sampling_sizes)) + '_' + str(metric) + \"_pts_last_new\"\n",
    "    experiment_df.to_csv(results_name + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_plot_df = experiment_df.T.reset_index()\n",
    "rename_col_names = {0: 'Deception w/o weights',\n",
    "                    1: 'Rapport w/o weights',\n",
    "                    2: 'Deception by TP',\n",
    "                    3: 'Rapport by TP',\n",
    "                    4: 'Deception by WT',\n",
    "                    5: 'Rapport by WT',\n",
    "                    6: 'Deception by PC',\n",
    "                    7: 'Rapport by PC',\n",
    "                    8: 'Deception by TL',\n",
    "                    9: 'Rapport by TL',\n",
    "                    10: 'Deception by SP',\n",
    "                    11: 'Rapport by SP'}\n",
    "exp_plot_df = exp_plot_df.rename(columns=rename_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = exp_plot_df.plot('index',list(exp_plot_df.columns)[1:],style='.-', figsize=(12,9), colormap='tab20')\n",
    "plot.set_xlabel('Sample size', size=10)\n",
    "plot.set_ylabel('F1 Scores', size=10)\n",
    "lgd = plot.legend(loc='center left',bbox_to_anchor=(1.0, 0.5), borderaxespad=0.)\n",
    "plot = plot.get_figure()\n",
    "plot.savefig(results_name + '.jpg', bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df = pd.read_csv(\"/home/kaiyuan/Desktop/ky_code/affcon/diplomacy-betrayal/output/dataset_sampling_29_F1_pts_last_new.csv\")\n",
    "\n",
    "predict_option = \"deception\"\n",
    "\n",
    "if predict_option == \"rapport\":\n",
    "    row_names = [0,2,4,6,8,10]\n",
    "else:\n",
    "    row_names = [1,3,5,7,9,11]\n",
    "\n",
    "experiment_df = experiment_df.drop(row_names)\n",
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_plot_df = experiment_df.T.reset_index()\n",
    "rename_col_names = {0: 'Deception w/o weights',\n",
    "                    1: 'Rapport w/o weights',\n",
    "                    2: 'Deception by TP',\n",
    "                    3: 'Rapport by TP',\n",
    "                    4: 'Deception by WT',\n",
    "                    5: 'Rapport by WT',\n",
    "                    6: 'Deception by PC',\n",
    "                    7: 'Rapport by PC',\n",
    "                    8: 'Deception by TL',\n",
    "                    9: 'Rapport by TL',\n",
    "                    10: 'Deception by SP',\n",
    "                    11: 'Rapport by SP'}\n",
    "exp_plot_df = exp_plot_df.rename(columns=rename_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = exp_plot_df.plot('index',list(exp_plot_df.columns)[1:],style='.-', figsize=(12,9), colormap='tab20')\n",
    "plot.set_xlabel('Sample size', size=10)\n",
    "plot.set_ylabel('F1 Scores', size=10)\n",
    "lgd = plot.legend(loc='center left',bbox_to_anchor=(1.0, 0.5), borderaxespad=0.)\n",
    "plot = plot.get_figure()\n",
    "plot.savefig(results_name + '.jpg', bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
