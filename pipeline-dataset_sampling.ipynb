{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from metadata_options.ipynb\n",
      "importing Jupyter notebook from models_nn.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Input, InputLayer, Dropout, Dense, Flatten, Embedding, Add, Concatenate\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "## Own code \n",
    "import import_ipynb\n",
    "import metadata_options\n",
    "import models_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with Throughput & WorkTime\n",
    "df = pd.read_csv('./data/kokil dec 6 reprepare/conf_pc_worker_sem.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "## Model Options ##\n",
    "######################################\n",
    "# options: lstm, cnn, lstm-attn\n",
    "\n",
    "model_name = 'lstm-attn'\n",
    "models_nn.MODEL_NAME = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input.sentence_id</th>\n",
       "      <th>HITId</th>\n",
       "      <th>Input.convo_id</th>\n",
       "      <th>Input.train_test_val</th>\n",
       "      <th>Input.msg_id</th>\n",
       "      <th>Input.timestamp</th>\n",
       "      <th>Input.full_text</th>\n",
       "      <th>Input.speaker</th>\n",
       "      <th>Input.reply_to</th>\n",
       "      <th>Input.speaker_intention</th>\n",
       "      <th>...</th>\n",
       "      <th>prt</th>\n",
       "      <th>punct</th>\n",
       "      <th>purpcl</th>\n",
       "      <th>quantmod</th>\n",
       "      <th>rcmod</th>\n",
       "      <th>rel</th>\n",
       "      <th>root</th>\n",
       "      <th>tmod</th>\n",
       "      <th>xcomp</th>\n",
       "      <th>xsubj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>3MG8450X2OASXZ0WO9O5AH70GU3UPA</td>\n",
       "      <td>Game1-italy-germany</td>\n",
       "      <td>Train</td>\n",
       "      <td>Game1-italy-germany-3</td>\n",
       "      <td>87</td>\n",
       "      <td>It seems like there are a lot of ways that cou...</td>\n",
       "      <td>germany-Game1</td>\n",
       "      <td>Game1-italy-germany-2</td>\n",
       "      <td>Truth</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>38G0E1M85M552JXSALX4G9WI2I6UVX</td>\n",
       "      <td>Game1-italy-germany</td>\n",
       "      <td>Train</td>\n",
       "      <td>Game1-italy-germany-7</td>\n",
       "      <td>117</td>\n",
       "      <td>Sorry Italy I've been away doing, um, German t...</td>\n",
       "      <td>germany-Game1</td>\n",
       "      <td>Game1-italy-germany-6</td>\n",
       "      <td>Truth</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>3HYV4299H0WQ2B4TCS7PKDQ75WHE81</td>\n",
       "      <td>Game1-italy-germany</td>\n",
       "      <td>Train</td>\n",
       "      <td>Game1-italy-germany-8</td>\n",
       "      <td>119</td>\n",
       "      <td>I don't think I'm ready to go for that idea, h...</td>\n",
       "      <td>germany-Game1</td>\n",
       "      <td>Game1-italy-germany-7</td>\n",
       "      <td>Truth</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>3XU9MCX6VOC4P079IHIO9TCNYLGR2P</td>\n",
       "      <td>Game1-italy-germany</td>\n",
       "      <td>Train</td>\n",
       "      <td>Game1-italy-germany-9</td>\n",
       "      <td>121</td>\n",
       "      <td>I am pretty conflicted about whether to guess ...</td>\n",
       "      <td>italy-Game1</td>\n",
       "      <td>Game1-italy-germany-8</td>\n",
       "      <td>Truth</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>3FVBZG9CLJEK4WQS7P2GC1H2EEQH0Q</td>\n",
       "      <td>Game1-italy-germany</td>\n",
       "      <td>Train</td>\n",
       "      <td>Game1-italy-germany-9</td>\n",
       "      <td>121</td>\n",
       "      <td>I am going to take it literally and say  even ...</td>\n",
       "      <td>italy-Game1</td>\n",
       "      <td>Game1-italy-germany-8</td>\n",
       "      <td>Truth</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 862 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Input.sentence_id                           HITId       Input.convo_id  \\\n",
       "5                 11  3MG8450X2OASXZ0WO9O5AH70GU3UPA  Game1-italy-germany   \n",
       "6                 12  38G0E1M85M552JXSALX4G9WI2I6UVX  Game1-italy-germany   \n",
       "7                 14  3HYV4299H0WQ2B4TCS7PKDQ75WHE81  Game1-italy-germany   \n",
       "8                 15  3XU9MCX6VOC4P079IHIO9TCNYLGR2P  Game1-italy-germany   \n",
       "9                 16  3FVBZG9CLJEK4WQS7P2GC1H2EEQH0Q  Game1-italy-germany   \n",
       "\n",
       "  Input.train_test_val           Input.msg_id  Input.timestamp  \\\n",
       "5                Train  Game1-italy-germany-3               87   \n",
       "6                Train  Game1-italy-germany-7              117   \n",
       "7                Train  Game1-italy-germany-8              119   \n",
       "8                Train  Game1-italy-germany-9              121   \n",
       "9                Train  Game1-italy-germany-9              121   \n",
       "\n",
       "                                     Input.full_text  Input.speaker  \\\n",
       "5  It seems like there are a lot of ways that cou...  germany-Game1   \n",
       "6  Sorry Italy I've been away doing, um, German t...  germany-Game1   \n",
       "7  I don't think I'm ready to go for that idea, h...  germany-Game1   \n",
       "8  I am pretty conflicted about whether to guess ...    italy-Game1   \n",
       "9  I am going to take it literally and say  even ...    italy-Game1   \n",
       "\n",
       "          Input.reply_to Input.speaker_intention  ...  prt punct  purpcl  \\\n",
       "5  Game1-italy-germany-2                   Truth  ...  0.0   0.0     0.0   \n",
       "6  Game1-italy-germany-6                   Truth  ...  0.0   0.0     0.0   \n",
       "7  Game1-italy-germany-7                   Truth  ...  0.0   0.0     0.0   \n",
       "8  Game1-italy-germany-8                   Truth  ...  0.0   0.0     0.0   \n",
       "9  Game1-italy-germany-8                   Truth  ...  0.0   0.0     0.0   \n",
       "\n",
       "   quantmod  rcmod  rel  root  tmod xcomp  xsubj  \n",
       "5       0.0    1.0  0.0   1.0   0.0   0.0    0.0  \n",
       "6       0.0    0.0  0.0   1.0   0.0   0.0    0.0  \n",
       "7       0.0    0.0  0.0   1.0   0.0   1.0    0.0  \n",
       "8       0.0    0.0  0.0   1.0   0.0   0.0    0.0  \n",
       "9       0.0    0.0  0.0   1.0   0.0   2.0    1.0  \n",
       "\n",
       "[5 rows x 862 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# pipeline-onehot Function blocks #\n",
    "###################################\n",
    "\n",
    "def sss_train_test_split(dataframe, class_name, n_splits, test_size, random_state):\n",
    "    y = dataframe[class_name].copy()\n",
    "    X = dataframe.drop([class_name], axis=1)\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    splits_generator = sss.split(X, y)\n",
    "\n",
    "    for train_idx, test_idx in splits_generator:\n",
    "        indices_train = train_idx\n",
    "        indices_test = test_idx\n",
    "\n",
    "    train = df.take(indices_train)\n",
    "    test = df.take(indices_test)\n",
    "    \n",
    "    return indices_train, indices_test, train, test\n",
    "\n",
    "def generate_class_weights(train_data, class_name, annotation_name):\n",
    "    # Check if first class label is numeric or alphabetic\n",
    "    if class_name == annotation_name:\n",
    "        ## Convert to int type\n",
    "        tmp_y_train = train_data[annotation_name].copy()\n",
    "        tmp_y_train[annotation_name] = train_data[annotation_name].apply(lambda x : 1 if x == \"Straightforward\" else 0)\n",
    "        y_train = tmp_y_train[annotation_name].to_numpy()\n",
    "    else:\n",
    "        y_train = train_data[annotation_name].to_numpy()\n",
    "    \n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    return y_train, class_weight_dict\n",
    "\n",
    "def label_preprocessing(y_data, label_encoder):\n",
    "    out = label_encoder.fit_transform(y_data).reshape(-1,1)\n",
    "    return out\n",
    "\n",
    "def individual_model(annot_name, x_train_data, y_train_data, x_val_data, y_val_data, class_weight_dict, indiv_batch_size, indiv_epochs):\n",
    "    model = models_nn.create_nn_model()\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = RMSprop(),\n",
    "                  metrics = ['accuracy', models_nn.f1_m, models_nn.recall_m, models_nn.precision_m])\n",
    "    history = model.fit(x_train_data,y_train_data,\n",
    "                        batch_size=indiv_batch_size,\n",
    "                        epochs=indiv_epochs,\n",
    "                        validation_data=(x_val_data, y_val_data), \n",
    "                        callbacks=[models_nn.early_stop],\n",
    "                        class_weight=class_weight_dict,\n",
    "                        verbose=0)\n",
    "    \n",
    "    pred = model.predict(x_train_data)\n",
    "    pred_test = model.predict(x_val_data)\n",
    "\n",
    "    pred_test_round = pred_test.round()\n",
    "    \n",
    "    validation_metrics_dict = history.history\n",
    "    val_f1_list = history.history['val_f1_m']\n",
    "    best_val_f1 = max(val_f1_list)\n",
    "    best_val_prec = history.history['val_precision_m'][val_f1_list.index(best_val_f1)]\n",
    "    best_val_recall = history.history['val_recall_m'][val_f1_list.index(best_val_f1)]\n",
    "    macro_scores = precision_recall_fscore_support(y_val_data, pred_test_round, average='macro')\n",
    "    print(\"#############################################################\")\n",
    "    print(\"Metrics for {} individual model:\".format(annot_name))\n",
    "    print(\"Best validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(best_val_f1,\n",
    "                                                                                 best_val_prec,\n",
    "                                                                                 best_val_recall))\n",
    "    print(\"Macro validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(macro_scores[2],\n",
    "                                                                                  macro_scores[0],\n",
    "                                                                                  macro_scores[1]))\n",
    "    return pred, pred_test\n",
    "\n",
    "# (HARD-CODED)\n",
    "def generate_encodings(gamemove_pred, reasoning_pred, shareinfo_pred, rapport_pred):\n",
    "    pred_df_arr_full = []\n",
    "    pred_df_arr = []\n",
    "    for i in range(0, len(gamemove_pred)):\n",
    "        pred_obj_1 = {}\n",
    "        pred_obj_1['gamemove'] = gamemove_pred[i][0]\n",
    "        pred_obj_1['reasoning'] = reasoning_pred[i][0]\n",
    "        pred_obj_1['shareinfo'] = shareinfo_pred[i][0]\n",
    "        pred_df_arr.append(pred_obj_1)\n",
    "\n",
    "        pred_obj_2 = pred_obj_1.copy()\n",
    "        pred_obj_2['rapport'] = rapport_pred[i][0]\n",
    "        pred_df_arr_full.append(pred_obj_2)\n",
    "\n",
    "    pred_df_full = pd.DataFrame(pred_df_arr_full)\n",
    "    pred_df = pd.DataFrame(pred_df_arr)\n",
    "    return pred_df_full, pred_df\n",
    "\n",
    "def joint_model(weights_name, pred_df_full, y_train_1, pred_df_full_test, y_test_1,\n",
    "                pred_df, y_train_2, pred_df_test, y_test_2,\n",
    "                class_weight_dict_1, class_weight_dict_2, joint_batch_size, joint_epochs):\n",
    "    def helper(predict_name, pred_df, y_train, pred_df_test, y_test, class_weight_dict_1, joint_batch_size, joint_epochs):\n",
    "        joint_full_model_1 = models_nn.create_joint_model(pred_df_full)\n",
    "        history_1 = joint_full_model_1.fit(x=pred_df_full, \n",
    "                                           y=y_train_1, \n",
    "                                           epochs=joint_epochs, \n",
    "                                           batch_size=joint_batch_size, \n",
    "                                           validation_data=(pred_df_full_test,y_test_1), \n",
    "                                           #callbacks=[models_nn.callback], \n",
    "                                           class_weight=class_weight_dict_1,\n",
    "                                           verbose=0)\n",
    "        joint_predict_1 = joint_full_model_1.predict(pred_df_full_test)\n",
    "        joint_predict_round_1 = []\n",
    "        for a in joint_predict_1:\n",
    "            joint_predict_round_1.append(np.argmax(a))\n",
    "        out1 = precision_recall_fscore_support(y_test_1, np.array(joint_predict_round_1), average='macro')\n",
    "\n",
    "        val_f1_list = history_1.history['val_f1_m']\n",
    "        best_val_f1 = max(val_f1_list)\n",
    "        best_val_prec = history_1.history['val_precision_m'][val_f1_list.index(best_val_f1)]\n",
    "        best_val_recall = history_1.history['val_recall_m'][val_f1_list.index(best_val_f1)]\n",
    "        macro_scores = out1\n",
    "        if weights_name == None:\n",
    "            print(\"Metrics for {} joint model w/o weights:\".format(predict_name))\n",
    "        else:\n",
    "            print(\"Metrics for {} joint model weighted by {}\".format(predict_name, weights_name))\n",
    "        print(\"Best validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(best_val_f1,\n",
    "                                                                                     best_val_prec,\n",
    "                                                                                     best_val_recall))\n",
    "        print(\"Macro validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(macro_scores[2],\n",
    "                                                                                      macro_scores[0],\n",
    "                                                                                      macro_scores[1]))\n",
    "        return [best_val_f1, best_val_prec, best_val_recall], macro_scores\n",
    "    \n",
    "    print(\"#############################################################\")\n",
    "    decep_1, decep_2 = helper(\"Deception\", pred_df_full, y_train_1, pred_df_full_test, y_test_1, class_weight_dict_1, joint_batch_size, joint_epochs)\n",
    "    rapport_1, rapport_2 = helper(\"Rapport\", pred_df, y_train_2, pred_df_test, y_test_2, class_weight_dict_2, joint_batch_size, joint_epochs)\n",
    "\n",
    "    \n",
    "    return decep_1, decep_2, rapport_1, rapport_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Main function for dataset sampling experiments #\n",
    "##################################################\n",
    "\n",
    "# Currently only supports annotations with 2 classes, i.e. binary\n",
    "\n",
    "def dataset_sampling(dataframe, class_name, sampling_size_list, metadata_options_list, model_name):\n",
    "    \n",
    "    # Misc variables\n",
    "    results = {}\n",
    "    \n",
    "    # Model settings (for individual annotation models)\n",
    "    models_nn.MODEL_NAME = model_name\n",
    "    \n",
    "    # Full dataframe proportions\n",
    "    full_size = dataframe.shape[0]\n",
    "    full_counts = dataframe[class_name].value_counts()\n",
    "    print(\"Full dataset proportions w.r.t. {}\".format(class_name))\n",
    "    print(full_counts)\n",
    "    full_counts_dict = full_counts.to_dict()\n",
    "    full_counts_list = list(full_counts_dict.values())\n",
    "    \n",
    "    ## class_proportions is a list of class proportions, first item corresponding to first class, etc\n",
    "    class_proportions = []\n",
    "    for each_class_counts in full_counts_list:\n",
    "        class_proportions.append(each_class_counts / full_size)\n",
    "\n",
    "    # Looping through sample_size_list\n",
    "    for each_sample_size in sampling_size_list:\n",
    "        \n",
    "        print(\"#################################\")\n",
    "        print(\"Sample size: {}\".format(each_sample_size))\n",
    "        print(\"#################################\")\n",
    "        \n",
    "        ## Counting number of datapoints per class proportionate to main dataset\n",
    "        class_sizes = [round(each_sample_size * class_proportions[0])]\n",
    "        class_sizes.append(each_sample_size - class_sizes[0])\n",
    "\n",
    "        ## Creating sub dataframe\n",
    "        s0 = dataframe.loc[dataframe[class_name] == list(full_counts_dict.keys())[0]].sample(class_sizes[0]).index\n",
    "        s1 = dataframe.loc[dataframe[class_name] == list(full_counts_dict.keys())[1]].sample(class_sizes[1]).index\n",
    "        sub_df = dataframe.loc[s0.union(s1)]\n",
    "\n",
    "        # Metadata settings\n",
    "        print(\"#############################################################\")\n",
    "        print(\"Metadata options for current sample\")\n",
    "        df_throughput, df_worktime, df_agreement, df_textlength, df_special = metadata_options.set_OHE_pipeline_options(sub_df, *metadata_options_list)\n",
    "  \n",
    "        ## Train_test_split using SSS\n",
    "        indices_train, indices_test, train, test = sss_train_test_split(sub_df, class_name, n_splits, test_size, random_state)\n",
    "        \n",
    "        ## Generate class weights dict and y_train data (HARD-CODED)\n",
    "        y_train_deception, deception_class_weight_dict = generate_class_weights(train, class_name, \"Input.deception_quadrant\")\n",
    "        y_train_rapport, rapport_class_weight_dict = generate_class_weights(train, class_name, 'Answer.3rapport.yes_label')\n",
    "        y_train_share_information, share_info_class_weight_dict = generate_class_weights(train, class_name, 'Answer.4shareinformation.yes_label')\n",
    "        y_train_reasoning, reasoning_class_weight_dict = generate_class_weights(train, class_name, 'Answer.2reasoning.yes_label')\n",
    "        y_train_gamemove, gamemove_class_weight_dict = generate_class_weights(train, class_name, 'Answer.1gamemove.yes_label')\n",
    "        print(\"#############################################################\")\n",
    "        print(\"Class weights generated\")\n",
    "        print(\"Deception: {} \\nRapport: {} \\nShare Information: {} \\nReasoning: {} \\nGamemove: {}\".format(deception_class_weight_dict,\n",
    "                                                                                                          rapport_class_weight_dict,\n",
    "                                                                                                          share_info_class_weight_dict,\n",
    "                                                                                                          reasoning_class_weight_dict,\n",
    "                                                                                                          gamemove_class_weight_dict))\n",
    "        \n",
    "        ## Train and test data preparation (HARD-CODED)\n",
    "        X_train_col = train['Input.full_text']\n",
    "        \n",
    "        new_deception_test = test[\"Input.deception_quadrant\"].copy()\n",
    "        new_deception_test['Input.deception_quadrant'] = test[\"Input.deception_quadrant\"].apply(lambda x : 1 if x == \"Straightforward\" else 0)\n",
    "        y_test_deception = new_deception_test['Input.deception_quadrant'].tolist()\n",
    "        y_test_rapport = test['Answer.3rapport.yes_label'].tolist()\n",
    "        y_test_share_information = test['Answer.4shareinformation.yes_label'].tolist()\n",
    "        y_test_reasoning = test['Answer.2reasoning.yes_label'].tolist()\n",
    "        y_test_gamemove = test['Answer.1gamemove.yes_label'].tolist()\n",
    "        X_test_col = test['Input.full_text']\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "\n",
    "        y_train_deception = label_preprocessing(y_train_deception, le)\n",
    "        y_train_rapport = label_preprocessing(y_train_rapport, le)\n",
    "        y_train_share_information = label_preprocessing(y_train_share_information, le)\n",
    "        y_train_reasoning = label_preprocessing(y_train_reasoning, le)\n",
    "        y_train_gamemove = label_preprocessing(y_train_gamemove, le)\n",
    "        \n",
    "        y_test_deception = label_preprocessing(y_test_deception, le)\n",
    "        y_test_rapport = label_preprocessing(y_test_rapport, le)\n",
    "        y_test_share_information = label_preprocessing(y_test_share_information, le)\n",
    "        y_test_reasoning = label_preprocessing(y_test_reasoning, le)\n",
    "        y_test_gamemove = label_preprocessing(y_test_gamemove, le)\n",
    "        \n",
    "        ## Tokenizer settings\n",
    "        max_words = 1000\n",
    "        max_len = 220\n",
    "\n",
    "        tok = Tokenizer(num_words=max_words)\n",
    "\n",
    "        tok.fit_on_texts(X_train_col)\n",
    "        X_train_sequences = tok.texts_to_sequences(X_train_col)\n",
    "        X_train = pad_sequences(X_train_sequences, maxlen=max_len)\n",
    "\n",
    "        X_test_sequences = tok.texts_to_sequences(X_test_col)\n",
    "        X_test = pad_sequences(X_test_sequences, maxlen=max_len)\n",
    "        \n",
    "        ## Individual Models (HARD-CODED)\n",
    "        ### Deception pred and pred_test not needed\n",
    "        _, _ = individual_model('Deception', X_train, y_train_deception, X_test, y_test_deception, deception_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        rapport_pred, rapport_pred_test = individual_model('Rapport', X_train, y_train_rapport, X_test, y_test_rapport, rapport_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        shareinfo_pred, shareinfo_pred_test = individual_model('Share Info', X_train, y_train_share_information, X_test, y_test_share_information, share_info_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        reasoning_pred, reasoning_pred_test = individual_model('Reasoning', X_train, y_train_reasoning, X_test, y_test_reasoning, reasoning_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        gamemove_pred, gamemove_pred_test = individual_model('Gamemove', X_train, y_train_gamemove, X_test, y_test_gamemove, gamemove_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        \n",
    "        ## Generate one-hot encodings (HARD-CODED)\n",
    "        pred_df_full, pred_df = generate_encodings(gamemove_pred, reasoning_pred, shareinfo_pred, rapport_pred)\n",
    "        pred_test_df_full, pred_test_df = generate_encodings(gamemove_pred_test, reasoning_pred_test, shareinfo_pred_test, rapport_pred_test)\n",
    "        \n",
    "        ## Generate weighted one-hot encodings (HARD-CODED)\n",
    "        pred_df_full_throughput, pred_df_throughput, pred_df_full_worktime, pred_df_worktime, pred_df_full_agreement, pred_df_agreement, pred_df_full_textlength, pred_df_textlength, pred_df_full_special, pred_df_special = metadata_options.construct_weighted_dataframe(indices_train, df_throughput, df_worktime, df_agreement, df_textlength, df_special, pred_df, pred_df_full)\n",
    "        pred_df_full_throughput_test, pred_df_throughput_test, pred_df_full_worktime_test, pred_df_worktime_test, pred_df_full_agreement_test, pred_df_agreement_test, pred_df_full_textlength_test, pred_df_textlength_test, pred_df_full_special_test, pred_df_special_test = metadata_options.construct_weighted_dataframe(indices_test, df_throughput, df_worktime, df_agreement, df_textlength, df_special, pred_test_df, pred_test_df_full)\n",
    "        print(\"#############################################################\")\n",
    "        print(\"Weighted one-hot encodings generated\")\n",
    "        \n",
    "        ## Joint model w/o weights\n",
    "        out1_wo_weights, _, out2_wo_weights, _ = joint_model(None, pred_df_full, y_train_deception, pred_test_df_full, y_test_deception,\n",
    "                                                       pred_df, y_train_rapport, pred_test_df, y_test_rapport,\n",
    "                                                       deception_class_weight_dict, rapport_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by Throughput\n",
    "        out1_tp, _, out2_tp, _ = joint_model('Throughput', pred_df_full_throughput, y_train_deception, pred_df_full_throughput_test, y_test_deception,\n",
    "                                       pred_df_throughput, y_train_rapport, pred_df_throughput_test, y_test_rapport,\n",
    "                                       deception_class_weight_dict, rapport_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by Worktime\n",
    "        out1_wt, _, out2_wt, _ = joint_model('Worktime', pred_df_full_worktime, y_train_deception, pred_df_full_worktime_test, y_test_deception,\n",
    "                                       pred_df_worktime, y_train_rapport, pred_df_worktime_test, y_test_rapport,\n",
    "                                       deception_class_weight_dict, rapport_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by PC Agreement\n",
    "        out1_pc, _, out2_pc, _ = joint_model('PC Agreement', pred_df_full_agreement, y_train_deception, pred_df_full_agreement_test, y_test_deception,\n",
    "                                       pred_df_agreement, y_train_rapport, pred_df_agreement_test, y_test_rapport,\n",
    "                                       deception_class_weight_dict, rapport_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by Text Length\n",
    "        out1_tl, _, out2_tl, _ = joint_model('Text Length', pred_df_full_textlength, y_train_deception, pred_df_full_textlength_test, y_test_deception,\n",
    "                                       pred_df_textlength, y_train_rapport, pred_df_textlength_test, y_test_rapport,\n",
    "                                       deception_class_weight_dict, rapport_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by Special options\n",
    "        out1_sp, _, out2_sp, _ = joint_model('Special', pred_df_full_special, y_train_deception, pred_df_full_special_test, y_test_deception,\n",
    "                                       pred_df_special, y_train_rapport, pred_df_special_test, y_test_rapport,\n",
    "                                       deception_class_weight_dict, rapport_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        results['run_' + str(each_sample_size)] = [out1_wo_weights, out2_wo_weights, out1_tp, out2_tp, out1_wt, out2_wt, \n",
    "                                                   out1_pc, out2_pc, out1_tl, out2_tl, out1_sp, out2_sp]\n",
    "    print(\"Done\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Arguments for current experiment #\n",
    "####################################\n",
    "\n",
    "# Metadata options\n",
    "throughput_option = 'TP3'\n",
    "worktime_option = 'WT1'\n",
    "pc_agreement_option = 'PC3'\n",
    "textlength_option = 'TL1'\n",
    "special_option = 'SP3'\n",
    "k_option_for_tp = 1\n",
    "metadata_options_choices = [throughput_option, worktime_option, pc_agreement_option, textlength_option, special_option, k_option_for_tp]\n",
    "\n",
    "# Train_test_split SSS options\n",
    "n_splits = 1\n",
    "test_size = 0.2\n",
    "random_state = 0\n",
    "\n",
    "# Individual model options\n",
    "model_name = 'lstm-attn'\n",
    "indiv_batch_size = 128\n",
    "indiv_epochs = 15\n",
    "\n",
    "# Joint model options\n",
    "joint_batch_size = 64\n",
    "joint_epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset proportions w.r.t. Input.deception_quadrant\n",
      "Straightforward    10808\n",
      "Cassandra            558\n",
      "Name: Input.deception_quadrant, dtype: int64\n",
      "#################################\n",
      "Sample size: 11366\n",
      "#################################\n",
      "#############################################################\n",
      "Metadata options for current sample\n",
      "TP3 + k: weighted by 1 inverted k-power U-shaped variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "Plot below: old throughput (x-axis) vs new throughput (y-axis)\n",
      "WT1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "PC3: weighted by 1 PC agreement weight per annotation in each OHE, i.e. (a, b, c, d) -> (w1*a, w2*b, w3*c, w4*d)\n",
      "TL1: weighted by 1 normalised number of characters per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "SP3: weighted by average of PC1 and PC2 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "#############################################################\n",
      "Class weights generated\n",
      "Deception: {0: 10.192825112107624, 1: 0.525792273883877} \n",
      "Rapport: {0: 3.582348305752561, 1: 0.5811069922024799} \n",
      "Share Information: {0: 3.117969821673525, 1: 0.59549384333246} \n",
      "Reasoning: {0: 2.9596354166666665, 1: 0.6016410799364743} \n",
      "Gamemove: {0: 7.048062015503876, 1: 0.5381792352314431}\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": " [_Derived_]RecvAsync is cancelled.\n\t [[{{node RMSprop/RMSprop/update/ReadVariableOp_2/_53}}]]\n\t [[gradient_tape/functional_3/embedding_1/embedding_lookup/Reshape/_50]] [Op:__inference_train_function_4544]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-591d5b607dd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                 \u001b[0msampling_size_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampling_sizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                                 \u001b[0mmetadata_options_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata_options_choices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                                 model_name=model_name)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-68e960e335d7>\u001b[0m in \u001b[0;36mdataset_sampling\u001b[1;34m(dataframe, class_name, sampling_size_list, metadata_options_list, model_name)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;31m## Individual Models (HARD-CODED)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;31m### Deception pred and pred_test not needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindividual_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Deception'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_deception\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_deception\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeception_class_weight_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[0mrapport_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrapport_pred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindividual_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Rapport'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_rapport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_rapport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrapport_class_weight_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mshareinfo_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshareinfo_pred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindividual_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Share Info'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_share_information\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_share_information\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshare_info_class_weight_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-571d31470568>\u001b[0m in \u001b[0;36mindividual_model\u001b[1;34m(annot_name, x_train_data, y_train_data, x_val_data, y_val_data, class_weight_dict, indiv_batch_size, indiv_epochs)\u001b[0m\n\u001b[0;32m     48\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodels_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                         verbose=0)\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCancelledError\u001b[0m:  [_Derived_]RecvAsync is cancelled.\n\t [[{{node RMSprop/RMSprop/update/ReadVariableOp_2/_53}}]]\n\t [[gradient_tape/functional_3/embedding_1/embedding_lookup/Reshape/_50]] [Op:__inference_train_function_4544]\n\nFunction call stack:\ntrain_function\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sampling_sizes = [250, 500, 750, \n",
    "#                   1000, 1250, 1500, 1750, \n",
    "#                   2000, 2250, 2500, 2750,\n",
    "#                   3000, 3500, \n",
    "#                   4000, 4500, \n",
    "#                   5000, 5500, \n",
    "#                   6000, 6500, \n",
    "#                   7000, 7500, \n",
    "#                   8000, 8500, \n",
    "#                   9000, 9500, \n",
    "#                   10000, 10500, 11000, 11366]\n",
    "sampling_sizes = [11366]\n",
    "\n",
    "results_dict = dataset_sampling(dataframe=df, \n",
    "                                class_name=\"Input.deception_quadrant\", \n",
    "                                sampling_size_list=sampling_sizes, \n",
    "                                metadata_options_list=metadata_options_choices, \n",
    "                                model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_into_pandas(results_dictionary, metric_of_focus):\n",
    "    def helper(metric_of_focus_number):\n",
    "        new_dict = {}\n",
    "        for each_key, each_values_list in results_dictionary.items():\n",
    "            new_dict[each_key[4:]] = []\n",
    "            for each in each_values_list:\n",
    "                new_dict[each_key[4:]].append(each[metric_of_focus_number])\n",
    "        out_df = pd.DataFrame.from_dict(new_dict)\n",
    "        return out_df\n",
    "    \n",
    "    if metric_of_focus == 'F1':\n",
    "        metric_of_focus_number = 0\n",
    "    elif metric_of_focus == 'Precision':\n",
    "        metric_of_focus_number = 1\n",
    "    elif metric_of_focus == 'Recall':\n",
    "        metric_of_focus_number = 2\n",
    "        \n",
    "    return helper(metric_of_focus_number)\n",
    "\n",
    "experiment_df = translate_into_pandas(results_dict, 'Precision')\n",
    "results_name = \"./output/dataset_sampling_\" + str(len(sampling_sizes)) + \"pts\"\n",
    "experiment_df.to_csv(results_name + \".csv\", index=False)\n",
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_plot_df = experiment_df.T.reset_index()\n",
    "rename_col_names = {0: 'Deception w/o weights',\n",
    "                    1: 'Rapport w/o weights',\n",
    "                    2: 'Deception by TP',\n",
    "                    3: 'Rapport by TP',\n",
    "                    4: 'Deception by WT',\n",
    "                    5: 'Rapport by WT',\n",
    "                    6: 'Deception by PC',\n",
    "                    7: 'Rapport by PC',\n",
    "                    8: 'Deception by TL',\n",
    "                    9: 'Rapport by TL',\n",
    "                    10: 'Deception by SP',\n",
    "                    11: 'Rapport by SP'}\n",
    "exp_plot_df = exp_plot_df.rename(columns=rename_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = exp_plot_df.plot('index',list(exp_plot_df.columns)[1:],style='.-', figsize=(12,9))\n",
    "plot.set_xlabel('Sample size', size=10)\n",
    "plot.set_ylabel('F1 Scores', size=10)\n",
    "lgd = plot.legend(loc='center left',bbox_to_anchor=(1.0, 0.5), borderaxespad=0.)\n",
    "plot = plot.get_figure()\n",
    "plot.savefig(results_name + '.jpg', bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
