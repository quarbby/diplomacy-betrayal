{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from popularity_metadata_options.ipynb\n",
      "importing Jupyter notebook from models_nn.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Input, InputLayer, Dropout, Dense, Flatten, Embedding, Add, Concatenate\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "## Own code \n",
    "import import_ipynb\n",
    "import popularity_metadata_options\n",
    "import models_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with Throughput & WorkTime\n",
    "df = pd.read_csv('./data/popularity/mean_merge.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text_x</th>\n",
       "      <th>Emotional_disclosure</th>\n",
       "      <th>Information_disclosure</th>\n",
       "      <th>score_x</th>\n",
       "      <th>emo_disc.1</th>\n",
       "      <th>emo_disc.2</th>\n",
       "      <th>emo_disc.3</th>\n",
       "      <th>emo_disc.4</th>\n",
       "      <th>info_disc.1</th>\n",
       "      <th>info_disc.2</th>\n",
       "      <th>...</th>\n",
       "      <th>Throughput.2_y</th>\n",
       "      <th>Throughput.3_y</th>\n",
       "      <th>Throughput.4_y</th>\n",
       "      <th>WorkTime.1_y</th>\n",
       "      <th>WorkTime.2_y</th>\n",
       "      <th>WorkTime.3_y</th>\n",
       "      <th>WorkTime.4_y</th>\n",
       "      <th>popularity_y</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>''  Alot of people DONT think like that when t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>650</td>\n",
       "      <td>561</td>\n",
       "      <td>92</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Official' would be one way to describe it.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>215</td>\n",
       "      <td>1950</td>\n",
       "      <td>3272</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>11</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"...you mix me a cocktail.\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>1235</td>\n",
       "      <td>3272</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Be kind, for everyone you meet is fighting a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1557</td>\n",
       "      <td>2526</td>\n",
       "      <td>872</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Consider yourself lucky because I chose you\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2505</td>\n",
       "      <td>48</td>\n",
       "      <td>478</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         full_text_x  Emotional_disclosure  \\\n",
       "0  ''  Alot of people DONT think like that when t...                     0   \n",
       "1        'Official' would be one way to describe it.                     1   \n",
       "2                        \"...you mix me a cocktail.\"                     0   \n",
       "3  \"Be kind, for everyone you meet is fighting a ...                     0   \n",
       "4     \"Consider yourself lucky because I chose you\"?                     0   \n",
       "\n",
       "   Information_disclosure  score_x  emo_disc.1  emo_disc.2  emo_disc.3  \\\n",
       "0                       0        2        True        True        True   \n",
       "1                       0        2       False       False       False   \n",
       "2                       0        2        True        True        True   \n",
       "3                       0        3        True        True        True   \n",
       "4                       0        1        True        True        True   \n",
       "\n",
       "   emo_disc.4  info_disc.1  info_disc.2  ...  Throughput.2_y  Throughput.3_y  \\\n",
       "0       False        False         True  ...             650             561   \n",
       "1        True         True         True  ...             215            1950   \n",
       "2       False        False         True  ...              41            1235   \n",
       "3       False         True         True  ...            1557            2526   \n",
       "4        True         True         True  ...            2505              48   \n",
       "\n",
       "   Throughput.4_y  WorkTime.1_y  WorkTime.2_y  WorkTime.3_y  WorkTime.4_y  \\\n",
       "0              92            10            11             7            19   \n",
       "1            3272            10            43            11            41   \n",
       "2            3272             9            47            10             4   \n",
       "3             872             6            10             7            13   \n",
       "4             478             8             9            66            23   \n",
       "\n",
       "   popularity_y  num_words  num_chars  \n",
       "0             0         12         57  \n",
       "1             0          8         43  \n",
       "2             0          5         27  \n",
       "3             0         14         77  \n",
       "4             0          7         46  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# pipeline-onehot Function blocks #\n",
    "###################################\n",
    "\n",
    "def sss_train_test_split(dataframe, class_name, n_splits, test_size, random_state):\n",
    "    y = dataframe[class_name].copy()\n",
    "    X = dataframe.drop([class_name], axis=1)\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    splits_generator = sss.split(X, y)\n",
    "\n",
    "    for train_idx, test_idx in splits_generator:\n",
    "        indices_train = train_idx\n",
    "        indices_test = test_idx\n",
    "\n",
    "    train = df.take(indices_train)\n",
    "    test = df.take(indices_test)\n",
    "    \n",
    "    return indices_train, indices_test, train, test\n",
    "\n",
    "def generate_class_weights(train_data, class_name, annotation_name):\n",
    "    y_train = train_data[annotation_name].to_numpy()\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    return y_train, class_weight_dict\n",
    "\n",
    "def label_preprocessing(y_data, label_encoder):\n",
    "    out = label_encoder.fit_transform(y_data).reshape(-1,1)\n",
    "    return out\n",
    "\n",
    "def individual_model(annot_name, x_train_data, y_train_data, x_val_data, y_val_data, class_weight_dict, indiv_batch_size, indiv_epochs):\n",
    "    model = models_nn.create_nn_model()\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = RMSprop(),\n",
    "                  metrics = ['accuracy', models_nn.f1_m, models_nn.recall_m, models_nn.precision_m])\n",
    "    history = model.fit(x_train_data,y_train_data,\n",
    "                        batch_size=indiv_batch_size,\n",
    "                        epochs=indiv_epochs,\n",
    "                        validation_data=(x_val_data, y_val_data), \n",
    "#                         callbacks=[models_nn.early_stop],\n",
    "                        class_weight=class_weight_dict,\n",
    "                        verbose=0)\n",
    "    \n",
    "    pred = model.predict(x_train_data)\n",
    "    pred_test = model.predict(x_val_data)\n",
    "\n",
    "    pred_test_round = pred_test.round()\n",
    "    \n",
    "    validation_metrics_dict = history.history\n",
    "    val_f1_list = history.history['val_f1_m']\n",
    "    best_val_f1 = max(val_f1_list)\n",
    "    best_val_prec = history.history['val_precision_m'][val_f1_list.index(best_val_f1)]\n",
    "    best_val_recall = history.history['val_recall_m'][val_f1_list.index(best_val_f1)]\n",
    "    macro_scores = precision_recall_fscore_support(y_val_data, pred_test_round, average='macro')\n",
    "    print(\"#############################################################\")\n",
    "    print(\"Metrics for {} individual model:\".format(annot_name))\n",
    "    print(\"Best validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(best_val_f1,\n",
    "                                                                                 best_val_prec,\n",
    "                                                                                 best_val_recall))\n",
    "    print(\"Macro validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(macro_scores[2],\n",
    "                                                                                  macro_scores[0],\n",
    "                                                                                  macro_scores[1]))\n",
    "    return pred, pred_test\n",
    "\n",
    "# (HARD-CODED)\n",
    "def generate_encodings(info_support_pred, emo_support_pred, info_disclosure_pred, emo_disclosure_pred):\n",
    "    pred_df_arr_full = []\n",
    "\n",
    "    for i in range(0, len(info_support_pred)):\n",
    "        pred_obj_1 = {}\n",
    "        pred_obj_1['info_support'] = info_support_pred[i][0]\n",
    "        pred_obj_1['emo_support'] = emo_support_pred[i][0]\n",
    "        pred_obj_1['info_disclosure'] = info_disclosure_pred[i][0]\n",
    "        pred_obj_1['emo_disclosure'] = emo_disclosure_pred[i][0]\n",
    "        pred_df_arr_full.append(pred_obj_1)\n",
    "\n",
    "    pred_df_full = pd.DataFrame(pred_df_arr_full)\n",
    "    return pred_df_full\n",
    "\n",
    "def joint_model(weights_name, pred_df_full, y_train_1, pred_df_full_test, y_test_1,\n",
    "                class_weight_dict_1, joint_batch_size, joint_epochs):\n",
    "    \n",
    "    def helper(predict_name, pred_df, y_train, pred_df_test, y_test, class_weight_dict_1, joint_batch_size, joint_epochs):\n",
    "        joint_full_model_1 = models_nn.create_joint_model(pred_df_full)\n",
    "        history_1 = joint_full_model_1.fit(x=pred_df_full, \n",
    "                                           y=y_train_1, \n",
    "                                           epochs=joint_epochs, \n",
    "                                           batch_size=joint_batch_size, \n",
    "                                           validation_data=(pred_df_full_test,y_test_1), \n",
    "#                                            callbacks=[models_nn.callback], \n",
    "                                           class_weight=class_weight_dict_1,\n",
    "                                           verbose=0)\n",
    "        joint_predict_1 = joint_full_model_1.predict(pred_df_full_test)\n",
    "        joint_predict_round_1 = []\n",
    "        for a in joint_predict_1:\n",
    "            joint_predict_round_1.append(np.argmax(a))\n",
    "        out1 = precision_recall_fscore_support(y_test_1, np.array(joint_predict_round_1), average='macro')\n",
    "\n",
    "        val_f1_list = history_1.history['val_f1_m']\n",
    "        best_val_f1 = max(val_f1_list)\n",
    "        best_val_prec = history_1.history['val_precision_m'][val_f1_list.index(best_val_f1)]\n",
    "        best_val_recall = history_1.history['val_recall_m'][val_f1_list.index(best_val_f1)]\n",
    "        macro_scores = out1\n",
    "        if weights_name == None:\n",
    "            print(\"Metrics for {} joint model w/o weights:\".format(predict_name))\n",
    "        else:\n",
    "            print(\"Metrics for {} joint model weighted by {}\".format(predict_name, weights_name))\n",
    "        print(\"Best validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(best_val_f1,\n",
    "                                                                                     best_val_prec,\n",
    "                                                                                     best_val_recall))\n",
    "        print(\"Macro validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(macro_scores[2],\n",
    "                                                                                      macro_scores[0],\n",
    "                                                                                      macro_scores[1]))\n",
    "        return [best_val_f1, best_val_prec, best_val_recall], macro_scores\n",
    "    \n",
    "    print(\"#############################################################\")\n",
    "    decep_1, decep_2 = helper(\"Popularity\", pred_df_full, y_train_1, pred_df_full_test, y_test_1, class_weight_dict_1, joint_batch_size, joint_epochs)\n",
    "    \n",
    "    return decep_1, decep_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Main function for dataset sampling experiments #\n",
    "##################################################\n",
    "\n",
    "# Currently only supports annotations with 2 classes, i.e. binary\n",
    "\n",
    "def dataset_sampling(dataframe, class_name, sampling_size_list, metadata_options_list, model_name):\n",
    "    \n",
    "    # Misc variables\n",
    "    results = {}\n",
    "    \n",
    "    # Model settings (for individual annotation models)\n",
    "    models_nn.MODEL_NAME = model_name\n",
    "    \n",
    "    # Full dataframe proportions\n",
    "    full_size = dataframe.shape[0]\n",
    "    full_counts = dataframe[class_name].value_counts()\n",
    "    print(\"Full dataset proportions w.r.t. {}\".format(class_name))\n",
    "    print(full_counts)\n",
    "    full_counts_dict = full_counts.to_dict()\n",
    "    full_counts_list = list(full_counts_dict.values())\n",
    "    \n",
    "    ## class_proportions is a list of class proportions, first item corresponding to first class, etc\n",
    "    class_proportions = []\n",
    "    for each_class_counts in full_counts_list:\n",
    "        class_proportions.append(each_class_counts / full_size)\n",
    "\n",
    "    # Looping through sample_size_list\n",
    "    for each_sample_size in sampling_size_list:\n",
    "        \n",
    "        print(\"#################################\")\n",
    "        print(\"Sample size: {}\".format(each_sample_size))\n",
    "        print(\"#################################\")\n",
    "        \n",
    "        ## Counting number of datapoints per class proportionate to main dataset\n",
    "        class_sizes = [round(each_sample_size * class_proportions[0])]\n",
    "        class_sizes.append(each_sample_size - class_sizes[0])\n",
    "\n",
    "        ## Creating sub dataframe\n",
    "        s0 = dataframe.loc[dataframe[class_name] == list(full_counts_dict.keys())[0]].sample(class_sizes[0]).index\n",
    "        s1 = dataframe.loc[dataframe[class_name] == list(full_counts_dict.keys())[1]].sample(class_sizes[1]).index\n",
    "        sub_df = dataframe.loc[s0.union(s1)]\n",
    "\n",
    "        # Metadata settings\n",
    "        print(\"#############################################################\")\n",
    "        print(\"Metadata options for current sample\")\n",
    "        df_throughput, df_worktime, df_textlength, df_special = popularity_metadata_options.set_OHE_pipeline_options(sub_df, *metadata_options_list)\n",
    "  \n",
    "        ## Train_test_split using SSS\n",
    "        indices_train, indices_test, train, test = sss_train_test_split(sub_df, class_name, n_splits, test_size, random_state)\n",
    "        \n",
    "        ## Generate class weights dict and y_train data (HARD-CODED)\n",
    "        y_train_popularity, popularity_class_weight_dict = generate_class_weights(train, class_name, \"popularity_x\")\n",
    "        y_train_emo_disclosure, emo_disclosure_class_weight_dict = generate_class_weights(train, class_name, 'Emotional_disclosure')\n",
    "        y_train_info_disclosure, info_disclosure_class_weight_dict = generate_class_weights(train, class_name, 'Information_disclosure')\n",
    "        y_train_emo_support, emo_support_class_weight_dict = generate_class_weights(train, class_name, 'Emo_support')\n",
    "        y_train_info_support, info_support_class_weight_dict = generate_class_weights(train, class_name, 'Info_support')\n",
    "        print(\"#############################################################\")\n",
    "        print(\"Class weights generated\")\n",
    "        print(\"Popularity: {} \\nEmo Disclosure: {} \\nInfo Disclosure: {} \\nEmo Support: {} \\nInfo Support: {}\".format(popularity_class_weight_dict,\n",
    "                                                                                                          emo_disclosure_class_weight_dict,\n",
    "                                                                                                          info_disclosure_class_weight_dict,\n",
    "                                                                                                          emo_support_class_weight_dict,\n",
    "                                                                                                          info_support_class_weight_dict))\n",
    "        \n",
    "        ## Train and test data preparation (HARD-CODED)\n",
    "        X_train_col = train['full_text_x']\n",
    "        \n",
    "        y_test_popularity = test['popularity_x'].tolist()\n",
    "        y_test_emo_disclosure = test['Emotional_disclosure'].tolist()\n",
    "        y_test_info_disclosure = test['Information_disclosure'].tolist()\n",
    "        y_test_emo_support = test['Emo_support'].tolist()\n",
    "        y_test_info_support = test['Info_support'].tolist()\n",
    "\n",
    "        X_test_col = test['full_text_x']\n",
    "        \n",
    "        # Label encodings\n",
    "        le = LabelEncoder()\n",
    "\n",
    "        y_train_popularity = train['popularity_x'].tolist()\n",
    "        y_train_popularity = le.fit_transform(y_train_popularity)\n",
    "        y_train_popularity = y_train_popularity.reshape(-1,1)\n",
    "\n",
    "        y_train_emo_disclosure = train['Emotional_disclosure'].tolist()\n",
    "        y_train_emo_disclosure = le.fit_transform(y_train_emo_disclosure)\n",
    "        y_train_emo_disclosure = y_train_emo_disclosure.reshape(-1,1)\n",
    "\n",
    "        y_train_info_disclosure = train['Information_disclosure'].tolist()\n",
    "        y_train_info_disclosure = le.fit_transform(y_train_info_disclosure)\n",
    "        y_train_info_disclosure = y_train_info_disclosure.reshape(-1,1)\n",
    "\n",
    "        y_train_emo_support = train['Emo_support'].tolist()\n",
    "        y_train_emo_support = le.fit_transform(y_train_emo_support)\n",
    "        y_train_emo_support = y_train_emo_support.reshape(-1,1)\n",
    "\n",
    "        y_train_info_support = train['Info_support'].tolist()\n",
    "        y_train_info_support = le.fit_transform(y_train_info_support)\n",
    "        y_train_info_support = y_train_info_support.reshape(-1,1)\n",
    "        \n",
    "        ## Tokenizer settings\n",
    "        max_words = 1000\n",
    "        max_len = 220\n",
    "\n",
    "        tok = Tokenizer(num_words=max_words)\n",
    "\n",
    "        tok.fit_on_texts(X_train_col)\n",
    "        X_train_sequences = tok.texts_to_sequences(X_train_col)\n",
    "        X_train = pad_sequences(X_train_sequences, maxlen=max_len)\n",
    "\n",
    "        X_test_sequences = tok.texts_to_sequences(X_test_col)\n",
    "        X_test = pad_sequences(X_test_sequences, maxlen=max_len)\n",
    "        \n",
    "        ## Individual Models (HARD-CODED)\n",
    "        ### Deception pred and pred_test not needed\n",
    "        y_train_popularity = np.asarray(y_train_popularity)\n",
    "        y_test_popularity = np.asarray(y_test_popularity)\n",
    "        y_train_emo_disclosure = np.asarray(y_train_emo_disclosure)\n",
    "        y_test_emo_disclosure = np.asarray(y_test_emo_disclosure)\n",
    "        y_train_info_disclosure = np.asarray(y_train_info_disclosure)\n",
    "        y_test_info_disclosure = np.asarray(y_test_info_disclosure)\n",
    "        y_train_emo_support = np.asarray(y_train_emo_support)\n",
    "        y_test_emo_support = np.asarray(y_test_emo_support)\n",
    "        y_train_info_support = np.asarray(y_train_info_support)\n",
    "        y_test_info_support = np.asarray(y_test_info_support)\n",
    "        _, _ = individual_model('Popularity', X_train, y_train_popularity, X_test, y_test_popularity, popularity_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        emo_disclosure_pred, emo_disclosure_pred_test = individual_model('Emotional Disclosure', X_train, y_train_emo_disclosure, X_test, y_test_emo_disclosure, emo_disclosure_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        info_disclosure_pred, info_disclosure_pred_test = individual_model('Information Disclosure', X_train, y_train_info_disclosure, X_test, y_test_info_disclosure, info_disclosure_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        emo_support_pred, emo_support_pred_test = individual_model('Emotional Support', X_train, y_train_emo_support, X_test, y_test_emo_support, emo_support_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        info_support_pred, info_support_pred_test = individual_model('Information Support', X_train, y_train_info_support, X_test, y_test_info_support, info_support_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        \n",
    "        ## Generate one-hot encodings (HARD-CODED)\n",
    "        pred_df = generate_encodings(info_support_pred, emo_support_pred, info_disclosure_pred, emo_disclosure_pred)\n",
    "        pred_test_df = generate_encodings(info_support_pred_test, emo_support_pred_test, info_disclosure_pred_test, emo_disclosure_pred_test)\n",
    "        \n",
    "        # Generate weighted one-hot encodings (HARD-CODED)\n",
    "        pred_df_throughput, pred_df_worktime = popularity_metadata_options.construct_weighted_dataframe(indices_train, df_throughput, df_worktime, pred_df)\n",
    "        pred_df_throughput_test, pred_df_worktime_test = popularity_metadata_options.construct_weighted_dataframe(indices_test, df_throughput, df_worktime, pred_test_df)\n",
    "        print(\"#############################################################\")\n",
    "        print(\"Weighted one-hot encodings generated\")\n",
    "        \n",
    "        ## Joint model w/o weights\n",
    "        out1_wo_weights, _ = joint_model(None, pred_df, y_train_popularity, pred_test_df, y_test_popularity, \n",
    "                                                             popularity_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        # Joint model weighted by Throughput\n",
    "        out1_tp, _ = joint_model('Throughput', pred_df_throughput, y_train_popularity, pred_df_throughput_test, y_test_popularity,\n",
    "                                                       popularity_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        # Joint model weighted by Worktime\n",
    "        out1_wt, _ = joint_model('Worktime', pred_df_worktime, y_train_popularity, pred_df_worktime_test, y_test_popularity,\n",
    "                                                       popularity_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by PC Agreement\n",
    "#         out1_pc, _ = joint_model('PC Agreement', pred_df_full, y_train_popularity, pred_test_df_full, y_test_popularity,\n",
    "#                                                        popularity_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by Text Length\n",
    "        out1_tl, _ = joint_model('Text Length', pred_df_full, y_train_popularity, pred_test_df_full, y_test_popularity,\n",
    "                                                       popularity_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        \n",
    "        ## Joint model weighted by Special options\n",
    "        out1_sp, _ = joint_model('Special', pred_df_full, y_train_popularity, pred_test_df_full, y_test_popularity,\n",
    "                                                       popularity_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        \n",
    "#         results['run_' + str(each_sample_size)] = [out1_wo_weights, out2_wo_weights, out1_tp, out1_wt, \n",
    "#                                                    out1_pc, out1_tl, out1_sp]\n",
    "        results['run_' + str(each_sample_size)] = [out1_wo_weights, out1_tp, out1_tl, out1_sp]\n",
    "    print(\"Done\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Arguments for current experiment #\n",
    "####################################\n",
    "\n",
    "# Metadata options\n",
    "throughput_option = 'TP1'\n",
    "worktime_option = 'WT1'\n",
    "pc_agreement_option = 'PC1'\n",
    "textlength_option = 'TL1'\n",
    "special_option = 'SP1'\n",
    "k_option_for_tp = 1\n",
    "metadata_options_choices = [throughput_option, worktime_option, pc_agreement_option, textlength_option, special_option, k_option_for_tp]\n",
    "\n",
    "# Train_test_split SSS options\n",
    "n_splits = 1\n",
    "test_size = 0.2\n",
    "random_state = 0\n",
    "\n",
    "# Individual model options\n",
    "model_name = 'lstm'\n",
    "models_nn.MODEL_NAME = model_name\n",
    "indiv_batch_size = 64\n",
    "indiv_epochs = 15\n",
    "\n",
    "# Joint model options\n",
    "joint_batch_size = 64\n",
    "joint_epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset proportions w.r.t. popularity_x\n",
      "0    14254\n",
      "1     3219\n",
      "Name: popularity_x, dtype: int64\n",
      "#################################\n",
      "Sample size: 17473\n",
      "#################################\n",
      "#############################################################\n",
      "Metadata options for current sample\n",
      "TP1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "WT1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "TL1: weighted by 1 normalised number of characters per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "SP1: weighted by average of TP1 and TP2 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "#############################################################\n",
      "Class weights generated\n",
      "Popularity: {0: 0.6129088836271157, 1: 2.7141747572815533} \n",
      "Emo Disclosure: {0: 0.7872268528947961, 1: 1.370392156862745} \n",
      "Info Disclosure: {0: 0.9004122648801856, 1: 1.1243564993564994} \n",
      "Emo Support: {0: 0.562902706185567, 1: 4.474391805377721} \n",
      "Info Support: {0: 0.5711367165154858, 1: 4.01435956346927}\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": " [_Derived_]RecvAsync is cancelled.\n\t [[{{node gradient_tape/functional_3/embedding_1/embedding_lookup/Reshape/_38}}]] [Op:__inference_train_function_4351]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7fd696d597ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                 \u001b[0msampling_size_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampling_sizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                 \u001b[0mmetadata_options_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata_options_choices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                                 model_name=model_name)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-336c7d0be077>\u001b[0m in \u001b[0;36mdataset_sampling\u001b[1;34m(dataframe, class_name, sampling_size_list, metadata_options_list, model_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0my_train_info_support\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_info_support\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0my_test_info_support\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_info_support\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindividual_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Popularity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_popularity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_popularity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpopularity_class_weight_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0memo_disclosure_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memo_disclosure_pred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindividual_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Emotional Disclosure'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_emo_disclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_emo_disclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memo_disclosure_class_weight_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0minfo_disclosure_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_disclosure_pred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindividual_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Information Disclosure'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_info_disclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_info_disclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_disclosure_class_weight_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8165acc33d88>\u001b[0m in \u001b[0;36mindividual_model\u001b[1;34m(annot_name, x_train_data, y_train_data, x_val_data, y_val_data, class_weight_dict, indiv_batch_size, indiv_epochs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#                         callbacks=[models_nn.early_stop],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                         verbose=0)\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCancelledError\u001b[0m:  [_Derived_]RecvAsync is cancelled.\n\t [[{{node gradient_tape/functional_3/embedding_1/embedding_lookup/Reshape/_38}}]] [Op:__inference_train_function_4351]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "sampling_sizes = [17473]\n",
    "\n",
    "results_dict = dataset_sampling(dataframe=df, \n",
    "                                class_name=\"popularity_x\", \n",
    "                                sampling_size_list=sampling_sizes, \n",
    "                                metadata_options_list=metadata_options_choices, \n",
    "                                model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate_into_pandas(results_dictionary, metric_of_focus):\n",
    "#     def helper(metric_of_focus_number):\n",
    "#         new_dict = {}\n",
    "#         for each_key, each_values_list in results_dictionary.items():\n",
    "#             new_dict[each_key[4:]] = []\n",
    "#             for each in each_values_list:\n",
    "#                 new_dict[each_key[4:]].append(each[metric_of_focus_number])\n",
    "#         out_df = pd.DataFrame.from_dict(new_dict)\n",
    "#         return out_df\n",
    "    \n",
    "#     if metric_of_focus == 'F1':\n",
    "#         metric_of_focus_number = 0\n",
    "#     elif metric_of_focus == 'Precision':\n",
    "#         metric_of_focus_number = 1\n",
    "#     elif metric_of_focus == 'Recall':\n",
    "#         metric_of_focus_number = 2\n",
    "        \n",
    "#     return helper(metric_of_focus_number)\n",
    "\n",
    "# experiment_df = translate_into_pandas(results_dict, 'Precision')\n",
    "# results_name = \"./output/dataset_sampling_\" + str(len(sampling_sizes)) + \"pts\"\n",
    "# experiment_df.to_csv(results_name + \".csv\", index=False)\n",
    "# experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_plot_df = experiment_df.T.reset_index()\n",
    "# rename_col_names = {0: 'Deception w/o weights',\n",
    "#                     1: 'Rapport w/o weights',\n",
    "#                     2: 'Deception by TP',\n",
    "#                     3: 'Rapport by TP',\n",
    "#                     4: 'Deception by WT',\n",
    "#                     5: 'Rapport by WT',\n",
    "#                     6: 'Deception by PC',\n",
    "#                     7: 'Rapport by PC',\n",
    "#                     8: 'Deception by TL',\n",
    "#                     9: 'Rapport by TL',\n",
    "#                     10: 'Deception by SP',\n",
    "#                     11: 'Rapport by SP'}\n",
    "# exp_plot_df = exp_plot_df.rename(columns=rename_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot = exp_plot_df.plot('index',list(exp_plot_df.columns)[1:],style='.-', figsize=(12,9))\n",
    "# plot.set_xlabel('Sample size', size=10)\n",
    "# plot.set_ylabel('F1 Scores', size=10)\n",
    "# lgd = plot.legend(loc='center left',bbox_to_anchor=(1.0, 0.5), borderaxespad=0.)\n",
    "# plot = plot.get_figure()\n",
    "# plot.savefig(results_name + '.jpg', bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
