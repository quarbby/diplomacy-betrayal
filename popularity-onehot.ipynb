{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from popularity_metadata_options.ipynb\n",
      "importing Jupyter notebook from models_nn.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Input, InputLayer, Dropout, Dense, Flatten, Embedding, Add, Concatenate\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "## Own code \n",
    "import import_ipynb\n",
    "import popularity_metadata_options\n",
    "import models_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with Throughput & WorkTime\n",
    "df = pd.read_csv('./data/popularity/mean_merge.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text_x</th>\n",
       "      <th>Emotional_disclosure</th>\n",
       "      <th>Information_disclosure</th>\n",
       "      <th>score_x</th>\n",
       "      <th>emo_disc.1</th>\n",
       "      <th>emo_disc.2</th>\n",
       "      <th>emo_disc.3</th>\n",
       "      <th>emo_disc.4</th>\n",
       "      <th>info_disc.1</th>\n",
       "      <th>info_disc.2</th>\n",
       "      <th>...</th>\n",
       "      <th>info_supp.4</th>\n",
       "      <th>Throughput.1_y</th>\n",
       "      <th>Throughput.2_y</th>\n",
       "      <th>Throughput.3_y</th>\n",
       "      <th>Throughput.4_y</th>\n",
       "      <th>WorkTime.1_y</th>\n",
       "      <th>WorkTime.2_y</th>\n",
       "      <th>WorkTime.3_y</th>\n",
       "      <th>WorkTime.4_y</th>\n",
       "      <th>popularity_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>''  Alot of people DONT think like that when t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1130</td>\n",
       "      <td>650</td>\n",
       "      <td>561</td>\n",
       "      <td>92</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Official' would be one way to describe it.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1072</td>\n",
       "      <td>215</td>\n",
       "      <td>1950</td>\n",
       "      <td>3272</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>11</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"...you mix me a cocktail.\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>151</td>\n",
       "      <td>41</td>\n",
       "      <td>1235</td>\n",
       "      <td>3272</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Be kind, for everyone you meet is fighting a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>2300</td>\n",
       "      <td>1557</td>\n",
       "      <td>2526</td>\n",
       "      <td>872</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Consider yourself lucky because I chose you\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>266</td>\n",
       "      <td>2505</td>\n",
       "      <td>48</td>\n",
       "      <td>478</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         full_text_x  Emotional_disclosure  \\\n",
       "0  ''  Alot of people DONT think like that when t...                     0   \n",
       "1        'Official' would be one way to describe it.                     1   \n",
       "2                        \"...you mix me a cocktail.\"                     0   \n",
       "3  \"Be kind, for everyone you meet is fighting a ...                     0   \n",
       "4     \"Consider yourself lucky because I chose you\"?                     0   \n",
       "\n",
       "   Information_disclosure  score_x  emo_disc.1  emo_disc.2  emo_disc.3  \\\n",
       "0                       0        2        True        True        True   \n",
       "1                       0        2       False       False       False   \n",
       "2                       0        2        True        True        True   \n",
       "3                       0        3        True        True        True   \n",
       "4                       0        1        True        True        True   \n",
       "\n",
       "   emo_disc.4  info_disc.1  info_disc.2  ...  info_supp.4  Throughput.1_y  \\\n",
       "0       False        False         True  ...        False            1130   \n",
       "1        True         True         True  ...        False            1072   \n",
       "2       False        False         True  ...        False             151   \n",
       "3       False         True         True  ...        False            2300   \n",
       "4        True         True         True  ...        False             266   \n",
       "\n",
       "   Throughput.2_y  Throughput.3_y  Throughput.4_y  WorkTime.1_y  WorkTime.2_y  \\\n",
       "0             650             561              92            10            11   \n",
       "1             215            1950            3272            10            43   \n",
       "2              41            1235            3272             9            47   \n",
       "3            1557            2526             872             6            10   \n",
       "4            2505              48             478             8             9   \n",
       "\n",
       "   WorkTime.3_y  WorkTime.4_y  popularity_y  \n",
       "0             7            19             0  \n",
       "1            11            41             0  \n",
       "2            10             4             0  \n",
       "3             7            13             0  \n",
       "4            66            23             0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# pipeline-onehot Function blocks #\n",
    "###################################\n",
    "\n",
    "def sss_train_test_split(dataframe, class_name, n_splits, test_size, random_state):\n",
    "    y = dataframe[class_name].copy()\n",
    "    X = dataframe.drop([class_name], axis=1)\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    splits_generator = sss.split(X, y)\n",
    "\n",
    "    for train_idx, test_idx in splits_generator:\n",
    "        indices_train = train_idx\n",
    "        indices_test = test_idx\n",
    "\n",
    "    train = df.take(indices_train)\n",
    "    test = df.take(indices_test)\n",
    "    \n",
    "    return indices_train, indices_test, train, test\n",
    "\n",
    "def generate_class_weights(train_data, class_name, annotation_name):\n",
    "    y_train = train_data[annotation_name].to_numpy()\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    return y_train, class_weight_dict\n",
    "\n",
    "def label_preprocessing(y_data, label_encoder):\n",
    "    out = label_encoder.fit_transform(y_data).reshape(-1,1)\n",
    "    return out\n",
    "\n",
    "def individual_model(annot_name, x_train_data, y_train_data, x_val_data, y_val_data, class_weight_dict, indiv_batch_size, indiv_epochs):\n",
    "    model = models_nn.create_nn_model()\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = RMSprop(),\n",
    "                  metrics = ['accuracy', models_nn.f1_m, models_nn.recall_m, models_nn.precision_m])\n",
    "    history = model.fit(x_train_data,y_train_data,\n",
    "                        batch_size=indiv_batch_size,\n",
    "                        epochs=indiv_epochs,\n",
    "                        validation_data=(x_val_data, y_val_data), \n",
    "#                         callbacks=[models_nn.early_stop],\n",
    "                        class_weight=class_weight_dict,\n",
    "                        verbose=0)\n",
    "    \n",
    "    pred = model.predict(x_train_data)\n",
    "    pred_test = model.predict(x_val_data)\n",
    "\n",
    "    pred_test_round = pred_test.round()\n",
    "    \n",
    "    validation_metrics_dict = history.history\n",
    "    val_f1_list = history.history['val_f1_m']\n",
    "    best_val_f1 = max(val_f1_list)\n",
    "    best_val_prec = history.history['val_precision_m'][val_f1_list.index(best_val_f1)]\n",
    "    best_val_recall = history.history['val_recall_m'][val_f1_list.index(best_val_f1)]\n",
    "    macro_scores = precision_recall_fscore_support(y_val_data, pred_test_round, average='macro')\n",
    "    print(\"#############################################################\")\n",
    "    print(\"Metrics for {} individual model:\".format(annot_name))\n",
    "    print(\"Best validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(best_val_f1,\n",
    "                                                                                 best_val_prec,\n",
    "                                                                                 best_val_recall))\n",
    "    print(\"Macro validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(macro_scores[2],\n",
    "                                                                                  macro_scores[0],\n",
    "                                                                                  macro_scores[1]))\n",
    "    return pred, pred_test\n",
    "\n",
    "# (HARD-CODED)\n",
    "def generate_encodings(info_support_pred, emo_support_pred, info_disclosure_pred, emo_disclosure_pred):\n",
    "    pred_df_arr_full = []\n",
    "\n",
    "    for i in range(0, len(info_support_pred)):\n",
    "        pred_obj_1 = {}\n",
    "        pred_obj_1['info_support'] = info_support_pred[i][0]\n",
    "        pred_obj_1['emo_support'] = emo_support_pred[i][0]\n",
    "        pred_obj_1['info_disclosure'] = info_disclosure_pred[i][0]\n",
    "        pred_obj_1['emo_disclosure'] = emo_disclosure_pred[i][0]\n",
    "        pred_df_arr_full.append(pred_obj_1)\n",
    "\n",
    "    pred_df_full = pd.DataFrame(pred_df_arr_full)\n",
    "    return pred_df_full\n",
    "\n",
    "def joint_model(weights_name, pred_df_full, y_train_1, pred_df_full_test, y_test_1,\n",
    "                class_weight_dict_1, joint_batch_size, joint_epochs):\n",
    "    \n",
    "    def helper(predict_name, pred_df, y_train, pred_df_test, y_test, class_weight_dict_1, joint_batch_size, joint_epochs):\n",
    "        joint_full_model_1 = models_nn.create_joint_model(pred_df_full)\n",
    "        history_1 = joint_full_model_1.fit(x=pred_df_full, \n",
    "                                           y=y_train_1, \n",
    "                                           epochs=joint_epochs, \n",
    "                                           batch_size=joint_batch_size, \n",
    "                                           validation_data=(pred_df_full_test,y_test_1), \n",
    "#                                            callbacks=[models_nn.callback], \n",
    "                                           class_weight=class_weight_dict_1,\n",
    "                                           verbose=0)\n",
    "        joint_predict_1 = joint_full_model_1.predict(pred_df_full_test)\n",
    "        joint_predict_round_1 = []\n",
    "        for a in joint_predict_1:\n",
    "            joint_predict_round_1.append(np.argmax(a))\n",
    "        out1 = precision_recall_fscore_support(y_test_1, np.array(joint_predict_round_1), average='macro')\n",
    "\n",
    "        val_f1_list = history_1.history['val_f1_m']\n",
    "        best_val_f1 = max(val_f1_list)\n",
    "        best_val_prec = history_1.history['val_precision_m'][val_f1_list.index(best_val_f1)]\n",
    "        best_val_recall = history_1.history['val_recall_m'][val_f1_list.index(best_val_f1)]\n",
    "        macro_scores = out1\n",
    "        if weights_name == None:\n",
    "            print(\"Metrics for {} joint model w/o weights:\".format(predict_name))\n",
    "        else:\n",
    "            print(\"Metrics for {} joint model weighted by {}\".format(predict_name, weights_name))\n",
    "        print(\"Best validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(best_val_f1,\n",
    "                                                                                     best_val_prec,\n",
    "                                                                                     best_val_recall))\n",
    "        print(\"Macro validation metrics: F1 = {}, Precision = {}, Recall = {}\".format(macro_scores[2],\n",
    "                                                                                      macro_scores[0],\n",
    "                                                                                      macro_scores[1]))\n",
    "        return [best_val_f1, best_val_prec, best_val_recall], macro_scores\n",
    "    \n",
    "    print(\"#############################################################\")\n",
    "    decep_1, decep_2 = helper(\"Popularity\", pred_df_full, y_train_1, pred_df_full_test, y_test_1, class_weight_dict_1, joint_batch_size, joint_epochs)\n",
    "    \n",
    "    return decep_1, decep_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Main function for dataset sampling experiments #\n",
    "##################################################\n",
    "\n",
    "# Currently only supports annotations with 2 classes, i.e. binary\n",
    "\n",
    "def dataset_sampling(dataframe, class_name, sampling_size_list, metadata_options_list, model_name):\n",
    "    \n",
    "    # Misc variables\n",
    "    results = {}\n",
    "    \n",
    "    # Model settings (for individual annotation models)\n",
    "    models_nn.MODEL_NAME = model_name\n",
    "    \n",
    "    # Full dataframe proportions\n",
    "    full_size = dataframe.shape[0]\n",
    "    full_counts = dataframe[class_name].value_counts()\n",
    "    print(\"Full dataset proportions w.r.t. {}\".format(class_name))\n",
    "    print(full_counts)\n",
    "    full_counts_dict = full_counts.to_dict()\n",
    "    full_counts_list = list(full_counts_dict.values())\n",
    "    \n",
    "    ## class_proportions is a list of class proportions, first item corresponding to first class, etc\n",
    "    class_proportions = []\n",
    "    for each_class_counts in full_counts_list:\n",
    "        class_proportions.append(each_class_counts / full_size)\n",
    "\n",
    "    # Looping through sample_size_list\n",
    "    for each_sample_size in sampling_size_list:\n",
    "        \n",
    "        print(\"#################################\")\n",
    "        print(\"Sample size: {}\".format(each_sample_size))\n",
    "        print(\"#################################\")\n",
    "        \n",
    "        ## Counting number of datapoints per class proportionate to main dataset\n",
    "        class_sizes = [round(each_sample_size * class_proportions[0])]\n",
    "        class_sizes.append(each_sample_size - class_sizes[0])\n",
    "\n",
    "        ## Creating sub dataframe\n",
    "        s0 = dataframe.loc[dataframe[class_name] == list(full_counts_dict.keys())[0]].sample(class_sizes[0]).index\n",
    "        s1 = dataframe.loc[dataframe[class_name] == list(full_counts_dict.keys())[1]].sample(class_sizes[1]).index\n",
    "        sub_df = dataframe.loc[s0.union(s1)]\n",
    "\n",
    "        # Metadata settings\n",
    "        print(\"#############################################################\")\n",
    "        print(\"Metadata options for current sample\")\n",
    "        df_throughput, df_worktime = popularity_metadata_options.set_OHE_pipeline_options(sub_df, *metadata_options_list)\n",
    "  \n",
    "        ## Train_test_split using SSS\n",
    "        indices_train, indices_test, train, test = sss_train_test_split(sub_df, class_name, n_splits, test_size, random_state)\n",
    "        \n",
    "        ## Generate class weights dict and y_train data (HARD-CODED)\n",
    "        y_train_popularity, popularity_class_weight_dict = generate_class_weights(train, class_name, \"popularity_x\")\n",
    "        y_train_emo_disclosure, emo_disclosure_class_weight_dict = generate_class_weights(train, class_name, 'Emotional_disclosure')\n",
    "        y_train_info_disclosure, info_disclosure_class_weight_dict = generate_class_weights(train, class_name, 'Information_disclosure')\n",
    "        y_train_emo_support, emo_support_class_weight_dict = generate_class_weights(train, class_name, 'Emo_support')\n",
    "        y_train_info_support, info_support_class_weight_dict = generate_class_weights(train, class_name, 'Info_support')\n",
    "        print(\"#############################################################\")\n",
    "        print(\"Class weights generated\")\n",
    "        print(\"Popularity: {} \\nEmo Disclosure: {} \\nInfo Disclosure: {} \\nEmo Support: {} \\nInfo Support: {}\".format(popularity_class_weight_dict,\n",
    "                                                                                                          emo_disclosure_class_weight_dict,\n",
    "                                                                                                          info_disclosure_class_weight_dict,\n",
    "                                                                                                          emo_support_class_weight_dict,\n",
    "                                                                                                          info_support_class_weight_dict))\n",
    "        \n",
    "        ## Train and test data preparation (HARD-CODED)\n",
    "        X_train_col = train['full_text_x']\n",
    "        \n",
    "        y_test_popularity = test['popularity_x'].tolist()\n",
    "        y_test_emo_disclosure = test['Emotional_disclosure'].tolist()\n",
    "        y_test_info_disclosure = test['Information_disclosure'].tolist()\n",
    "        y_test_emo_support = test['Emo_support'].tolist()\n",
    "        y_test_info_support = test['Info_support'].tolist()\n",
    "\n",
    "        X_test_col = test['full_text_x']\n",
    "        \n",
    "        # Label encodings\n",
    "        le = LabelEncoder()\n",
    "\n",
    "        y_train_popularity = train['popularity_x'].tolist()\n",
    "        y_train_popularity = le.fit_transform(y_train_popularity)\n",
    "        y_train_popularity = y_train_popularity.reshape(-1,1)\n",
    "\n",
    "        y_train_emo_disclosure = train['Emotional_disclosure'].tolist()\n",
    "        y_train_emo_disclosure = le.fit_transform(y_train_emo_disclosure)\n",
    "        y_train_emo_disclosure = y_train_emo_disclosure.reshape(-1,1)\n",
    "\n",
    "        y_train_info_disclosure = train['Information_disclosure'].tolist()\n",
    "        y_train_info_disclosure = le.fit_transform(y_train_info_disclosure)\n",
    "        y_train_info_disclosure = y_train_info_disclosure.reshape(-1,1)\n",
    "\n",
    "        y_train_emo_support = train['Emo_support'].tolist()\n",
    "        y_train_emo_support = le.fit_transform(y_train_emo_support)\n",
    "        y_train_emo_support = y_train_emo_support.reshape(-1,1)\n",
    "\n",
    "        y_train_info_support = train['Info_support'].tolist()\n",
    "        y_train_info_support = le.fit_transform(y_train_info_support)\n",
    "        y_train_info_support = y_train_info_support.reshape(-1,1)\n",
    "        \n",
    "        ## Tokenizer settings\n",
    "        max_words = 1000\n",
    "        max_len = 220\n",
    "\n",
    "        tok = Tokenizer(num_words=max_words)\n",
    "\n",
    "        tok.fit_on_texts(X_train_col)\n",
    "        X_train_sequences = tok.texts_to_sequences(X_train_col)\n",
    "        X_train = pad_sequences(X_train_sequences, maxlen=max_len)\n",
    "\n",
    "        X_test_sequences = tok.texts_to_sequences(X_test_col)\n",
    "        X_test = pad_sequences(X_test_sequences, maxlen=max_len)\n",
    "        \n",
    "        ## Individual Models (HARD-CODED)\n",
    "        ### Deception pred and pred_test not needed\n",
    "        y_train_popularity = np.asarray(y_train_popularity)\n",
    "        y_test_popularity = np.asarray(y_test_popularity)\n",
    "        y_train_emo_disclosure = np.asarray(y_train_emo_disclosure)\n",
    "        y_test_emo_disclosure = np.asarray(y_test_emo_disclosure)\n",
    "        y_train_info_disclosure = np.asarray(y_train_info_disclosure)\n",
    "        y_test_info_disclosure = np.asarray(y_test_info_disclosure)\n",
    "        y_train_emo_support = np.asarray(y_train_emo_support)\n",
    "        y_test_emo_support = np.asarray(y_test_emo_support)\n",
    "        y_train_info_support = np.asarray(y_train_info_support)\n",
    "        y_test_info_support = np.asarray(y_test_info_support)\n",
    "        _, _ = individual_model('Popularity', X_train, y_train_popularity, X_test, y_test_popularity, popularity_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        emo_disclosure_pred, emo_disclosure_pred_test = individual_model('Emotional Disclosure', X_train, y_train_emo_disclosure, X_test, y_test_emo_disclosure, emo_disclosure_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        info_disclosure_pred, info_disclosure_pred_test = individual_model('Information Disclosure', X_train, y_train_info_disclosure, X_test, y_test_info_disclosure, info_disclosure_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        emo_support_pred, emo_support_pred_test = individual_model('Emotional Support', X_train, y_train_emo_support, X_test, y_test_emo_support, emo_support_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        info_support_pred, info_support_pred_test = individual_model('Information Support', X_train, y_train_info_support, X_test, y_test_info_support, info_support_class_weight_dict, indiv_batch_size, indiv_epochs)\n",
    "        \n",
    "        ## Generate one-hot encodings (HARD-CODED)\n",
    "        pred_df = generate_encodings(info_support_pred, emo_support_pred, info_disclosure_pred, emo_disclosure_pred)\n",
    "        pred_test_df = generate_encodings(info_support_pred_test, emo_support_pred_test, info_disclosure_pred_test, emo_disclosure_pred_test)\n",
    "        \n",
    "        # Generate weighted one-hot encodings (HARD-CODED)\n",
    "        pred_df_throughput, pred_df_worktime = popularity_metadata_options.construct_weighted_dataframe(indices_train, df_throughput, df_worktime, pred_df)\n",
    "        pred_df_throughput_test, pred_df_worktime_test = popularity_metadata_options.construct_weighted_dataframe(indices_test, df_throughput, df_worktime, pred_test_df)\n",
    "        print(\"#############################################################\")\n",
    "        print(\"Weighted one-hot encodings generated\")\n",
    "        \n",
    "        ## Joint model w/o weights\n",
    "        out1_wo_weights, _ = joint_model(None, pred_df, y_train_popularity, pred_test_df, y_test_popularity, \n",
    "                                                             popularity_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        # Joint model weighted by Throughput\n",
    "        out1_tp, _ = joint_model('Throughput', pred_df_throughput, y_train_popularity, pred_df_throughput_test, y_test_popularity,\n",
    "                                                       popularity_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        # Joint model weighted by Worktime\n",
    "        out1_wt, _ = joint_model('Worktime', pred_df_worktime, y_train_popularity, pred_df_worktime_test, y_test_popularity,\n",
    "                                                       popularity_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by PC Agreement\n",
    "#         out1_pc, _ = joint_model('PC Agreement', pred_df_full, y_train_popularity, pred_test_df_full, y_test_popularity,\n",
    "#                                                        popularity_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        ## Joint model weighted by Text Length\n",
    "#         out1_tl, _ = joint_model('Text Length', pred_df_full, y_train_popularity, pred_test_df_full, y_test_popularity,\n",
    "#                                                        popularity_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        \n",
    "        ## Joint model weighted by Special options\n",
    "#         out1_sp, _ = joint_model('Special', pred_df_full, y_train_popularity, pred_test_df_full, y_test_popularity,\n",
    "#                                                        popularity_class_weight_dict, joint_batch_size, joint_epochs)\n",
    "        \n",
    "#         results['run_' + str(each_sample_size)] = [out1_wo_weights, out2_wo_weights, out1_tp, out1_wt, \n",
    "#                                                    out1_pc, out1_tl, out1_sp]\n",
    "        results['run_' + str(each_sample_size)] = [out1_wo_weights]\n",
    "    print(\"Done\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Arguments for current experiment #\n",
    "####################################\n",
    "\n",
    "# Metadata options\n",
    "throughput_option = 'TP2'\n",
    "worktime_option = 'WT2'\n",
    "pc_agreement_option = 'PC2'\n",
    "textlength_option = 'TL2'\n",
    "special_option = 'SP2'\n",
    "k_option_for_tp = 1\n",
    "metadata_options_choices = [throughput_option, worktime_option, pc_agreement_option, textlength_option, special_option, k_option_for_tp]\n",
    "\n",
    "# Train_test_split SSS options\n",
    "n_splits = 1\n",
    "test_size = 0.2\n",
    "random_state = 0\n",
    "\n",
    "# Individual model options\n",
    "model_name = 'lstm'\n",
    "models_nn.MODEL_NAME = model_name\n",
    "indiv_batch_size = 64\n",
    "indiv_epochs = 15\n",
    "\n",
    "# Joint model options\n",
    "joint_batch_size = 64\n",
    "joint_epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset proportions w.r.t. popularity_x\n",
      "0    14254\n",
      "1     3219\n",
      "Name: popularity_x, dtype: int64\n",
      "#################################\n",
      "Sample size: 17473\n",
      "#################################\n",
      "#############################################################\n",
      "Metadata options for current sample\n",
      "TP2: weighted by 1 linear variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "Plot below: old throughput (x-axis) vs new throughput (y-axis)\n",
      "WT2: weighted by 1 linear variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "#############################################################\n",
      "Class weights generated\n",
      "Popularity: {0: 0.6129088836271157, 1: 2.7141747572815533} \n",
      "Emo Disclosure: {0: 0.7872268528947961, 1: 1.370392156862745} \n",
      "Info Disclosure: {0: 0.9004122648801856, 1: 1.1243564993564994} \n",
      "Emo Support: {0: 0.562902706185567, 1: 4.474391805377721} \n",
      "Info Support: {0: 0.5711367165154858, 1: 4.01435956346927}\n",
      "#############################################################\n",
      "Metrics for Popularity individual model:\n",
      "Best validation metrics: F1 = 0.3138243854045868, Precision = 0.20289655029773712, Recall = 0.7362021803855896\n",
      "Macro validation metrics: F1 = 0.5180731484651375, Precision = 0.522619623717962, Recall = 0.530957046781014\n",
      "#############################################################\n",
      "Metrics for Emotional Disclosure individual model:\n",
      "Best validation metrics: F1 = 0.6105164289474487, Precision = 0.521834135055542, Recall = 0.7461088299751282\n",
      "Macro validation metrics: F1 = 0.6575056343163448, Precision = 0.6563466002698315, Recall = 0.6660613950211766\n",
      "#############################################################\n",
      "Metrics for Information Disclosure individual model:\n",
      "Best validation metrics: F1 = 0.6535513997077942, Precision = 0.6154199838638306, Recall = 0.7028385996818542\n",
      "Macro validation metrics: F1 = 0.6467280187088504, Precision = 0.6493865048770879, Recall = 0.6515234823517523\n",
      "#############################################################\n",
      "Metrics for Emotional Support individual model:\n",
      "Best validation metrics: F1 = 0.5838237404823303, Precision = 0.5246405601501465, Recall = 0.7051535248756409\n",
      "Macro validation metrics: F1 = 0.5289614270519539, Precision = 0.5970984226207825, Recall = 0.7384573254001573\n",
      "#############################################################\n",
      "Metrics for Information Support individual model:\n",
      "Best validation metrics: F1 = 0.4466477036476135, Precision = 0.38383421301841736, Recall = 0.5784762501716614\n",
      "Macro validation metrics: F1 = 0.6508021524847477, Precision = 0.6329650979437909, Recall = 0.7239019523148792\n",
      "#############################################################\n",
      "Weighted one-hot encodings generated\n",
      "#############################################################\n",
      "Metrics for Popularity joint model w/o weights:\n",
      "Best validation metrics: F1 = 0.28687700629234314, Precision = 0.18407443165779114, Recall = 0.6811532378196716\n",
      "Macro validation metrics: F1 = 0.44925937598487237, Precision = 0.4078683834048641, Recall = 0.5\n",
      "#############################################################\n",
      "Metrics for Popularity joint model weighted by Throughput\n",
      "Best validation metrics: F1 = 0.3077771067619324, Precision = 0.1904197633266449, Recall = 0.8463007211685181\n",
      "Macro validation metrics: F1 = 0.44925937598487237, Precision = 0.4078683834048641, Recall = 0.5\n",
      "#############################################################\n",
      "Metrics for Popularity joint model weighted by Worktime\n",
      "Best validation metrics: F1 = 0.3123909831047058, Precision = 0.18761084973812103, Recall = 0.9869272112846375\n",
      "Macro validation metrics: F1 = 0.44925937598487237, Precision = 0.4078683834048641, Recall = 0.5\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampling_sizes = [17473]\n",
    "\n",
    "results_dict = dataset_sampling(dataframe=df, \n",
    "                                class_name=\"popularity_x\", \n",
    "                                sampling_size_list=sampling_sizes, \n",
    "                                metadata_options_list=metadata_options_choices, \n",
    "                                model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate_into_pandas(results_dictionary, metric_of_focus):\n",
    "#     def helper(metric_of_focus_number):\n",
    "#         new_dict = {}\n",
    "#         for each_key, each_values_list in results_dictionary.items():\n",
    "#             new_dict[each_key[4:]] = []\n",
    "#             for each in each_values_list:\n",
    "#                 new_dict[each_key[4:]].append(each[metric_of_focus_number])\n",
    "#         out_df = pd.DataFrame.from_dict(new_dict)\n",
    "#         return out_df\n",
    "    \n",
    "#     if metric_of_focus == 'F1':\n",
    "#         metric_of_focus_number = 0\n",
    "#     elif metric_of_focus == 'Precision':\n",
    "#         metric_of_focus_number = 1\n",
    "#     elif metric_of_focus == 'Recall':\n",
    "#         metric_of_focus_number = 2\n",
    "        \n",
    "#     return helper(metric_of_focus_number)\n",
    "\n",
    "# experiment_df = translate_into_pandas(results_dict, 'Precision')\n",
    "# results_name = \"./output/dataset_sampling_\" + str(len(sampling_sizes)) + \"pts\"\n",
    "# experiment_df.to_csv(results_name + \".csv\", index=False)\n",
    "# experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_plot_df = experiment_df.T.reset_index()\n",
    "# rename_col_names = {0: 'Deception w/o weights',\n",
    "#                     1: 'Rapport w/o weights',\n",
    "#                     2: 'Deception by TP',\n",
    "#                     3: 'Rapport by TP',\n",
    "#                     4: 'Deception by WT',\n",
    "#                     5: 'Rapport by WT',\n",
    "#                     6: 'Deception by PC',\n",
    "#                     7: 'Rapport by PC',\n",
    "#                     8: 'Deception by TL',\n",
    "#                     9: 'Rapport by TL',\n",
    "#                     10: 'Deception by SP',\n",
    "#                     11: 'Rapport by SP'}\n",
    "# exp_plot_df = exp_plot_df.rename(columns=rename_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot = exp_plot_df.plot('index',list(exp_plot_df.columns)[1:],style='.-', figsize=(12,9))\n",
    "# plot.set_xlabel('Sample size', size=10)\n",
    "# plot.set_ylabel('F1 Scores', size=10)\n",
    "# lgd = plot.legend(loc='center left',bbox_to_anchor=(1.0, 0.5), borderaxespad=0.)\n",
    "# plot = plot.get_figure()\n",
    "# plot.savefig(results_name + '.jpg', bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
