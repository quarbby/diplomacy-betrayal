{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from metadata_options.ipynb\n",
      "importing Jupyter notebook from models_nn.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Input, InputLayer, Dropout, Dense, Flatten, Embedding, Add, Concatenate\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "## Own code \n",
    "import import_ipynb\n",
    "import metadata_options\n",
    "import models_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with Throughput & WorkTime\n",
    "df = pd.read_csv('./data/kokil dec 6 reprepare/conf_pc_worker_sem.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP2: weighted by 1 linear variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "Plot below: old throughput (x-axis) vs new throughput (y-axis)\n",
      "WT2: weighted by 1 linear variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "PC2: weighted by 1 linear variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "TL2: weighted by 1 normalised number of words per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "SP2: weighted by average of WT1 and WT2 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######################################\n",
    "## Weighted Onehot Encoding options ##\n",
    "######################################\n",
    "\n",
    "##############\n",
    "# Throughput #\n",
    "##############\n",
    "# TP1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# TP2: weighted by 1 linear variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# TP3 + k: weighted by 1 inverted k-power U-shaped variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# TP4 + k: weighted by 1 upright k-power U-shaped variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# (For TP3 & TP4, k=1 results in V-shaped variance, and as k>1 increases, sides will curve into U-shaped variance)\n",
    "\n",
    "############\n",
    "# Worktime #\n",
    "############\n",
    "# WT1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# WT2: weighted by 1 linear variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "\n",
    "################\n",
    "# PC agreement #\n",
    "################\n",
    "# PC1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# PC2: weighted by 1 linear variance per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# PC3: weighted by 1 PC agreement weight per annotation in each OHE, i.e. (a, b, c, d) -> (w1*a, w2*b, w3*c, w4*d)\n",
    "\n",
    "#####################\n",
    "# Input text length #\n",
    "#####################\n",
    "# TL1: weighted by 1 normalised number of characters per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# TL2: weighted by 1 normalised number of words per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "\n",
    "###################\n",
    "# Special Options #\n",
    "###################\n",
    "# SP1: weighted by average of TP1 and TP2 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# SP2: weighted by average of WT1 and WT2 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# SP3: weighted by average of PC1 and PC2 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# RAND_UNI: weighted by 1 uniformly distributed random number between 0 to 1 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "# RAND_NORM: weighted by 1 normally distributed random number between 0 to 1 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
    "\n",
    "# Select 1 option from each of the few variants above, e.g. TP2, WT1, PC3, TL1, SP3, and input into function\n",
    "# set_OHE_pipeline_options. If not selecting TP3 or TP4, input k (option_k) will be ignored. After\n",
    "# editing the options, run the entire notebook for results accordingly.\n",
    "\n",
    "# Edit option choices here\n",
    "throughput_option = 'TP2'\n",
    "worktime_option = 'WT2'\n",
    "pc_agreement_option = 'PC2'\n",
    "textlength_option = 'TL2'\n",
    "special_option = 'SP2'\n",
    "k_option_for_tp = 3\n",
    "\n",
    "df_throughput, df_worktime, df_agreement, df_textlength, df_special = metadata_options.set_OHE_pipeline_options(df, throughput_option, worktime_option, pc_agreement_option, textlength_option, special_option, k_option_for_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "## Model Options ##\n",
    "######################################\n",
    "# options: lstm, cnn, lstm-attn\n",
    "\n",
    "model_name = 'lstm'\n",
    "models_nn.MODEL_NAME = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input.sentence_id</th>\n",
       "      <th>HITId</th>\n",
       "      <th>Input.convo_id</th>\n",
       "      <th>Input.train_test_val</th>\n",
       "      <th>Input.msg_id</th>\n",
       "      <th>Input.timestamp</th>\n",
       "      <th>Input.full_text</th>\n",
       "      <th>Input.speaker</th>\n",
       "      <th>Input.reply_to</th>\n",
       "      <th>Input.speaker_intention</th>\n",
       "      <th>...</th>\n",
       "      <th>prt</th>\n",
       "      <th>punct</th>\n",
       "      <th>purpcl</th>\n",
       "      <th>quantmod</th>\n",
       "      <th>rcmod</th>\n",
       "      <th>rel</th>\n",
       "      <th>root</th>\n",
       "      <th>tmod</th>\n",
       "      <th>xcomp</th>\n",
       "      <th>xsubj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>3MG8450X2OASXZ0WO9O5AH70GU3UPA</td>\n",
       "      <td>Game1-italy-germany</td>\n",
       "      <td>Train</td>\n",
       "      <td>Game1-italy-germany-3</td>\n",
       "      <td>87</td>\n",
       "      <td>It seems like there are a lot of ways that cou...</td>\n",
       "      <td>germany-Game1</td>\n",
       "      <td>Game1-italy-germany-2</td>\n",
       "      <td>Truth</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>38G0E1M85M552JXSALX4G9WI2I6UVX</td>\n",
       "      <td>Game1-italy-germany</td>\n",
       "      <td>Train</td>\n",
       "      <td>Game1-italy-germany-7</td>\n",
       "      <td>117</td>\n",
       "      <td>Sorry Italy I've been away doing, um, German t...</td>\n",
       "      <td>germany-Game1</td>\n",
       "      <td>Game1-italy-germany-6</td>\n",
       "      <td>Truth</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>3HYV4299H0WQ2B4TCS7PKDQ75WHE81</td>\n",
       "      <td>Game1-italy-germany</td>\n",
       "      <td>Train</td>\n",
       "      <td>Game1-italy-germany-8</td>\n",
       "      <td>119</td>\n",
       "      <td>I don't think I'm ready to go for that idea, h...</td>\n",
       "      <td>germany-Game1</td>\n",
       "      <td>Game1-italy-germany-7</td>\n",
       "      <td>Truth</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>3XU9MCX6VOC4P079IHIO9TCNYLGR2P</td>\n",
       "      <td>Game1-italy-germany</td>\n",
       "      <td>Train</td>\n",
       "      <td>Game1-italy-germany-9</td>\n",
       "      <td>121</td>\n",
       "      <td>I am pretty conflicted about whether to guess ...</td>\n",
       "      <td>italy-Game1</td>\n",
       "      <td>Game1-italy-germany-8</td>\n",
       "      <td>Truth</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>3FVBZG9CLJEK4WQS7P2GC1H2EEQH0Q</td>\n",
       "      <td>Game1-italy-germany</td>\n",
       "      <td>Train</td>\n",
       "      <td>Game1-italy-germany-9</td>\n",
       "      <td>121</td>\n",
       "      <td>I am going to take it literally and say  even ...</td>\n",
       "      <td>italy-Game1</td>\n",
       "      <td>Game1-italy-germany-8</td>\n",
       "      <td>Truth</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 862 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Input.sentence_id                           HITId       Input.convo_id  \\\n",
       "5                 11  3MG8450X2OASXZ0WO9O5AH70GU3UPA  Game1-italy-germany   \n",
       "6                 12  38G0E1M85M552JXSALX4G9WI2I6UVX  Game1-italy-germany   \n",
       "7                 14  3HYV4299H0WQ2B4TCS7PKDQ75WHE81  Game1-italy-germany   \n",
       "8                 15  3XU9MCX6VOC4P079IHIO9TCNYLGR2P  Game1-italy-germany   \n",
       "9                 16  3FVBZG9CLJEK4WQS7P2GC1H2EEQH0Q  Game1-italy-germany   \n",
       "\n",
       "  Input.train_test_val           Input.msg_id  Input.timestamp  \\\n",
       "5                Train  Game1-italy-germany-3               87   \n",
       "6                Train  Game1-italy-germany-7              117   \n",
       "7                Train  Game1-italy-germany-8              119   \n",
       "8                Train  Game1-italy-germany-9              121   \n",
       "9                Train  Game1-italy-germany-9              121   \n",
       "\n",
       "                                     Input.full_text  Input.speaker  \\\n",
       "5  It seems like there are a lot of ways that cou...  germany-Game1   \n",
       "6  Sorry Italy I've been away doing, um, German t...  germany-Game1   \n",
       "7  I don't think I'm ready to go for that idea, h...  germany-Game1   \n",
       "8  I am pretty conflicted about whether to guess ...    italy-Game1   \n",
       "9  I am going to take it literally and say  even ...    italy-Game1   \n",
       "\n",
       "          Input.reply_to Input.speaker_intention  ...  prt punct  purpcl  \\\n",
       "5  Game1-italy-germany-2                   Truth  ...  0.0   0.0     0.0   \n",
       "6  Game1-italy-germany-6                   Truth  ...  0.0   0.0     0.0   \n",
       "7  Game1-italy-germany-7                   Truth  ...  0.0   0.0     0.0   \n",
       "8  Game1-italy-germany-8                   Truth  ...  0.0   0.0     0.0   \n",
       "9  Game1-italy-germany-8                   Truth  ...  0.0   0.0     0.0   \n",
       "\n",
       "   quantmod  rcmod  rel  root  tmod xcomp  xsubj  \n",
       "5       0.0    1.0  0.0   1.0   0.0   0.0    0.0  \n",
       "6       0.0    0.0  0.0   1.0   0.0   0.0    0.0  \n",
       "7       0.0    0.0  0.0   1.0   0.0   1.0    0.0  \n",
       "8       0.0    0.0  0.0   1.0   0.0   0.0    0.0  \n",
       "9       0.0    0.0  0.0   1.0   0.0   2.0    1.0  \n",
       "\n",
       "[5 rows x 862 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old train_test_split code\n",
    "# train, test, indices_train, indices_test = train_test_split(df, indices, test_size=0.2)\n",
    "\n",
    "# New train_test_split using Stratified Shaffled Splits\n",
    "y = df[\"Input.deception_quadrant\"].copy()\n",
    "X = df.drop([\"Input.deception_quadrant\"], axis=1)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "splits_generator = sss.split(X, y)\n",
    "\n",
    "for train_idx, test_idx in splits_generator:\n",
    "    indices_train = train_idx\n",
    "    indices_test = test_idx\n",
    "\n",
    "train = df.take(indices_train)\n",
    "test = df.take(indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11366, 862) (9092, 862) (2274, 862)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape, train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 0.23.2\n"
     ]
    }
   ],
   "source": [
    "# check scikit-learn version\n",
    "import sklearn\n",
    "print('sklearn: %s' % sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lynne\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1], y=[1 1 1 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "C:\\Users\\lynne\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0. 1.], y=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "C:\\Users\\lynne\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0. 1.], y=[1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "C:\\Users\\lynne\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0. 1.], y=[0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "C:\\Users\\lynne\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0. 1.], y=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "new_deception_train = train[\"Input.deception_quadrant\"].copy()\n",
    "new_deception_train['Input.deception_quadrant'] = train[\"Input.deception_quadrant\"].apply(lambda x : 1 if x == \"Straightforward\" else 0)\n",
    "y_train_deception = new_deception_train['Input.deception_quadrant'].to_numpy()\n",
    "deception_class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_deception),\n",
    "                                                 y_train_deception)\n",
    "deception_class_weight_dict = dict(enumerate(deception_class_weights))\n",
    "\n",
    "y_train_rapport = train['Answer.3rapport.yes_label'].tolist()\n",
    "rapport_class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_rapport),\n",
    "                                                 y_train_rapport)\n",
    "rapport_class_weight_dict = dict(enumerate(rapport_class_weights))\n",
    "\n",
    "y_train_share_information = train['Answer.4shareinformation.yes_label'].tolist()\n",
    "share_info_class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_share_information),\n",
    "                                                 y_train_share_information)\n",
    "share_info_class_weight_dict = dict(enumerate(share_info_class_weights))\n",
    "\n",
    "y_train_reasoning = train['Answer.2reasoning.yes_label'].tolist()\n",
    "reasoning_class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_reasoning),\n",
    "                                                 y_train_reasoning)\n",
    "reasoning_class_weight_dict = dict(enumerate(reasoning_class_weights))\n",
    "\n",
    "y_train_gamemove = train['Answer.1gamemove.yes_label'].tolist()\n",
    "gamemove_class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_gamemove),\n",
    "                                                 y_train_gamemove)\n",
    "gamemove_class_weight_dict = dict(enumerate(gamemove_class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights\n",
      "Deception: {0: 10.192825112107624, 1: 0.525792273883877} \n",
      "Rapport: {0: 3.582348305752561, 1: 0.5811069922024799} \n",
      "Share Information: {0: 3.117969821673525, 1: 0.59549384333246} \n",
      "Reasoning: {0: 2.9596354166666665, 1: 0.6016410799364743} \n",
      "Gamemove: {0: 7.048062015503876, 1: 0.5381792352314431}\n"
     ]
    }
   ],
   "source": [
    "print(\"Class weights\")\n",
    "print(\"Deception: {} \\nRapport: {} \\nShare Information: {} \\nReasoning: {} \\nGamemove: {}\".format(deception_class_weight_dict,\n",
    "                                                                                                  rapport_class_weight_dict,\n",
    "                                                                                                  share_info_class_weight_dict,\n",
    "                                                                                                  reasoning_class_weight_dict,\n",
    "                                                                                                  gamemove_class_weight_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_col = train['Input.full_text']\n",
    "\n",
    "new_deception_test = test[\"Input.deception_quadrant\"].copy()\n",
    "new_deception_test['Input.deception_quadrant'] = test[\"Input.deception_quadrant\"].apply(lambda x : 1 if x == \"Straightforward\" else 0)\n",
    "y_test_deception = new_deception_test['Input.deception_quadrant'].tolist()\n",
    "y_test_rapport = test['Answer.3rapport.yes_label'].tolist()\n",
    "y_test_share_information = test['Answer.4shareinformation.yes_label'].tolist()\n",
    "y_test_reasoning = test['Answer.2reasoning.yes_label'].tolist()\n",
    "y_test_gamemove = test['Answer.1gamemove.yes_label'].tolist()\n",
    "\n",
    "X_test_col = test['Input.full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lynne\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "# def label_preprocessing(y_data, label_encoder):\n",
    "#     out = label_encoder.fit_transform(y_data).reshape(-1,1)\n",
    "#     return out\n",
    "\n",
    "y_train_deception = le.fit_transform(y_train_deception)\n",
    "y_train_deception = y_train_deception.reshape(-1,1)\n",
    "\n",
    "y_train_rapport = le.fit_transform(y_train_rapport)\n",
    "y_train_rapport = y_train_rapport.reshape(-1,1)\n",
    "\n",
    "y_train_share_information = le.fit_transform(y_train_share_information)\n",
    "y_train_share_information = y_train_share_information.reshape(-1,1)\n",
    "\n",
    "y_train_reasoning = le.fit_transform(y_train_reasoning)\n",
    "y_train_reasoning = y_train_reasoning.reshape(-1,1)\n",
    "\n",
    "y_train_gamemove = le.fit_transform(y_train_gamemove)\n",
    "y_train_gamemove = y_train_gamemove.reshape(-1,1)\n",
    "\n",
    "y_train_deception = le.fit_transform(y_train_deception)\n",
    "y_train_deception = y_train_deception.reshape(-1,1)\n",
    "\n",
    "y_test_rapport = le.fit_transform(y_test_rapport)\n",
    "y_test_rapport = y_test_rapport.reshape(-1,1)\n",
    "\n",
    "y_test_share_information = le.fit_transform(y_test_share_information)\n",
    "y_test_share_information = y_test_share_information.reshape(-1,1)\n",
    "\n",
    "y_test_reasoning = le.fit_transform(y_test_reasoning)\n",
    "y_test_reasoning = y_test_reasoning.reshape(-1,1)\n",
    "\n",
    "y_test_gamemove = le.fit_transform(y_test_gamemove)\n",
    "y_test_gamemove = y_test_gamemove.reshape(-1,1)\n",
    "\n",
    "y_test_deception = le.fit_transform(y_test_deception)\n",
    "y_test_deception = y_test_deception.reshape(-1,1)\n",
    "\n",
    "y_test_rapport = np.asarray(y_test_rapport)\n",
    "y_train_rapport = np.asarray(y_train_rapport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 220\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "\n",
    "tok.fit_on_texts(X_train_col)\n",
    "X_train_sequences = tok.texts_to_sequences(X_train_col)\n",
    "X_train = pad_sequences(X_train_sequences, maxlen=max_len)\n",
    "\n",
    "X_test_sequences = tok.texts_to_sequences(X_test_col)\n",
    "X_test = pad_sequences(X_test_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct individual  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 3s 43ms/step - loss: 0.6930 - accuracy: 0.3623 - f1_m: 0.3419 - recall_m: 0.3136 - precision_m: 0.6262 - val_loss: 0.6272 - val_accuracy: 0.8712 - val_f1_m: 0.9305 - val_recall_m: 1.0000 - val_precision_m: 0.8704\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.6883 - accuracy: 0.6583 - f1_m: 0.7451 - recall_m: 0.6945 - precision_m: 0.8849 - val_loss: 0.7285 - val_accuracy: 0.4125 - val_f1_m: 0.5268 - val_recall_m: 0.3787 - val_precision_m: 0.8782\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.6698 - accuracy: 0.6003 - f1_m: 0.7173 - recall_m: 0.6035 - precision_m: 0.9059 - val_loss: 0.5787 - val_accuracy: 0.7696 - val_f1_m: 0.8660 - val_recall_m: 0.8635 - val_precision_m: 0.8691\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.6435 - accuracy: 0.6467 - f1_m: 0.7510 - recall_m: 0.6475 - precision_m: 0.9167 - val_loss: 0.7078 - val_accuracy: 0.5281 - val_f1_m: 0.6660 - val_recall_m: 0.5443 - val_precision_m: 0.8614\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.6211 - accuracy: 0.6366 - f1_m: 0.7387 - recall_m: 0.6228 - precision_m: 0.9285 - val_loss: 0.5977 - val_accuracy: 0.6781 - val_f1_m: 0.8000 - val_recall_m: 0.7465 - val_precision_m: 0.8637\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.6003 - accuracy: 0.6555 - f1_m: 0.7567 - recall_m: 0.6446 - precision_m: 0.9332 - val_loss: 0.7834 - val_accuracy: 0.4714 - val_f1_m: 0.6045 - val_recall_m: 0.4666 - val_precision_m: 0.8618\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.5836 - accuracy: 0.6642 - f1_m: 0.7551 - recall_m: 0.6432 - precision_m: 0.9271 - val_loss: 0.5652 - val_accuracy: 0.7010 - val_f1_m: 0.8178 - val_recall_m: 0.7781 - val_precision_m: 0.8632\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.5708 - accuracy: 0.6803 - f1_m: 0.7814 - recall_m: 0.6742 - precision_m: 0.9443 - val_loss: 0.9740 - val_accuracy: 0.3320 - val_f1_m: 0.4156 - val_recall_m: 0.2734 - val_precision_m: 0.8692\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.5509 - accuracy: 0.6701 - f1_m: 0.7713 - recall_m: 0.6555 - precision_m: 0.9531 - val_loss: 0.8695 - val_accuracy: 0.4635 - val_f1_m: 0.5921 - val_recall_m: 0.4499 - val_precision_m: 0.8683\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.5375 - accuracy: 0.6771 - f1_m: 0.7741 - recall_m: 0.6561 - precision_m: 0.9567 - val_loss: 0.6485 - val_accuracy: 0.6561 - val_f1_m: 0.7829 - val_recall_m: 0.7176 - val_precision_m: 0.8630\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.5395 - accuracy: 0.6874 - f1_m: 0.7827 - recall_m: 0.6691 - precision_m: 0.9544 - val_loss: 0.8185 - val_accuracy: 0.5004 - val_f1_m: 0.6371 - val_recall_m: 0.5047 - val_precision_m: 0.8649\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.5155 - accuracy: 0.6925 - f1_m: 0.7893 - recall_m: 0.6741 - precision_m: 0.9633 - val_loss: 0.9034 - val_accuracy: 0.4947 - val_f1_m: 0.6285 - val_recall_m: 0.4935 - val_precision_m: 0.8683\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.5037 - accuracy: 0.7009 - f1_m: 0.7915 - recall_m: 0.6791 - precision_m: 0.9587 - val_loss: 1.0089 - val_accuracy: 0.3773 - val_f1_m: 0.4849 - val_recall_m: 0.3377 - val_precision_m: 0.8654\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4939 - accuracy: 0.6991 - f1_m: 0.7939 - recall_m: 0.6787 - precision_m: 0.9673 - val_loss: 0.6976 - val_accuracy: 0.6785 - val_f1_m: 0.8004 - val_recall_m: 0.7455 - val_precision_m: 0.8650\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4786 - accuracy: 0.7259 - f1_m: 0.8117 - recall_m: 0.7053 - precision_m: 0.9643 - val_loss: 0.9528 - val_accuracy: 0.3597 - val_f1_m: 0.4611 - val_recall_m: 0.3160 - val_precision_m: 0.8608\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4708 - accuracy: 0.7122 - f1_m: 0.8035 - recall_m: 0.6924 - precision_m: 0.9695 - val_loss: 0.7451 - val_accuracy: 0.6315 - val_f1_m: 0.7615 - val_recall_m: 0.6826 - val_precision_m: 0.8632\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4498 - accuracy: 0.7352 - f1_m: 0.8200 - recall_m: 0.7147 - precision_m: 0.9716 - val_loss: 0.7262 - val_accuracy: 0.6724 - val_f1_m: 0.7951 - val_recall_m: 0.7361 - val_precision_m: 0.8660\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4487 - accuracy: 0.7319 - f1_m: 0.8150 - recall_m: 0.7077 - precision_m: 0.9710 - val_loss: 0.7844 - val_accuracy: 0.6627 - val_f1_m: 0.7867 - val_recall_m: 0.7182 - val_precision_m: 0.8712\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4391 - accuracy: 0.7476 - f1_m: 0.8272 - recall_m: 0.7226 - precision_m: 0.9746 - val_loss: 0.8126 - val_accuracy: 0.6354 - val_f1_m: 0.7653 - val_recall_m: 0.6867 - val_precision_m: 0.8657\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4296 - accuracy: 0.7401 - f1_m: 0.8218 - recall_m: 0.7147 - precision_m: 0.9751 - val_loss: 0.7712 - val_accuracy: 0.6728 - val_f1_m: 0.7951 - val_recall_m: 0.7353 - val_precision_m: 0.8667\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4191 - accuracy: 0.7534 - f1_m: 0.8363 - recall_m: 0.7363 - precision_m: 0.9758 - val_loss: 1.0318 - val_accuracy: 0.6064 - val_f1_m: 0.7389 - val_recall_m: 0.6458 - val_precision_m: 0.8660\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.4081 - accuracy: 0.7504 - f1_m: 0.8288 - recall_m: 0.7225 - precision_m: 0.9804 - val_loss: 0.9152 - val_accuracy: 0.6530 - val_f1_m: 0.7793 - val_recall_m: 0.7090 - val_precision_m: 0.8669\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3994 - accuracy: 0.7519 - f1_m: 0.8295 - recall_m: 0.7253 - precision_m: 0.9787 - val_loss: 0.8210 - val_accuracy: 0.6605 - val_f1_m: 0.7843 - val_recall_m: 0.7161 - val_precision_m: 0.8688\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3922 - accuracy: 0.7579 - f1_m: 0.8333 - recall_m: 0.7294 - precision_m: 0.9801 - val_loss: 1.0535 - val_accuracy: 0.4978 - val_f1_m: 0.6315 - val_recall_m: 0.4965 - val_precision_m: 0.8697\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3858 - accuracy: 0.7557 - f1_m: 0.8323 - recall_m: 0.7278 - precision_m: 0.9815 - val_loss: 1.0203 - val_accuracy: 0.6675 - val_f1_m: 0.7904 - val_recall_m: 0.7263 - val_precision_m: 0.8685\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3776 - accuracy: 0.7668 - f1_m: 0.8324 - recall_m: 0.7335 - precision_m: 0.9680 - val_loss: 0.9952 - val_accuracy: 0.4186 - val_f1_m: 0.5423 - val_recall_m: 0.3974 - val_precision_m: 0.8590\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3767 - accuracy: 0.7559 - f1_m: 0.8332 - recall_m: 0.7269 - precision_m: 0.9828 - val_loss: 1.4192 - val_accuracy: 0.5550 - val_f1_m: 0.6899 - val_recall_m: 0.5735 - val_precision_m: 0.8687\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3637 - accuracy: 0.7710 - f1_m: 0.8440 - recall_m: 0.7434 - precision_m: 0.9840 - val_loss: 1.2159 - val_accuracy: 0.4802 - val_f1_m: 0.6156 - val_recall_m: 0.4819 - val_precision_m: 0.8572\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3647 - accuracy: 0.7643 - f1_m: 0.8410 - recall_m: 0.7388 - precision_m: 0.9842 - val_loss: 1.0026 - val_accuracy: 0.6513 - val_f1_m: 0.7766 - val_recall_m: 0.7030 - val_precision_m: 0.8691\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3429 - accuracy: 0.7854 - f1_m: 0.8576 - recall_m: 0.7621 - precision_m: 0.9846 - val_loss: 1.3969 - val_accuracy: 0.5405 - val_f1_m: 0.6771 - val_recall_m: 0.5579 - val_precision_m: 0.8639\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3508 - accuracy: 0.7701 - f1_m: 0.8439 - recall_m: 0.7432 - precision_m: 0.9825 - val_loss: 1.2710 - val_accuracy: 0.7493 - val_f1_m: 0.8520 - val_recall_m: 0.8345 - val_precision_m: 0.8712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3486 - accuracy: 0.7850 - f1_m: 0.8579 - recall_m: 0.7635 - precision_m: 0.9836 - val_loss: 1.0847 - val_accuracy: 0.6297 - val_f1_m: 0.7578 - val_recall_m: 0.6722 - val_precision_m: 0.8704\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3343 - accuracy: 0.7866 - f1_m: 0.8526 - recall_m: 0.7574 - precision_m: 0.9858 - val_loss: 1.0513 - val_accuracy: 0.6574 - val_f1_m: 0.7817 - val_recall_m: 0.7088 - val_precision_m: 0.8722\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3171 - accuracy: 0.7934 - f1_m: 0.8632 - recall_m: 0.7695 - precision_m: 0.9878 - val_loss: 1.1801 - val_accuracy: 0.5941 - val_f1_m: 0.7261 - val_recall_m: 0.6234 - val_precision_m: 0.8713\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3215 - accuracy: 0.7982 - f1_m: 0.8682 - recall_m: 0.7778 - precision_m: 0.9880 - val_loss: 1.1668 - val_accuracy: 0.6394 - val_f1_m: 0.7666 - val_recall_m: 0.6853 - val_precision_m: 0.8715\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3204 - accuracy: 0.7992 - f1_m: 0.8700 - recall_m: 0.7812 - precision_m: 0.9860 - val_loss: 1.3045 - val_accuracy: 0.6495 - val_f1_m: 0.7760 - val_recall_m: 0.7020 - val_precision_m: 0.8692\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3030 - accuracy: 0.8047 - f1_m: 0.8704 - recall_m: 0.7810 - precision_m: 0.9882 - val_loss: 1.8709 - val_accuracy: 0.5827 - val_f1_m: 0.7165 - val_recall_m: 0.6107 - val_precision_m: 0.8687\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3160 - accuracy: 0.7961 - f1_m: 0.8599 - recall_m: 0.7702 - precision_m: 0.9844 - val_loss: 1.4388 - val_accuracy: 0.6671 - val_f1_m: 0.7899 - val_recall_m: 0.7226 - val_precision_m: 0.8722\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2949 - accuracy: 0.8077 - f1_m: 0.8736 - recall_m: 0.7861 - precision_m: 0.9871 - val_loss: 2.0209 - val_accuracy: 0.4789 - val_f1_m: 0.6137 - val_recall_m: 0.4773 - val_precision_m: 0.8618\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2957 - accuracy: 0.8098 - f1_m: 0.8772 - recall_m: 0.7910 - precision_m: 0.9890 - val_loss: 1.3564 - val_accuracy: 0.6658 - val_f1_m: 0.7882 - val_recall_m: 0.7203 - val_precision_m: 0.8719\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2795 - accuracy: 0.8194 - f1_m: 0.8814 - recall_m: 0.7989 - precision_m: 0.9873 - val_loss: 1.7010 - val_accuracy: 0.6649 - val_f1_m: 0.7882 - val_recall_m: 0.7216 - val_precision_m: 0.8700\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2936 - accuracy: 0.8182 - f1_m: 0.8789 - recall_m: 0.7951 - precision_m: 0.9874 - val_loss: 1.3991 - val_accuracy: 0.6539 - val_f1_m: 0.7789 - val_recall_m: 0.7042 - val_precision_m: 0.8723\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2826 - accuracy: 0.8186 - f1_m: 0.8832 - recall_m: 0.8030 - precision_m: 0.9866 - val_loss: 1.6417 - val_accuracy: 0.6873 - val_f1_m: 0.8055 - val_recall_m: 0.7492 - val_precision_m: 0.8723\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2683 - accuracy: 0.8336 - f1_m: 0.8918 - recall_m: 0.8155 - precision_m: 0.9867 - val_loss: 1.5870 - val_accuracy: 0.6768 - val_f1_m: 0.7965 - val_recall_m: 0.7302 - val_precision_m: 0.8770\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4947 - accuracy: 0.8211 - f1_m: 0.8809 - recall_m: 0.8095 - precision_m: 0.9823 - val_loss: 1.5209 - val_accuracy: 0.6473 - val_f1_m: 0.7725 - val_recall_m: 0.6925 - val_precision_m: 0.8750\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2483 - accuracy: 0.8491 - f1_m: 0.9053 - recall_m: 0.8386 - precision_m: 0.9861 - val_loss: 1.9308 - val_accuracy: 0.4679 - val_f1_m: 0.5978 - val_recall_m: 0.4559 - val_precision_m: 0.8709\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2773 - accuracy: 0.8345 - f1_m: 0.8922 - recall_m: 0.8193 - precision_m: 0.9856 - val_loss: 1.3053 - val_accuracy: 0.7111 - val_f1_m: 0.8233 - val_recall_m: 0.7796 - val_precision_m: 0.8731\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2654 - accuracy: 0.8553 - f1_m: 0.9049 - recall_m: 0.8415 - precision_m: 0.9833 - val_loss: 2.4223 - val_accuracy: 0.5255 - val_f1_m: 0.6606 - val_recall_m: 0.5323 - val_precision_m: 0.8731\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2573 - accuracy: 0.8598 - f1_m: 0.9109 - recall_m: 0.8497 - precision_m: 0.9844 - val_loss: 1.6469 - val_accuracy: 0.6799 - val_f1_m: 0.7997 - val_recall_m: 0.7388 - val_precision_m: 0.8727\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2513 - accuracy: 0.8619 - f1_m: 0.9116 - recall_m: 0.8511 - precision_m: 0.9855 - val_loss: 1.4787 - val_accuracy: 0.6759 - val_f1_m: 0.7964 - val_recall_m: 0.7342 - val_precision_m: 0.8715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27e734183c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rapport model\n",
    "rapport_model = models_nn.create_nn_model()\n",
    "rapport_model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', models_nn.f1_m, \n",
    "                                                                              models_nn.recall_m, models_nn.precision_m])\n",
    "rapport_model.fit(X_train,y_train_rapport,\n",
    "                  batch_size=128,\n",
    "                  epochs=50,\n",
    "                  validation_data=(X_test, y_test_rapport), \n",
    "#                   callbacks=[models_nn.early_stop],\n",
    "                  class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.503282522496854, 0.5057164220504348, 0.4891135395712358, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rapport_pred = rapport_model.predict(X_train)\n",
    "rapport_pred_test = rapport_model.predict(X_test)\n",
    "\n",
    "rapport_pred_test_round = rapport_pred_test.round()\n",
    "precision_recall_fscore_support(y_test_rapport, rapport_pred_test_round, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 3s 42ms/step - loss: 0.6932 - accuracy: 0.6771 - f1_m: 0.7438 - recall_m: 0.7031 - precision_m: 0.9297 - val_loss: 0.6595 - val_accuracy: 0.8615 - val_f1_m: 0.9248 - val_recall_m: 0.9110 - val_precision_m: 0.9396\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.6764 - accuracy: 0.6747 - f1_m: 0.7768 - recall_m: 0.6815 - precision_m: 0.9323 - val_loss: 1.4055 - val_accuracy: 0.0629 - val_f1_m: 8.9606e-04 - val_recall_m: 4.5167e-04 - val_precision_m: 0.0556\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.6427 - accuracy: 0.5870 - f1_m: 0.7014 - recall_m: 0.5758 - precision_m: 0.9506 - val_loss: 0.4839 - val_accuracy: 0.7726 - val_f1_m: 0.8691 - val_recall_m: 0.8090 - val_precision_m: 0.9408\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.5967 - accuracy: 0.6440 - f1_m: 0.7643 - recall_m: 0.6404 - precision_m: 0.9695 - val_loss: 0.4624 - val_accuracy: 0.7573 - val_f1_m: 0.8589 - val_recall_m: 0.7895 - val_precision_m: 0.9436\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.5636 - accuracy: 0.6468 - f1_m: 0.7645 - recall_m: 0.6358 - precision_m: 0.9746 - val_loss: 0.5737 - val_accuracy: 0.6513 - val_f1_m: 0.7838 - val_recall_m: 0.6755 - val_precision_m: 0.9362\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.5354 - accuracy: 0.6634 - f1_m: 0.7806 - recall_m: 0.6567 - precision_m: 0.9801 - val_loss: 0.5303 - val_accuracy: 0.7027 - val_f1_m: 0.8218 - val_recall_m: 0.7324 - val_precision_m: 0.9381\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.5062 - accuracy: 0.6842 - f1_m: 0.7912 - recall_m: 0.6707 - precision_m: 0.9822 - val_loss: 0.6207 - val_accuracy: 0.6266 - val_f1_m: 0.7641 - val_recall_m: 0.6457 - val_precision_m: 0.9387\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4742 - accuracy: 0.6881 - f1_m: 0.7949 - recall_m: 0.6729 - precision_m: 0.9851 - val_loss: 0.4824 - val_accuracy: 0.8030 - val_f1_m: 0.8894 - val_recall_m: 0.8476 - val_precision_m: 0.9370\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4587 - accuracy: 0.7081 - f1_m: 0.8056 - recall_m: 0.6899 - precision_m: 0.9860 - val_loss: 0.5486 - val_accuracy: 0.7300 - val_f1_m: 0.8404 - val_recall_m: 0.7613 - val_precision_m: 0.9408\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4514 - accuracy: 0.6985 - f1_m: 0.8048 - recall_m: 0.6864 - precision_m: 0.9871 - val_loss: 0.6092 - val_accuracy: 0.7357 - val_f1_m: 0.8447 - val_recall_m: 0.7696 - val_precision_m: 0.9388\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4389 - accuracy: 0.7084 - f1_m: 0.8127 - recall_m: 0.6938 - precision_m: 0.9907 - val_loss: 0.6524 - val_accuracy: 0.7894 - val_f1_m: 0.8803 - val_recall_m: 0.8310 - val_precision_m: 0.9377\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4347 - accuracy: 0.7245 - f1_m: 0.8228 - recall_m: 0.7086 - precision_m: 0.9899 - val_loss: 0.5813 - val_accuracy: 0.7608 - val_f1_m: 0.8621 - val_recall_m: 0.8015 - val_precision_m: 0.9344\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4102 - accuracy: 0.7304 - f1_m: 0.8287 - recall_m: 0.7177 - precision_m: 0.9902 - val_loss: 0.8907 - val_accuracy: 0.5589 - val_f1_m: 0.7052 - val_recall_m: 0.5641 - val_precision_m: 0.9443\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3985 - accuracy: 0.7383 - f1_m: 0.8336 - recall_m: 0.7241 - precision_m: 0.9913 - val_loss: 0.9014 - val_accuracy: 0.5607 - val_f1_m: 0.7088 - val_recall_m: 0.5720 - val_precision_m: 0.9366\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4002 - accuracy: 0.7377 - f1_m: 0.8323 - recall_m: 0.7214 - precision_m: 0.9916 - val_loss: 0.6786 - val_accuracy: 0.6482 - val_f1_m: 0.7809 - val_recall_m: 0.6705 - val_precision_m: 0.9382\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3894 - accuracy: 0.7497 - f1_m: 0.8397 - recall_m: 0.7332 - precision_m: 0.9929 - val_loss: 0.7525 - val_accuracy: 0.8369 - val_f1_m: 0.9102 - val_recall_m: 0.8849 - val_precision_m: 0.9384\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3965 - accuracy: 0.7482 - f1_m: 0.8418 - recall_m: 0.7350 - precision_m: 0.9934 - val_loss: 0.7251 - val_accuracy: 0.7076 - val_f1_m: 0.8246 - val_recall_m: 0.7355 - val_precision_m: 0.9411\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3696 - accuracy: 0.7659 - f1_m: 0.8518 - recall_m: 0.7499 - precision_m: 0.9939 - val_loss: 0.7811 - val_accuracy: 0.7397 - val_f1_m: 0.8478 - val_recall_m: 0.7755 - val_precision_m: 0.9368\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3778 - accuracy: 0.7491 - f1_m: 0.8363 - recall_m: 0.7297 - precision_m: 0.9924 - val_loss: 0.7595 - val_accuracy: 0.7753 - val_f1_m: 0.8715 - val_recall_m: 0.8151 - val_precision_m: 0.9382\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3727 - accuracy: 0.7687 - f1_m: 0.8548 - recall_m: 0.7556 - precision_m: 0.9938 - val_loss: 0.8053 - val_accuracy: 0.6174 - val_f1_m: 0.7569 - val_recall_m: 0.6387 - val_precision_m: 0.9347\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3519 - accuracy: 0.7806 - f1_m: 0.8661 - recall_m: 0.7711 - precision_m: 0.9946 - val_loss: 0.8267 - val_accuracy: 0.7150 - val_f1_m: 0.8305 - val_recall_m: 0.7476 - val_precision_m: 0.9373\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3421 - accuracy: 0.7845 - f1_m: 0.8696 - recall_m: 0.7757 - precision_m: 0.9944 - val_loss: 0.9428 - val_accuracy: 0.6007 - val_f1_m: 0.7420 - val_recall_m: 0.6152 - val_precision_m: 0.9395\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3459 - accuracy: 0.7799 - f1_m: 0.8649 - recall_m: 0.7711 - precision_m: 0.9937 - val_loss: 1.1716 - val_accuracy: 0.4433 - val_f1_m: 0.5936 - val_recall_m: 0.4344 - val_precision_m: 0.9409\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4386 - accuracy: 0.7697 - f1_m: 0.8570 - recall_m: 0.7616 - precision_m: 0.9935 - val_loss: 1.2948 - val_accuracy: 0.3975 - val_f1_m: 0.5412 - val_recall_m: 0.3802 - val_precision_m: 0.9446\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3284 - accuracy: 0.7860 - f1_m: 0.8693 - recall_m: 0.7766 - precision_m: 0.9948 - val_loss: 0.8582 - val_accuracy: 0.8615 - val_f1_m: 0.9249 - val_recall_m: 0.9130 - val_precision_m: 0.9374\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3349 - accuracy: 0.8041 - f1_m: 0.8810 - recall_m: 0.7936 - precision_m: 0.9940 - val_loss: 0.9693 - val_accuracy: 0.7493 - val_f1_m: 0.8542 - val_recall_m: 0.7863 - val_precision_m: 0.9365\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.3386 - accuracy: 0.7863 - f1_m: 0.8667 - recall_m: 0.7744 - precision_m: 0.9945 - val_loss: 0.7350 - val_accuracy: 0.8096 - val_f1_m: 0.8936 - val_recall_m: 0.8541 - val_precision_m: 0.9378\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3136 - accuracy: 0.8079 - f1_m: 0.8846 - recall_m: 0.8006 - precision_m: 0.9941 - val_loss: 1.1204 - val_accuracy: 0.6152 - val_f1_m: 0.7545 - val_recall_m: 0.6315 - val_precision_m: 0.9400\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.3207 - accuracy: 0.8062 - f1_m: 0.8818 - recall_m: 0.7963 - precision_m: 0.9940 - val_loss: 0.8647 - val_accuracy: 0.6517 - val_f1_m: 0.7840 - val_recall_m: 0.6761 - val_precision_m: 0.9355\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3012 - accuracy: 0.8112 - f1_m: 0.8851 - recall_m: 0.8001 - precision_m: 0.9951 - val_loss: 0.9829 - val_accuracy: 0.8091 - val_f1_m: 0.8932 - val_recall_m: 0.8536 - val_precision_m: 0.9377\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2929 - accuracy: 0.8220 - f1_m: 0.8893 - recall_m: 0.8078 - precision_m: 0.9958 - val_loss: 0.9373 - val_accuracy: 0.7076 - val_f1_m: 0.8257 - val_recall_m: 0.7396 - val_precision_m: 0.9363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3038 - accuracy: 0.8114 - f1_m: 0.8849 - recall_m: 0.8012 - precision_m: 0.9942 - val_loss: 1.0132 - val_accuracy: 0.7674 - val_f1_m: 0.8665 - val_recall_m: 0.8071 - val_precision_m: 0.9369\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2889 - accuracy: 0.8203 - f1_m: 0.8877 - recall_m: 0.8062 - precision_m: 0.9957 - val_loss: 0.9519 - val_accuracy: 0.7537 - val_f1_m: 0.8573 - val_recall_m: 0.7915 - val_precision_m: 0.9370\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2922 - accuracy: 0.8226 - f1_m: 0.8921 - recall_m: 0.8128 - precision_m: 0.9945 - val_loss: 0.9357 - val_accuracy: 0.7023 - val_f1_m: 0.8221 - val_recall_m: 0.7345 - val_precision_m: 0.9350\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2754 - accuracy: 0.8380 - f1_m: 0.9000 - recall_m: 0.8246 - precision_m: 0.9959 - val_loss: 1.0243 - val_accuracy: 0.7419 - val_f1_m: 0.8499 - val_recall_m: 0.7805 - val_precision_m: 0.9349\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2642 - accuracy: 0.8396 - f1_m: 0.9059 - recall_m: 0.8341 - precision_m: 0.9953 - val_loss: 1.2005 - val_accuracy: 0.7392 - val_f1_m: 0.8479 - val_recall_m: 0.7771 - val_precision_m: 0.9348\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2521 - accuracy: 0.8484 - f1_m: 0.9067 - recall_m: 0.8357 - precision_m: 0.9961 - val_loss: 1.0267 - val_accuracy: 0.7964 - val_f1_m: 0.8855 - val_recall_m: 0.8422 - val_precision_m: 0.9346\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3331 - accuracy: 0.8369 - f1_m: 0.9014 - recall_m: 0.8281 - precision_m: 0.9949 - val_loss: 0.9672 - val_accuracy: 0.7379 - val_f1_m: 0.8468 - val_recall_m: 0.7733 - val_precision_m: 0.9373\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2609 - accuracy: 0.8521 - f1_m: 0.9134 - recall_m: 0.8467 - precision_m: 0.9956 - val_loss: 1.5654 - val_accuracy: 0.5286 - val_f1_m: 0.6778 - val_recall_m: 0.5303 - val_precision_m: 0.9432\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2455 - accuracy: 0.8535 - f1_m: 0.9114 - recall_m: 0.8441 - precision_m: 0.9961 - val_loss: 1.1210 - val_accuracy: 0.7762 - val_f1_m: 0.8724 - val_recall_m: 0.8181 - val_precision_m: 0.9358\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2608 - accuracy: 0.8525 - f1_m: 0.9109 - recall_m: 0.8479 - precision_m: 0.9905 - val_loss: 1.6293 - val_accuracy: 0.5479 - val_f1_m: 0.6946 - val_recall_m: 0.5490 - val_precision_m: 0.9482\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.2357 - accuracy: 0.8560 - f1_m: 0.9142 - recall_m: 0.8465 - precision_m: 0.9969 - val_loss: 0.9762 - val_accuracy: 0.7700 - val_f1_m: 0.8685 - val_recall_m: 0.8114 - val_precision_m: 0.9355\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2383 - accuracy: 0.8724 - f1_m: 0.9264 - recall_m: 0.8687 - precision_m: 0.9954 - val_loss: 1.3824 - val_accuracy: 0.6605 - val_f1_m: 0.7905 - val_recall_m: 0.6833 - val_precision_m: 0.9398\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2327 - accuracy: 0.8666 - f1_m: 0.9219 - recall_m: 0.8611 - precision_m: 0.9971 - val_loss: 1.1101 - val_accuracy: 0.7115 - val_f1_m: 0.8284 - val_recall_m: 0.7436 - val_precision_m: 0.9366\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2248 - accuracy: 0.8752 - f1_m: 0.9254 - recall_m: 0.8658 - precision_m: 0.9969 - val_loss: 1.3006 - val_accuracy: 0.7353 - val_f1_m: 0.8452 - val_recall_m: 0.7718 - val_precision_m: 0.9356\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2019 - accuracy: 0.8822 - f1_m: 0.9320 - recall_m: 0.8774 - precision_m: 0.9969 - val_loss: 1.1526 - val_accuracy: 0.7524 - val_f1_m: 0.8565 - val_recall_m: 0.7890 - val_precision_m: 0.9380\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2226 - accuracy: 0.8740 - f1_m: 0.9264 - recall_m: 0.8699 - precision_m: 0.9956 - val_loss: 1.4731 - val_accuracy: 0.5488 - val_f1_m: 0.6970 - val_recall_m: 0.5543 - val_precision_m: 0.9422\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2001 - accuracy: 0.8902 - f1_m: 0.9372 - recall_m: 0.8864 - precision_m: 0.9969 - val_loss: 1.2919 - val_accuracy: 0.7194 - val_f1_m: 0.8341 - val_recall_m: 0.7532 - val_precision_m: 0.9357\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2132 - accuracy: 0.8862 - f1_m: 0.9333 - recall_m: 0.8826 - precision_m: 0.9960 - val_loss: 1.2564 - val_accuracy: 0.6785 - val_f1_m: 0.8048 - val_recall_m: 0.7080 - val_precision_m: 0.9339\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.1914 - accuracy: 0.8964 - f1_m: 0.9411 - recall_m: 0.8932 - precision_m: 0.9966 - val_loss: 1.4595 - val_accuracy: 0.6645 - val_f1_m: 0.7936 - val_recall_m: 0.6894 - val_precision_m: 0.9367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27e79a54a08>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Game move model\n",
    "gamemove_model = models_nn.create_nn_model()\n",
    "gamemove_model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', models_nn.f1_m, \n",
    "                                                                               models_nn.recall_m, models_nn.precision_m])\n",
    "gamemove_model.fit(X_train,\n",
    "                   y_train_gamemove,\n",
    "                   batch_size=128,\n",
    "                   epochs=50,\n",
    "                   validation_data=(X_test, y_test_gamemove), \n",
    "#                    callbacks=[models_nn.early_stop],\n",
    "                   class_weight=gamemove_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49882116026353296, 0.49568612425019154, 0.44751183826781565, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamemove_pred = gamemove_model.predict(X_train)\n",
    "gamemove_pred_test = gamemove_model.predict(X_test)\n",
    "\n",
    "gamemove_pred_test_round = gamemove_pred_test.round()\n",
    "precision_recall_fscore_support(y_test_gamemove, gamemove_pred_test_round, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.6937 - accuracy: 0.6501 - f1_m: 0.7356 - recall_m: 0.7338 - precision_m: 0.8334 - val_loss: 0.6768 - val_accuracy: 0.8210 - val_f1_m: 0.9005 - val_recall_m: 0.9984 - val_precision_m: 0.8213\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.6885 - accuracy: 0.6325 - f1_m: 0.7392 - recall_m: 0.6776 - precision_m: 0.8605 - val_loss: 0.6197 - val_accuracy: 0.7524 - val_f1_m: 0.8532 - val_recall_m: 0.8838 - val_precision_m: 0.8263\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.6695 - accuracy: 0.6177 - f1_m: 0.7203 - recall_m: 0.6276 - precision_m: 0.8781 - val_loss: 0.6999 - val_accuracy: 0.5312 - val_f1_m: 0.6487 - val_recall_m: 0.5326 - val_precision_m: 0.8367\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.6479 - accuracy: 0.6142 - f1_m: 0.7209 - recall_m: 0.6097 - precision_m: 0.8936 - val_loss: 0.5992 - val_accuracy: 0.6922 - val_f1_m: 0.8083 - val_recall_m: 0.7947 - val_precision_m: 0.8249\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.6260 - accuracy: 0.6385 - f1_m: 0.7348 - recall_m: 0.6286 - precision_m: 0.9046 - val_loss: 0.5735 - val_accuracy: 0.7203 - val_f1_m: 0.8306 - val_recall_m: 0.8399 - val_precision_m: 0.8236\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.6053 - accuracy: 0.6622 - f1_m: 0.7629 - recall_m: 0.6597 - precision_m: 0.9137 - val_loss: 0.5944 - val_accuracy: 0.7128 - val_f1_m: 0.8232 - val_recall_m: 0.8184 - val_precision_m: 0.8298\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.5860 - accuracy: 0.6751 - f1_m: 0.7658 - recall_m: 0.6612 - precision_m: 0.9226 - val_loss: 0.6119 - val_accuracy: 0.7010 - val_f1_m: 0.8150 - val_recall_m: 0.8066 - val_precision_m: 0.8257\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.5697 - accuracy: 0.6830 - f1_m: 0.7746 - recall_m: 0.6694 - precision_m: 0.9280 - val_loss: 0.6180 - val_accuracy: 0.7032 - val_f1_m: 0.8168 - val_recall_m: 0.8099 - val_precision_m: 0.8264\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.5505 - accuracy: 0.6887 - f1_m: 0.7831 - recall_m: 0.6804 - precision_m: 0.9319 - val_loss: 0.9301 - val_accuracy: 0.3553 - val_f1_m: 0.4097 - val_recall_m: 0.2751 - val_precision_m: 0.8178\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.5418 - accuracy: 0.6831 - f1_m: 0.7761 - recall_m: 0.6657 - precision_m: 0.9420 - val_loss: 0.7147 - val_accuracy: 0.6113 - val_f1_m: 0.7371 - val_recall_m: 0.6674 - val_precision_m: 0.8265\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.5237 - accuracy: 0.7061 - f1_m: 0.7928 - recall_m: 0.6866 - precision_m: 0.9475 - val_loss: 0.7257 - val_accuracy: 0.6003 - val_f1_m: 0.7250 - val_recall_m: 0.6448 - val_precision_m: 0.8311\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.5085 - accuracy: 0.7104 - f1_m: 0.7949 - recall_m: 0.6877 - precision_m: 0.9489 - val_loss: 0.7486 - val_accuracy: 0.6495 - val_f1_m: 0.7709 - val_recall_m: 0.7229 - val_precision_m: 0.8283\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.4975 - accuracy: 0.7097 - f1_m: 0.7952 - recall_m: 0.6859 - precision_m: 0.9532 - val_loss: 0.7627 - val_accuracy: 0.6517 - val_f1_m: 0.7727 - val_recall_m: 0.7257 - val_precision_m: 0.8291\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.4855 - accuracy: 0.7184 - f1_m: 0.8032 - recall_m: 0.6973 - precision_m: 0.9575 - val_loss: 0.9699 - val_accuracy: 0.5369 - val_f1_m: 0.6639 - val_recall_m: 0.5595 - val_precision_m: 0.8208\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4723 - accuracy: 0.7195 - f1_m: 0.8032 - recall_m: 0.6945 - precision_m: 0.9579 - val_loss: 1.2107 - val_accuracy: 0.7493 - val_f1_m: 0.8516 - val_recall_m: 0.8812 - val_precision_m: 0.8264\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4758 - accuracy: 0.7308 - f1_m: 0.8148 - recall_m: 0.7129 - precision_m: 0.9582 - val_loss: 1.0856 - val_accuracy: 0.4714 - val_f1_m: 0.5814 - val_recall_m: 0.4519 - val_precision_m: 0.8238\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4612 - accuracy: 0.7281 - f1_m: 0.8117 - recall_m: 0.7063 - precision_m: 0.9611 - val_loss: 1.1228 - val_accuracy: 0.4776 - val_f1_m: 0.5910 - val_recall_m: 0.4622 - val_precision_m: 0.8272\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4430 - accuracy: 0.7356 - f1_m: 0.8130 - recall_m: 0.7055 - precision_m: 0.9658 - val_loss: 0.8876 - val_accuracy: 0.6442 - val_f1_m: 0.7643 - val_recall_m: 0.7086 - val_precision_m: 0.8322\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4296 - accuracy: 0.7436 - f1_m: 0.8236 - recall_m: 0.7214 - precision_m: 0.9656 - val_loss: 1.2471 - val_accuracy: 0.5215 - val_f1_m: 0.6420 - val_recall_m: 0.5265 - val_precision_m: 0.8276\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4233 - accuracy: 0.7445 - f1_m: 0.8180 - recall_m: 0.7171 - precision_m: 0.9580 - val_loss: 0.8791 - val_accuracy: 0.3470 - val_f1_m: 0.3920 - val_recall_m: 0.2582 - val_precision_m: 0.8343\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4123 - accuracy: 0.7371 - f1_m: 0.8074 - recall_m: 0.7009 - precision_m: 0.9699 - val_loss: 0.8645 - val_accuracy: 0.6161 - val_f1_m: 0.7425 - val_recall_m: 0.6769 - val_precision_m: 0.8243\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3986 - accuracy: 0.7495 - f1_m: 0.8192 - recall_m: 0.7152 - precision_m: 0.9706 - val_loss: 0.9485 - val_accuracy: 0.5462 - val_f1_m: 0.6726 - val_recall_m: 0.5707 - val_precision_m: 0.8223\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3888 - accuracy: 0.7478 - f1_m: 0.8231 - recall_m: 0.7175 - precision_m: 0.9712 - val_loss: 1.5445 - val_accuracy: 0.5013 - val_f1_m: 0.6201 - val_recall_m: 0.4978 - val_precision_m: 0.8280\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3785 - accuracy: 0.7526 - f1_m: 0.8224 - recall_m: 0.7167 - precision_m: 0.9748 - val_loss: 1.0198 - val_accuracy: 0.6038 - val_f1_m: 0.7278 - val_recall_m: 0.6498 - val_precision_m: 0.8300\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3725 - accuracy: 0.7610 - f1_m: 0.8368 - recall_m: 0.7378 - precision_m: 0.9722 - val_loss: 1.7862 - val_accuracy: 0.4807 - val_f1_m: 0.5925 - val_recall_m: 0.4630 - val_precision_m: 0.8285\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3650 - accuracy: 0.7592 - f1_m: 0.8303 - recall_m: 0.7259 - precision_m: 0.9756 - val_loss: 0.9267 - val_accuracy: 0.5431 - val_f1_m: 0.6635 - val_recall_m: 0.5526 - val_precision_m: 0.8339\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3646 - accuracy: 0.7584 - f1_m: 0.8321 - recall_m: 0.7278 - precision_m: 0.9770 - val_loss: 1.4219 - val_accuracy: 0.6152 - val_f1_m: 0.7376 - val_recall_m: 0.6621 - val_precision_m: 0.8348\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3512 - accuracy: 0.7673 - f1_m: 0.8331 - recall_m: 0.7323 - precision_m: 0.9753 - val_loss: 1.1240 - val_accuracy: 0.5383 - val_f1_m: 0.6636 - val_recall_m: 0.5589 - val_precision_m: 0.8190\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3430 - accuracy: 0.7772 - f1_m: 0.8470 - recall_m: 0.7521 - precision_m: 0.9738 - val_loss: 1.4501 - val_accuracy: 0.6275 - val_f1_m: 0.7484 - val_recall_m: 0.6799 - val_precision_m: 0.8342\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3421 - accuracy: 0.7839 - f1_m: 0.8519 - recall_m: 0.7568 - precision_m: 0.9789 - val_loss: 1.2804 - val_accuracy: 0.5660 - val_f1_m: 0.6896 - val_recall_m: 0.5934 - val_precision_m: 0.8261\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3304 - accuracy: 0.7802 - f1_m: 0.8489 - recall_m: 0.7512 - precision_m: 0.9796 - val_loss: 1.5886 - val_accuracy: 0.6201 - val_f1_m: 0.7429 - val_recall_m: 0.6748 - val_precision_m: 0.8292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3264 - accuracy: 0.7833 - f1_m: 0.8494 - recall_m: 0.7516 - precision_m: 0.9805 - val_loss: 1.3735 - val_accuracy: 0.6161 - val_f1_m: 0.7394 - val_recall_m: 0.6687 - val_precision_m: 0.8290\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.3112 - accuracy: 0.7998 - f1_m: 0.8668 - recall_m: 0.7815 - precision_m: 0.9768 - val_loss: 1.7958 - val_accuracy: 0.6170 - val_f1_m: 0.7402 - val_recall_m: 0.6708 - val_precision_m: 0.8282\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3098 - accuracy: 0.7936 - f1_m: 0.8567 - recall_m: 0.7642 - precision_m: 0.9796 - val_loss: 1.1687 - val_accuracy: 0.6091 - val_f1_m: 0.7360 - val_recall_m: 0.6694 - val_precision_m: 0.8197\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3030 - accuracy: 0.8066 - f1_m: 0.8690 - recall_m: 0.7841 - precision_m: 0.9777 - val_loss: 1.4710 - val_accuracy: 0.5906 - val_f1_m: 0.7167 - val_recall_m: 0.6341 - val_precision_m: 0.8268\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2980 - accuracy: 0.8148 - f1_m: 0.8727 - recall_m: 0.7874 - precision_m: 0.9828 - val_loss: 1.5641 - val_accuracy: 0.6354 - val_f1_m: 0.7586 - val_recall_m: 0.7015 - val_precision_m: 0.8281\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2879 - accuracy: 0.8279 - f1_m: 0.8871 - recall_m: 0.8137 - precision_m: 0.9789 - val_loss: 2.5879 - val_accuracy: 0.5299 - val_f1_m: 0.6521 - val_recall_m: 0.5387 - val_precision_m: 0.8295\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2846 - accuracy: 0.8286 - f1_m: 0.8861 - recall_m: 0.8104 - precision_m: 0.9799 - val_loss: 1.5131 - val_accuracy: 0.5325 - val_f1_m: 0.6572 - val_recall_m: 0.5472 - val_precision_m: 0.8256\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2869 - accuracy: 0.8307 - f1_m: 0.8895 - recall_m: 0.8178 - precision_m: 0.9782 - val_loss: 2.1051 - val_accuracy: 0.6007 - val_f1_m: 0.7259 - val_recall_m: 0.6486 - val_precision_m: 0.8270\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2692 - accuracy: 0.8385 - f1_m: 0.8930 - recall_m: 0.8229 - precision_m: 0.9784 - val_loss: 1.7696 - val_accuracy: 0.6135 - val_f1_m: 0.7374 - val_recall_m: 0.6663 - val_precision_m: 0.8282\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2620 - accuracy: 0.8562 - f1_m: 0.9056 - recall_m: 0.8435 - precision_m: 0.9799 - val_loss: 1.8914 - val_accuracy: 0.6047 - val_f1_m: 0.7306 - val_recall_m: 0.6571 - val_precision_m: 0.8249\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2623 - accuracy: 0.8507 - f1_m: 0.9039 - recall_m: 0.8406 - precision_m: 0.9796 - val_loss: 3.0638 - val_accuracy: 0.4437 - val_f1_m: 0.5495 - val_recall_m: 0.4157 - val_precision_m: 0.8173\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2902 - accuracy: 0.8375 - f1_m: 0.8941 - recall_m: 0.8240 - precision_m: 0.9799 - val_loss: 1.8630 - val_accuracy: 0.5937 - val_f1_m: 0.7203 - val_recall_m: 0.6419 - val_precision_m: 0.8231\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2550 - accuracy: 0.8605 - f1_m: 0.9084 - recall_m: 0.8495 - precision_m: 0.9783 - val_loss: 1.8856 - val_accuracy: 0.6614 - val_f1_m: 0.7821 - val_recall_m: 0.7460 - val_precision_m: 0.8242\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2741 - accuracy: 0.8669 - f1_m: 0.9152 - recall_m: 0.8600 - precision_m: 0.9791 - val_loss: 2.2351 - val_accuracy: 0.5501 - val_f1_m: 0.6765 - val_recall_m: 0.5756 - val_precision_m: 0.8242\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2364 - accuracy: 0.8716 - f1_m: 0.9183 - recall_m: 0.8638 - precision_m: 0.9819 - val_loss: 1.8471 - val_accuracy: 0.5998 - val_f1_m: 0.7269 - val_recall_m: 0.6553 - val_precision_m: 0.8185\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2236 - accuracy: 0.8841 - f1_m: 0.9242 - recall_m: 0.8744 - precision_m: 0.9820 - val_loss: 3.0842 - val_accuracy: 0.6302 - val_f1_m: 0.7545 - val_recall_m: 0.6976 - val_precision_m: 0.8232\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2302 - accuracy: 0.8797 - f1_m: 0.9211 - recall_m: 0.8679 - precision_m: 0.9830 - val_loss: 2.2933 - val_accuracy: 0.6513 - val_f1_m: 0.7742 - val_recall_m: 0.7319 - val_precision_m: 0.8234\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2601 - accuracy: 0.8764 - f1_m: 0.9137 - recall_m: 0.8633 - precision_m: 0.9722 - val_loss: 1.3301 - val_accuracy: 0.5972 - val_f1_m: 0.7235 - val_recall_m: 0.6465 - val_precision_m: 0.8254\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2271 - accuracy: 0.8762 - f1_m: 0.9216 - recall_m: 0.8704 - precision_m: 0.9808 - val_loss: 3.5747 - val_accuracy: 0.5730 - val_f1_m: 0.6990 - val_recall_m: 0.6090 - val_precision_m: 0.8245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2802854efc8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reasoning model\n",
    "reasoning_model = models_nn.create_nn_model()\n",
    "reasoning_model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', models_nn.f1_m, models_nn.recall_m, models_nn.precision_m])\n",
    "reasoning_model.fit(X_train,\n",
    "                    y_train_reasoning,\n",
    "                    batch_size=128,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_test, y_test_reasoning), \n",
    "#                     callbacks=[models_nn.early_stop],\n",
    "                    class_weight=reasoning_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5049931642649803, 0.508144581178289, 0.4773028825520252, None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_pred = reasoning_model.predict(X_train)\n",
    "reasoning_pred_test = reasoning_model.predict(X_test)\n",
    "\n",
    "reasoning_pred_test_round = reasoning_pred_test.round()\n",
    "precision_recall_fscore_support(y_test_reasoning, reasoning_pred_test_round, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.6931 - accuracy: 0.5455 - f1_m: 0.5977 - recall_m: 0.5684 - precision_m: 0.7751 - val_loss: 0.6502 - val_accuracy: 0.8443 - val_f1_m: 0.9147 - val_recall_m: 1.0000 - val_precision_m: 0.8436\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.6874 - accuracy: 0.6028 - f1_m: 0.7044 - recall_m: 0.6374 - precision_m: 0.8610 - val_loss: 0.6740 - val_accuracy: 0.5383 - val_f1_m: 0.6626 - val_recall_m: 0.5409 - val_precision_m: 0.8598\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.6623 - accuracy: 0.5763 - f1_m: 0.6832 - recall_m: 0.5624 - precision_m: 0.8972 - val_loss: 0.6194 - val_accuracy: 0.6165 - val_f1_m: 0.7431 - val_recall_m: 0.6624 - val_precision_m: 0.8484\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.6415 - accuracy: 0.5793 - f1_m: 0.6855 - recall_m: 0.5574 - precision_m: 0.9117 - val_loss: 0.5584 - val_accuracy: 0.7062 - val_f1_m: 0.8192 - val_recall_m: 0.7955 - val_precision_m: 0.8466\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.6282 - accuracy: 0.6014 - f1_m: 0.6943 - recall_m: 0.5700 - precision_m: 0.9064 - val_loss: 0.6334 - val_accuracy: 0.6029 - val_f1_m: 0.7315 - val_recall_m: 0.6450 - val_precision_m: 0.8472\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.6052 - accuracy: 0.6064 - f1_m: 0.7038 - recall_m: 0.5731 - precision_m: 0.9340 - val_loss: 0.6019 - val_accuracy: 0.7010 - val_f1_m: 0.8144 - val_recall_m: 0.7845 - val_precision_m: 0.8490\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.5900 - accuracy: 0.6258 - f1_m: 0.7253 - recall_m: 0.5969 - precision_m: 0.9341 - val_loss: 0.5948 - val_accuracy: 0.6777 - val_f1_m: 0.7967 - val_recall_m: 0.7533 - val_precision_m: 0.8474\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.5715 - accuracy: 0.6331 - f1_m: 0.7335 - recall_m: 0.6088 - precision_m: 0.9403 - val_loss: 0.7890 - val_accuracy: 0.4842 - val_f1_m: 0.6070 - val_recall_m: 0.4740 - val_precision_m: 0.8501\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.5539 - accuracy: 0.6498 - f1_m: 0.7450 - recall_m: 0.6238 - precision_m: 0.9433 - val_loss: 0.8531 - val_accuracy: 0.4037 - val_f1_m: 0.5002 - val_recall_m: 0.3558 - val_precision_m: 0.8564\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.5357 - accuracy: 0.6502 - f1_m: 0.7451 - recall_m: 0.6177 - precision_m: 0.9518 - val_loss: 0.7063 - val_accuracy: 0.5668 - val_f1_m: 0.6972 - val_recall_m: 0.5947 - val_precision_m: 0.8465\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.5232 - accuracy: 0.6650 - f1_m: 0.7542 - recall_m: 0.6312 - precision_m: 0.9473 - val_loss: 0.8875 - val_accuracy: 0.4692 - val_f1_m: 0.5868 - val_recall_m: 0.4490 - val_precision_m: 0.8535\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.5250 - accuracy: 0.6697 - f1_m: 0.7563 - recall_m: 0.6294 - precision_m: 0.9596 - val_loss: 0.7164 - val_accuracy: 0.6003 - val_f1_m: 0.7290 - val_recall_m: 0.6423 - val_precision_m: 0.8453\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.5003 - accuracy: 0.6816 - f1_m: 0.7741 - recall_m: 0.6540 - precision_m: 0.9604 - val_loss: 0.9880 - val_accuracy: 0.3465 - val_f1_m: 0.4140 - val_recall_m: 0.2752 - val_precision_m: 0.8529\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4971 - accuracy: 0.6786 - f1_m: 0.7684 - recall_m: 0.6433 - precision_m: 0.9624 - val_loss: 0.9282 - val_accuracy: 0.4859 - val_f1_m: 0.6064 - val_recall_m: 0.4717 - val_precision_m: 0.8545\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4770 - accuracy: 0.6876 - f1_m: 0.7735 - recall_m: 0.6484 - precision_m: 0.9684 - val_loss: 0.8184 - val_accuracy: 0.5840 - val_f1_m: 0.7124 - val_recall_m: 0.6152 - val_precision_m: 0.8491\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4677 - accuracy: 0.6909 - f1_m: 0.7811 - recall_m: 0.6611 - precision_m: 0.9658 - val_loss: 0.8381 - val_accuracy: 0.6249 - val_f1_m: 0.7519 - val_recall_m: 0.6780 - val_precision_m: 0.8466\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4632 - accuracy: 0.7077 - f1_m: 0.7947 - recall_m: 0.6765 - precision_m: 0.9717 - val_loss: 1.0498 - val_accuracy: 0.4934 - val_f1_m: 0.6157 - val_recall_m: 0.4831 - val_precision_m: 0.8554\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.4499 - accuracy: 0.7111 - f1_m: 0.7941 - recall_m: 0.6759 - precision_m: 0.9703 - val_loss: 0.8498 - val_accuracy: 0.5972 - val_f1_m: 0.7261 - val_recall_m: 0.6374 - val_precision_m: 0.8459\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4406 - accuracy: 0.7134 - f1_m: 0.7954 - recall_m: 0.6779 - precision_m: 0.9702 - val_loss: 0.7845 - val_accuracy: 0.6341 - val_f1_m: 0.7595 - val_recall_m: 0.6901 - val_precision_m: 0.8470\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4250 - accuracy: 0.7247 - f1_m: 0.8014 - recall_m: 0.6873 - precision_m: 0.9717 - val_loss: 0.8405 - val_accuracy: 0.6192 - val_f1_m: 0.7451 - val_recall_m: 0.6630 - val_precision_m: 0.8531\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4217 - accuracy: 0.7309 - f1_m: 0.8104 - recall_m: 0.6991 - precision_m: 0.9688 - val_loss: 1.0866 - val_accuracy: 0.5796 - val_f1_m: 0.7069 - val_recall_m: 0.6054 - val_precision_m: 0.8515\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4100 - accuracy: 0.7297 - f1_m: 0.8077 - recall_m: 0.6921 - precision_m: 0.9771 - val_loss: 1.1503 - val_accuracy: 0.5281 - val_f1_m: 0.6517 - val_recall_m: 0.5260 - val_precision_m: 0.8594\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4004 - accuracy: 0.7455 - f1_m: 0.8230 - recall_m: 0.7138 - precision_m: 0.9763 - val_loss: 1.1135 - val_accuracy: 0.6069 - val_f1_m: 0.7353 - val_recall_m: 0.6509 - val_precision_m: 0.8470\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3969 - accuracy: 0.7447 - f1_m: 0.8165 - recall_m: 0.7091 - precision_m: 0.9738 - val_loss: 1.0630 - val_accuracy: 0.7471 - val_f1_m: 0.8500 - val_recall_m: 0.8570 - val_precision_m: 0.8445\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3881 - accuracy: 0.7567 - f1_m: 0.8210 - recall_m: 0.7214 - precision_m: 0.9588 - val_loss: 1.3619 - val_accuracy: 0.6372 - val_f1_m: 0.7639 - val_recall_m: 0.7000 - val_precision_m: 0.8430\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.3817 - accuracy: 0.7542 - f1_m: 0.8334 - recall_m: 0.7314 - precision_m: 0.9733 - val_loss: 1.4998 - val_accuracy: 0.6737 - val_f1_m: 0.7933 - val_recall_m: 0.7476 - val_precision_m: 0.8467\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3677 - accuracy: 0.7727 - f1_m: 0.8477 - recall_m: 0.7517 - precision_m: 0.9764 - val_loss: 1.1672 - val_accuracy: 0.5580 - val_f1_m: 0.6877 - val_recall_m: 0.5798 - val_precision_m: 0.8484\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3605 - accuracy: 0.7747 - f1_m: 0.8471 - recall_m: 0.7507 - precision_m: 0.9751 - val_loss: 1.2488 - val_accuracy: 0.6662 - val_f1_m: 0.7871 - val_recall_m: 0.7358 - val_precision_m: 0.8479\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3653 - accuracy: 0.7809 - f1_m: 0.8545 - recall_m: 0.7626 - precision_m: 0.9752 - val_loss: 1.2756 - val_accuracy: 0.6192 - val_f1_m: 0.7469 - val_recall_m: 0.6701 - val_precision_m: 0.8458\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3732 - accuracy: 0.7701 - f1_m: 0.8458 - recall_m: 0.7494 - precision_m: 0.9750 - val_loss: 1.4264 - val_accuracy: 0.5572 - val_f1_m: 0.6856 - val_recall_m: 0.5765 - val_precision_m: 0.8492\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3535 - accuracy: 0.7802 - f1_m: 0.8487 - recall_m: 0.7543 - precision_m: 0.9753 - val_loss: 1.5456 - val_accuracy: 0.5642 - val_f1_m: 0.6936 - val_recall_m: 0.5879 - val_precision_m: 0.8482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3415 - accuracy: 0.7899 - f1_m: 0.8577 - recall_m: 0.7675 - precision_m: 0.9765 - val_loss: 1.6535 - val_accuracy: 0.6095 - val_f1_m: 0.7372 - val_recall_m: 0.6540 - val_precision_m: 0.8470\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.4013 - accuracy: 0.8049 - f1_m: 0.8715 - recall_m: 0.7877 - precision_m: 0.9785 - val_loss: 1.8700 - val_accuracy: 0.5858 - val_f1_m: 0.7140 - val_recall_m: 0.6168 - val_precision_m: 0.8498\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3145 - accuracy: 0.8055 - f1_m: 0.8707 - recall_m: 0.7852 - precision_m: 0.9794 - val_loss: 1.4936 - val_accuracy: 0.6909 - val_f1_m: 0.8081 - val_recall_m: 0.7763 - val_precision_m: 0.8449\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.3329 - accuracy: 0.8033 - f1_m: 0.8690 - recall_m: 0.7880 - precision_m: 0.9728 - val_loss: 1.6442 - val_accuracy: 0.6530 - val_f1_m: 0.7759 - val_recall_m: 0.7172 - val_precision_m: 0.8479\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3290 - accuracy: 0.8131 - f1_m: 0.8764 - recall_m: 0.7972 - precision_m: 0.9759 - val_loss: 1.5259 - val_accuracy: 0.6552 - val_f1_m: 0.7802 - val_recall_m: 0.7288 - val_precision_m: 0.8412\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3253 - accuracy: 0.8091 - f1_m: 0.8751 - recall_m: 0.7961 - precision_m: 0.9749 - val_loss: 1.8876 - val_accuracy: 0.4815 - val_f1_m: 0.6024 - val_recall_m: 0.4685 - val_precision_m: 0.8500\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3144 - accuracy: 0.8156 - f1_m: 0.8731 - recall_m: 0.7930 - precision_m: 0.9777 - val_loss: 1.3039 - val_accuracy: 0.6469 - val_f1_m: 0.7709 - val_recall_m: 0.7100 - val_precision_m: 0.8467\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3144 - accuracy: 0.8272 - f1_m: 0.8889 - recall_m: 0.8193 - precision_m: 0.9740 - val_loss: 1.4010 - val_accuracy: 0.6099 - val_f1_m: 0.7383 - val_recall_m: 0.6564 - val_precision_m: 0.8476\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3249 - accuracy: 0.8260 - f1_m: 0.8861 - recall_m: 0.8204 - precision_m: 0.9714 - val_loss: 1.7160 - val_accuracy: 0.6192 - val_f1_m: 0.7460 - val_recall_m: 0.6683 - val_precision_m: 0.8477\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2924 - accuracy: 0.8312 - f1_m: 0.8884 - recall_m: 0.8159 - precision_m: 0.9771 - val_loss: 1.8625 - val_accuracy: 0.6025 - val_f1_m: 0.7299 - val_recall_m: 0.6434 - val_precision_m: 0.8478\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.3002 - accuracy: 0.8338 - f1_m: 0.8932 - recall_m: 0.8258 - precision_m: 0.9754 - val_loss: 2.0455 - val_accuracy: 0.5598 - val_f1_m: 0.6892 - val_recall_m: 0.5832 - val_precision_m: 0.8470\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2893 - accuracy: 0.8390 - f1_m: 0.8967 - recall_m: 0.8306 - precision_m: 0.9772 - val_loss: 2.0798 - val_accuracy: 0.6526 - val_f1_m: 0.7764 - val_recall_m: 0.7201 - val_precision_m: 0.8455\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 1.1998 - accuracy: 0.7631 - f1_m: 0.8000 - recall_m: 0.7384 - precision_m: 0.9543 - val_loss: 5.0339 - val_accuracy: 0.1557 - val_f1_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 1.4128 - accuracy: 0.6428 - f1_m: 0.7293 - recall_m: 0.6888 - precision_m: 0.8581 - val_loss: 0.8889 - val_accuracy: 0.6324 - val_f1_m: 0.7606 - val_recall_m: 0.6969 - val_precision_m: 0.8398\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.4060 - accuracy: 0.7854 - f1_m: 0.8598 - recall_m: 0.7804 - precision_m: 0.9602 - val_loss: 1.7045 - val_accuracy: 0.6121 - val_f1_m: 0.7401 - val_recall_m: 0.6603 - val_precision_m: 0.8456\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.2945 - accuracy: 0.8338 - f1_m: 0.8931 - recall_m: 0.8235 - precision_m: 0.9779 - val_loss: 2.3258 - val_accuracy: 0.6486 - val_f1_m: 0.7729 - val_recall_m: 0.7129 - val_precision_m: 0.8461\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2747 - accuracy: 0.8480 - f1_m: 0.9031 - recall_m: 0.8393 - precision_m: 0.9789 - val_loss: 1.9459 - val_accuracy: 0.6504 - val_f1_m: 0.7736 - val_recall_m: 0.7135 - val_precision_m: 0.8475\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2959 - accuracy: 0.8513 - f1_m: 0.9056 - recall_m: 0.8462 - precision_m: 0.9762 - val_loss: 1.9097 - val_accuracy: 0.6275 - val_f1_m: 0.7541 - val_recall_m: 0.6812 - val_precision_m: 0.8473\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.2697 - accuracy: 0.8562 - f1_m: 0.9071 - recall_m: 0.8458 - precision_m: 0.9795 - val_loss: 2.0387 - val_accuracy: 0.6781 - val_f1_m: 0.7970 - val_recall_m: 0.7547 - val_precision_m: 0.8468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2802e71e248>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Share Information model\n",
    "shareinfo_model = models_nn.create_nn_model()\n",
    "shareinfo_model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', models_nn.f1_m, models_nn.recall_m, models_nn.precision_m])\n",
    "shareinfo_model.fit(X_train,\n",
    "                    y_train_share_information,\n",
    "                    batch_size=128,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_test, y_test_share_information), \n",
    "#                     callbacks=[models_nn.early_stop],\n",
    "                    class_weight=share_info_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5061310190369541, 0.5086996822033898, 0.5004807432614918, None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shareinfo_pred = shareinfo_model.predict(X_train)\n",
    "shareinfo_pred_test = shareinfo_model.predict(X_test)\n",
    "\n",
    "shareinfo_pred_test_round = shareinfo_pred_test.round()\n",
    "precision_recall_fscore_support(y_test_share_information, shareinfo_pred_test_round, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.6966 - accuracy: 0.7837 - f1_m: 0.8474 - recall_m: 0.8160 - precision_m: 0.9363 - val_loss: 0.6344 - val_accuracy: 0.9507 - val_f1_m: 0.9747 - val_recall_m: 1.0000 - val_precision_m: 0.9514\n",
      "Epoch 2/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.6864 - accuracy: 0.7758 - f1_m: 0.8477 - recall_m: 0.7988 - precision_m: 0.9537 - val_loss: 0.6484 - val_accuracy: 0.6623 - val_f1_m: 0.7921 - val_recall_m: 0.6803 - val_precision_m: 0.9573\n",
      "Epoch 3/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.6641 - accuracy: 0.6147 - f1_m: 0.7286 - recall_m: 0.6160 - precision_m: 0.9648 - val_loss: 0.5944 - val_accuracy: 0.6570 - val_f1_m: 0.7869 - val_recall_m: 0.6715 - val_precision_m: 0.9607\n",
      "Epoch 4/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.6488 - accuracy: 0.6509 - f1_m: 0.7685 - recall_m: 0.6514 - precision_m: 0.9738 - val_loss: 0.5214 - val_accuracy: 0.7471 - val_f1_m: 0.8524 - val_recall_m: 0.7727 - val_precision_m: 0.9571\n",
      "Epoch 5/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.6290 - accuracy: 0.6395 - f1_m: 0.7568 - recall_m: 0.6396 - precision_m: 0.9740 - val_loss: 0.6504 - val_accuracy: 0.5866 - val_f1_m: 0.7317 - val_recall_m: 0.5991 - val_precision_m: 0.9538\n",
      "Epoch 6/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.6230 - accuracy: 0.6585 - f1_m: 0.7751 - recall_m: 0.6586 - precision_m: 0.9764 - val_loss: 0.4732 - val_accuracy: 0.7555 - val_f1_m: 0.8587 - val_recall_m: 0.7842 - val_precision_m: 0.9549\n",
      "Epoch 7/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.5962 - accuracy: 0.6729 - f1_m: 0.7854 - recall_m: 0.6704 - precision_m: 0.9796 - val_loss: 0.5328 - val_accuracy: 0.7045 - val_f1_m: 0.8239 - val_recall_m: 0.7286 - val_precision_m: 0.9552\n",
      "Epoch 8/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.5837 - accuracy: 0.6761 - f1_m: 0.7878 - recall_m: 0.6742 - precision_m: 0.9798 - val_loss: 0.5545 - val_accuracy: 0.6741 - val_f1_m: 0.7977 - val_recall_m: 0.6916 - val_precision_m: 0.9522\n",
      "Epoch 9/50\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5646 - accuracy: 0.6978 - f1_m: 0.8045 - recall_m: 0.6949 - precision_m: 0.9820 - val_loss: 0.6630 - val_accuracy: 0.5748 - val_f1_m: 0.7202 - val_recall_m: 0.5864 - val_precision_m: 0.9461\n",
      "Epoch 10/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.5627 - accuracy: 0.6917 - f1_m: 0.8010 - recall_m: 0.6880 - precision_m: 0.9818 - val_loss: 0.6370 - val_accuracy: 0.5655 - val_f1_m: 0.7118 - val_recall_m: 0.5746 - val_precision_m: 0.9485\n",
      "Epoch 11/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.5461 - accuracy: 0.6931 - f1_m: 0.7994 - recall_m: 0.6878 - precision_m: 0.9794 - val_loss: 0.5353 - val_accuracy: 0.7511 - val_f1_m: 0.8560 - val_recall_m: 0.7818 - val_precision_m: 0.9511\n",
      "Epoch 12/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.5463 - accuracy: 0.7202 - f1_m: 0.8232 - recall_m: 0.7202 - precision_m: 0.9811 - val_loss: 0.8220 - val_accuracy: 0.5026 - val_f1_m: 0.6540 - val_recall_m: 0.5041 - val_precision_m: 0.9498\n",
      "Epoch 13/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.5401 - accuracy: 0.7140 - f1_m: 0.8169 - recall_m: 0.7113 - precision_m: 0.9834 - val_loss: 0.4897 - val_accuracy: 0.7405 - val_f1_m: 0.8491 - val_recall_m: 0.7704 - val_precision_m: 0.9519\n",
      "Epoch 14/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.5213 - accuracy: 0.7223 - f1_m: 0.8223 - recall_m: 0.7185 - precision_m: 0.9859 - val_loss: 0.7557 - val_accuracy: 0.4534 - val_f1_m: 0.6046 - val_recall_m: 0.4481 - val_precision_m: 0.9537\n",
      "Epoch 15/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.5047 - accuracy: 0.7364 - f1_m: 0.8352 - recall_m: 0.7343 - precision_m: 0.9844 - val_loss: 0.8418 - val_accuracy: 0.4688 - val_f1_m: 0.6202 - val_recall_m: 0.4665 - val_precision_m: 0.9510\n",
      "Epoch 16/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.5171 - accuracy: 0.7375 - f1_m: 0.8328 - recall_m: 0.7350 - precision_m: 0.9868 - val_loss: 0.4572 - val_accuracy: 0.7621 - val_f1_m: 0.8637 - val_recall_m: 0.7960 - val_precision_m: 0.9494\n",
      "Epoch 17/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4871 - accuracy: 0.7476 - f1_m: 0.8441 - recall_m: 0.7463 - precision_m: 0.9846 - val_loss: 0.7178 - val_accuracy: 0.5246 - val_f1_m: 0.6735 - val_recall_m: 0.5286 - val_precision_m: 0.9504\n",
      "Epoch 18/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4716 - accuracy: 0.7553 - f1_m: 0.8482 - recall_m: 0.7517 - precision_m: 0.9877 - val_loss: 0.5434 - val_accuracy: 0.6974 - val_f1_m: 0.8145 - val_recall_m: 0.7178 - val_precision_m: 0.9504\n",
      "Epoch 19/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4558 - accuracy: 0.7552 - f1_m: 0.8498 - recall_m: 0.7526 - precision_m: 0.9882 - val_loss: 0.6096 - val_accuracy: 0.6363 - val_f1_m: 0.7687 - val_recall_m: 0.6495 - val_precision_m: 0.9529\n",
      "Epoch 20/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4516 - accuracy: 0.7751 - f1_m: 0.8638 - recall_m: 0.7733 - precision_m: 0.9878 - val_loss: 0.6013 - val_accuracy: 0.6513 - val_f1_m: 0.7850 - val_recall_m: 0.6749 - val_precision_m: 0.9481\n",
      "Epoch 21/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.6146 - accuracy: 0.7385 - f1_m: 0.8264 - recall_m: 0.7439 - precision_m: 0.9780 - val_loss: 0.5877 - val_accuracy: 0.7212 - val_f1_m: 0.8361 - val_recall_m: 0.7503 - val_precision_m: 0.9505\n",
      "Epoch 22/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4482 - accuracy: 0.7723 - f1_m: 0.8622 - recall_m: 0.7706 - precision_m: 0.9891 - val_loss: 0.5287 - val_accuracy: 0.7256 - val_f1_m: 0.8388 - val_recall_m: 0.7553 - val_precision_m: 0.9504\n",
      "Epoch 23/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4079 - accuracy: 0.7976 - f1_m: 0.8797 - recall_m: 0.7960 - precision_m: 0.9906 - val_loss: 0.5317 - val_accuracy: 0.7441 - val_f1_m: 0.8468 - val_recall_m: 0.7686 - val_precision_m: 0.9508\n",
      "Epoch 24/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4128 - accuracy: 0.8007 - f1_m: 0.8813 - recall_m: 0.7995 - precision_m: 0.9895 - val_loss: 0.5694 - val_accuracy: 0.6970 - val_f1_m: 0.8133 - val_recall_m: 0.7154 - val_precision_m: 0.9519\n",
      "Epoch 25/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4019 - accuracy: 0.7999 - f1_m: 0.8802 - recall_m: 0.7978 - precision_m: 0.9897 - val_loss: 0.6321 - val_accuracy: 0.6887 - val_f1_m: 0.8081 - val_recall_m: 0.7065 - val_precision_m: 0.9525\n",
      "Epoch 26/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3781 - accuracy: 0.8083 - f1_m: 0.8865 - recall_m: 0.8066 - precision_m: 0.9900 - val_loss: 0.7213 - val_accuracy: 0.5844 - val_f1_m: 0.7258 - val_recall_m: 0.5909 - val_precision_m: 0.9559\n",
      "Epoch 27/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3773 - accuracy: 0.8096 - f1_m: 0.8866 - recall_m: 0.8071 - precision_m: 0.9912 - val_loss: 0.7287 - val_accuracy: 0.5589 - val_f1_m: 0.7049 - val_recall_m: 0.5650 - val_precision_m: 0.9506\n",
      "Epoch 28/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3755 - accuracy: 0.8097 - f1_m: 0.8872 - recall_m: 0.8077 - precision_m: 0.9904 - val_loss: 0.8373 - val_accuracy: 0.5919 - val_f1_m: 0.7232 - val_recall_m: 0.5929 - val_precision_m: 0.9394\n",
      "Epoch 29/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3464 - accuracy: 0.8217 - f1_m: 0.8952 - recall_m: 0.8187 - precision_m: 0.9926 - val_loss: 0.7971 - val_accuracy: 0.6033 - val_f1_m: 0.7419 - val_recall_m: 0.6142 - val_precision_m: 0.9505\n",
      "Epoch 30/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3406 - accuracy: 0.8363 - f1_m: 0.9034 - recall_m: 0.8329 - precision_m: 0.9925 - val_loss: 0.8959 - val_accuracy: 0.5910 - val_f1_m: 0.7233 - val_recall_m: 0.5943 - val_precision_m: 0.9360\n",
      "Epoch 31/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3221 - accuracy: 0.8395 - f1_m: 0.9061 - recall_m: 0.8376 - precision_m: 0.9927 - val_loss: 0.7011 - val_accuracy: 0.6535 - val_f1_m: 0.7807 - val_recall_m: 0.6673 - val_precision_m: 0.9519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3174 - accuracy: 0.8400 - f1_m: 0.9065 - recall_m: 0.8371 - precision_m: 0.9934 - val_loss: 0.7462 - val_accuracy: 0.7023 - val_f1_m: 0.8173 - val_recall_m: 0.7220 - val_precision_m: 0.9517\n",
      "Epoch 33/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3177 - accuracy: 0.8444 - f1_m: 0.9097 - recall_m: 0.8426 - precision_m: 0.9935 - val_loss: 0.7479 - val_accuracy: 0.6966 - val_f1_m: 0.8131 - val_recall_m: 0.7152 - val_precision_m: 0.9511\n",
      "Epoch 34/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2931 - accuracy: 0.8613 - f1_m: 0.9204 - recall_m: 0.8592 - precision_m: 0.9947 - val_loss: 0.7738 - val_accuracy: 0.7133 - val_f1_m: 0.8253 - val_recall_m: 0.7357 - val_precision_m: 0.9487\n",
      "Epoch 35/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2925 - accuracy: 0.8622 - f1_m: 0.9212 - recall_m: 0.8621 - precision_m: 0.9928 - val_loss: 0.7457 - val_accuracy: 0.6961 - val_f1_m: 0.8125 - val_recall_m: 0.7148 - val_precision_m: 0.9510\n",
      "Epoch 36/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2789 - accuracy: 0.8690 - f1_m: 0.9249 - recall_m: 0.8682 - precision_m: 0.9939 - val_loss: 0.8005 - val_accuracy: 0.7058 - val_f1_m: 0.8110 - val_recall_m: 0.7168 - val_precision_m: 0.9408\n",
      "Epoch 37/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2766 - accuracy: 0.8735 - f1_m: 0.9274 - recall_m: 0.8723 - precision_m: 0.9940 - val_loss: 0.7836 - val_accuracy: 0.7392 - val_f1_m: 0.8435 - val_recall_m: 0.7636 - val_precision_m: 0.9491\n",
      "Epoch 38/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2597 - accuracy: 0.8770 - f1_m: 0.9288 - recall_m: 0.8745 - precision_m: 0.9946 - val_loss: 0.9008 - val_accuracy: 0.6693 - val_f1_m: 0.7855 - val_recall_m: 0.6813 - val_precision_m: 0.9357\n",
      "Epoch 39/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2623 - accuracy: 0.8749 - f1_m: 0.9283 - recall_m: 0.8735 - precision_m: 0.9944 - val_loss: 0.9361 - val_accuracy: 0.6060 - val_f1_m: 0.7362 - val_recall_m: 0.6128 - val_precision_m: 0.9343\n",
      "Epoch 40/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2630 - accuracy: 0.8938 - f1_m: 0.9400 - recall_m: 0.8931 - precision_m: 0.9950 - val_loss: 0.9336 - val_accuracy: 0.6733 - val_f1_m: 0.7871 - val_recall_m: 0.6825 - val_precision_m: 0.9390\n",
      "Epoch 41/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2325 - accuracy: 0.8922 - f1_m: 0.9391 - recall_m: 0.8912 - precision_m: 0.9956 - val_loss: 1.1125 - val_accuracy: 0.6957 - val_f1_m: 0.8127 - val_recall_m: 0.7166 - val_precision_m: 0.9496\n",
      "Epoch 42/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2433 - accuracy: 0.9053 - f1_m: 0.9471 - recall_m: 0.9051 - precision_m: 0.9955 - val_loss: 0.8853 - val_accuracy: 0.7252 - val_f1_m: 0.8335 - val_recall_m: 0.7472 - val_precision_m: 0.9518\n",
      "Epoch 43/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2205 - accuracy: 0.9053 - f1_m: 0.9468 - recall_m: 0.9042 - precision_m: 0.9961 - val_loss: 0.8423 - val_accuracy: 0.7634 - val_f1_m: 0.8598 - val_recall_m: 0.7912 - val_precision_m: 0.9491\n",
      "Epoch 44/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2342 - accuracy: 0.9098 - f1_m: 0.9491 - recall_m: 0.9090 - precision_m: 0.9954 - val_loss: 0.8803 - val_accuracy: 0.7128 - val_f1_m: 0.8257 - val_recall_m: 0.7375 - val_precision_m: 0.9474\n",
      "Epoch 45/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2113 - accuracy: 0.9148 - f1_m: 0.9524 - recall_m: 0.9140 - precision_m: 0.9962 - val_loss: 0.8712 - val_accuracy: 0.7520 - val_f1_m: 0.8519 - val_recall_m: 0.7766 - val_precision_m: 0.9514\n",
      "Epoch 46/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.1924 - accuracy: 0.9195 - f1_m: 0.9551 - recall_m: 0.9190 - precision_m: 0.9961 - val_loss: 1.1290 - val_accuracy: 0.6772 - val_f1_m: 0.7997 - val_recall_m: 0.6953 - val_precision_m: 0.9502\n",
      "Epoch 47/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.2004 - accuracy: 0.9159 - f1_m: 0.9524 - recall_m: 0.9141 - precision_m: 0.9963 - val_loss: 0.8344 - val_accuracy: 0.7867 - val_f1_m: 0.8751 - val_recall_m: 0.8155 - val_precision_m: 0.9509\n",
      "Epoch 48/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.1865 - accuracy: 0.9264 - f1_m: 0.9590 - recall_m: 0.9255 - precision_m: 0.9967 - val_loss: 1.0280 - val_accuracy: 0.7476 - val_f1_m: 0.8492 - val_recall_m: 0.7730 - val_precision_m: 0.9501\n",
      "Epoch 49/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.1987 - accuracy: 0.9293 - f1_m: 0.9608 - recall_m: 0.9294 - precision_m: 0.9961 - val_loss: 1.3207 - val_accuracy: 0.6135 - val_f1_m: 0.7408 - val_recall_m: 0.6181 - val_precision_m: 0.9370\n",
      "Epoch 50/50\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.1826 - accuracy: 0.9308 - f1_m: 0.9616 - recall_m: 0.9303 - precision_m: 0.9970 - val_loss: 1.1228 - val_accuracy: 0.6948 - val_f1_m: 0.8122 - val_recall_m: 0.7157 - val_precision_m: 0.9497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2803386d608>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deception model\n",
    "deception_model = models_nn.create_nn_model()\n",
    "deception_model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', models_nn.f1_m, models_nn.recall_m, models_nn.precision_m])\n",
    "deception_model.fit(X_train,\n",
    "                    y_train_deception,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_test, y_test_deception), \n",
    "#                     callbacks=[models_nn.early_stop],\n",
    "                    class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4962712020295698, 0.48392691951896394, 0.44596945760926804, None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deception_pred = deception_model.predict(X_train)\n",
    "deception_pred_test = deception_model.predict(X_test)\n",
    "deception_pred_test_round = deception_pred_test.round()\n",
    "precision_recall_fscore_support(y_test_deception, deception_pred_test_round, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deception_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique, counts = np.unique(deception_pred_test_round, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deception_model.history.history['val_f1_m']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train encodings\n",
    "pred_df_arr_full = []\n",
    "pred_df_arr = []\n",
    "for i in range(0, len(gamemove_pred)):\n",
    "    pred_obj_1 = {}\n",
    "    pred_obj_1['gamemove'] = gamemove_pred[i][0]\n",
    "    pred_obj_1['reasoning'] = reasoning_pred[i][0]\n",
    "    pred_obj_1['shareinfo'] = shareinfo_pred[i][0]\n",
    "    pred_df_arr.append(pred_obj_1)\n",
    "    \n",
    "    pred_obj_2 = pred_obj_1.copy()\n",
    "    pred_obj_2['rapport'] = rapport_pred[i][0]\n",
    "    pred_df_arr_full.append(pred_obj_2)\n",
    "    \n",
    "pred_df_full = pd.DataFrame(pred_df_arr_full)\n",
    "pred_df = pd.DataFrame(pred_df_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encodings\n",
    "pred_test_df_arr_full = []\n",
    "pred_test_df_arr = []\n",
    "\n",
    "for i in range(0, len(gamemove_pred_test)):\n",
    "    pred_obj_1 = {}\n",
    "    pred_obj_1['gamemove'] = gamemove_pred_test[i][0]\n",
    "    pred_obj_1['reasoning'] = reasoning_pred_test[i][0]\n",
    "    pred_obj_1['shareinfo'] = shareinfo_pred_test[i][0]\n",
    "    pred_test_df_arr.append(pred_obj_1)\n",
    "    \n",
    "    pred_obj_2 = pred_obj_1.copy()\n",
    "    pred_obj_2['rapport'] = rapport_pred_test[i][0]\n",
    "    pred_test_df_arr_full.append(pred_obj_2)\n",
    "    \n",
    "pred_test_df_full = pd.DataFrame(pred_test_df_arr_full)\n",
    "pred_test_df = pd.DataFrame(pred_test_df_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model with one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting deception\n",
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6935 - acc: 0.6189 - f1_m: 0.6562 - precision_m: 0.6809 - recall_m: 0.6345 - val_loss: 0.6965 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6934 - acc: 0.8785 - f1_m: 0.9318 - precision_m: 0.9444 - recall_m: 0.9204 - val_loss: 0.6926 - val_acc: 0.8681 - val_f1_m: 0.9278 - val_precision_m: 0.9496 - val_recall_m: 0.9086\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.8929 - f1_m: 0.9431 - precision_m: 0.9512 - recall_m: 0.9361 - val_loss: 0.6929 - val_acc: 0.8716 - val_f1_m: 0.9301 - val_precision_m: 0.9494 - val_recall_m: 0.9131\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.4328 - f1_m: 0.4385 - precision_m: 0.4433 - recall_m: 0.4343 - val_loss: 0.6940 - val_acc: 0.8690 - val_f1_m: 0.9283 - val_precision_m: 0.9492 - val_recall_m: 0.9099\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6933 - acc: 0.8946 - f1_m: 0.9420 - precision_m: 0.9514 - recall_m: 0.9348 - val_loss: 0.6907 - val_acc: 0.8764 - val_f1_m: 0.9330 - val_precision_m: 0.9497 - val_recall_m: 0.9185\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6933 - acc: 0.8962 - f1_m: 0.9428 - precision_m: 0.9490 - recall_m: 0.9374 - val_loss: 0.6910 - val_acc: 0.8777 - val_f1_m: 0.9338 - val_precision_m: 0.9494 - val_recall_m: 0.9204\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.1627 - f1_m: 0.1262 - precision_m: 0.1256 - recall_m: 0.1267 - val_loss: 0.6947 - val_acc: 0.8725 - val_f1_m: 0.9305 - val_precision_m: 0.9494 - val_recall_m: 0.9140\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6934 - acc: 0.9029 - f1_m: 0.9488 - precision_m: 0.9512 - recall_m: 0.9473 - val_loss: 0.6919 - val_acc: 0.8830 - val_f1_m: 0.9368 - val_precision_m: 0.9497 - val_recall_m: 0.9259\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.5279 - f1_m: 0.5368 - precision_m: 0.5382 - recall_m: 0.5359 - val_loss: 0.6925 - val_acc: 0.8795 - val_f1_m: 0.9348 - val_precision_m: 0.9499 - val_recall_m: 0.9218\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.9038 - f1_m: 0.9493 - precision_m: 0.9512 - recall_m: 0.9480 - val_loss: 0.6912 - val_acc: 0.8791 - val_f1_m: 0.9346 - val_precision_m: 0.9499 - val_recall_m: 0.9213\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8903 - f1_m: 0.9408 - precision_m: 0.9495 - recall_m: 0.9333 - val_loss: 0.6928 - val_acc: 0.8742 - val_f1_m: 0.9315 - val_precision_m: 0.9496 - val_recall_m: 0.9158\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8730 - f1_m: 0.9257 - precision_m: 0.9376 - recall_m: 0.9154 - val_loss: 0.6939 - val_acc: 0.8760 - val_f1_m: 0.9326 - val_precision_m: 0.9497 - val_recall_m: 0.9177\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.1022 - f1_m: 0.0664 - precision_m: 0.0662 - recall_m: 0.0665 - val_loss: 0.6943 - val_acc: 0.8703 - val_f1_m: 0.9293 - val_precision_m: 0.9501 - val_recall_m: 0.9108\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8142 - f1_m: 0.8602 - precision_m: 0.8709 - recall_m: 0.8503 - val_loss: 0.6939 - val_acc: 0.8712 - val_f1_m: 0.9298 - val_precision_m: 0.9502 - val_recall_m: 0.9117\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6932 - acc: 0.8844 - f1_m: 0.9384 - precision_m: 0.9515 - recall_m: 0.9267 - val_loss: 0.6929 - val_acc: 0.8751 - val_f1_m: 0.9321 - val_precision_m: 0.9500 - val_recall_m: 0.9163\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6932 - acc: 0.4955 - f1_m: 0.5068 - precision_m: 0.5094 - recall_m: 0.5047 - val_loss: 0.6928 - val_acc: 0.8791 - val_f1_m: 0.9347 - val_precision_m: 0.9507 - val_recall_m: 0.9204\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.9049 - f1_m: 0.9489 - precision_m: 0.9498 - recall_m: 0.9491 - val_loss: 0.6921 - val_acc: 0.8830 - val_f1_m: 0.9369 - val_precision_m: 0.9510 - val_recall_m: 0.9246\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.2645 - f1_m: 0.2425 - precision_m: 0.2441 - recall_m: 0.2411 - val_loss: 0.6954 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8733 - f1_m: 0.9224 - precision_m: 0.9311 - recall_m: 0.9146 - val_loss: 0.6937 - val_acc: 0.8676 - val_f1_m: 0.9280 - val_precision_m: 0.9509 - val_recall_m: 0.9076\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.8826 - f1_m: 0.9374 - precision_m: 0.9516 - recall_m: 0.9246 - val_loss: 0.6931 - val_acc: 0.8755 - val_f1_m: 0.9326 - val_precision_m: 0.9514 - val_recall_m: 0.9158\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6593 - f1_m: 0.6890 - precision_m: 0.6972 - recall_m: 0.6818 - val_loss: 0.6917 - val_acc: 0.8808 - val_f1_m: 0.9356 - val_precision_m: 0.9517 - val_recall_m: 0.9213\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.8929 - f1_m: 0.9431 - precision_m: 0.9517 - recall_m: 0.9355 - val_loss: 0.6926 - val_acc: 0.8764 - val_f1_m: 0.9331 - val_precision_m: 0.9515 - val_recall_m: 0.9167\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.8907 - f1_m: 0.9418 - precision_m: 0.9514 - recall_m: 0.9332 - val_loss: 0.6926 - val_acc: 0.8725 - val_f1_m: 0.9308 - val_precision_m: 0.9512 - val_recall_m: 0.9126\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.8820 - f1_m: 0.9347 - precision_m: 0.9491 - recall_m: 0.9217 - val_loss: 0.6927 - val_acc: 0.8760 - val_f1_m: 0.9329 - val_precision_m: 0.9514 - val_recall_m: 0.9163\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.6116 - f1_m: 0.6257 - precision_m: 0.6316 - recall_m: 0.6206 - val_loss: 0.6950 - val_acc: 0.0493 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.8674 - f1_m: 0.9220 - precision_m: 0.9378 - recall_m: 0.9078 - val_loss: 0.6936 - val_acc: 0.8751 - val_f1_m: 0.9324 - val_precision_m: 0.9514 - val_recall_m: 0.9154\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.9010 - f1_m: 0.9477 - precision_m: 0.9519 - recall_m: 0.9444 - val_loss: 0.6936 - val_acc: 0.8742 - val_f1_m: 0.9318 - val_precision_m: 0.9513 - val_recall_m: 0.9144\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.8909 - f1_m: 0.9418 - precision_m: 0.9518 - recall_m: 0.9333 - val_loss: 0.6942 - val_acc: 0.8478 - val_f1_m: 0.9163 - val_precision_m: 0.9500 - val_recall_m: 0.8863\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8564 - f1_m: 0.9220 - precision_m: 0.9512 - recall_m: 0.8957 - val_loss: 0.6931 - val_acc: 0.8536 - val_f1_m: 0.9198 - val_precision_m: 0.9499 - val_recall_m: 0.8927\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.8854 - f1_m: 0.9390 - precision_m: 0.9516 - recall_m: 0.9275 - val_loss: 0.6923 - val_acc: 0.8716 - val_f1_m: 0.9303 - val_precision_m: 0.9511 - val_recall_m: 0.9116\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6932 - acc: 0.8784 - f1_m: 0.9347 - precision_m: 0.9513 - recall_m: 0.9198 - val_loss: 0.6907 - val_acc: 0.8764 - val_f1_m: 0.9331 - val_precision_m: 0.9514 - val_recall_m: 0.9167\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6931 - acc: 0.8950 - f1_m: 0.9443 - precision_m: 0.9514 - recall_m: 0.9380 - val_loss: 0.6921 - val_acc: 0.8773 - val_f1_m: 0.9336 - val_precision_m: 0.9515 - val_recall_m: 0.9177\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_test_df_full,y_test_deception), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.50487990559933, 0.5079415554380865, 0.5036981351698717, None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_test_df_full)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [ 189 2085]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting rapport\n",
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6954 - acc: 0.8047 - f1_m: 0.8868 - precision_m: 0.8733 - recall_m: 0.9081 - val_loss: 0.6825 - val_acc: 0.6693 - val_f1_m: 0.7932 - val_precision_m: 0.8632 - val_recall_m: 0.7360\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6924 - acc: 0.7934 - f1_m: 0.8794 - precision_m: 0.8843 - recall_m: 0.8767 - val_loss: 0.6796 - val_acc: 0.6772 - val_f1_m: 0.7990 - val_precision_m: 0.8656 - val_recall_m: 0.7444\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6889 - acc: 0.8025 - f1_m: 0.8850 - precision_m: 0.8874 - recall_m: 0.8848 - val_loss: 0.6652 - val_acc: 0.7164 - val_f1_m: 0.8288 - val_precision_m: 0.8661 - val_recall_m: 0.7967\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6835 - acc: 0.8230 - f1_m: 0.8986 - precision_m: 0.8870 - recall_m: 0.9125 - val_loss: 0.6537 - val_acc: 0.7203 - val_f1_m: 0.8318 - val_precision_m: 0.8666 - val_recall_m: 0.8020\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6766 - acc: 0.8249 - f1_m: 0.8975 - precision_m: 0.8873 - recall_m: 0.9111 - val_loss: 0.6382 - val_acc: 0.7221 - val_f1_m: 0.8331 - val_precision_m: 0.8685 - val_recall_m: 0.8026\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6693 - acc: 0.8180 - f1_m: 0.8949 - precision_m: 0.8898 - recall_m: 0.9019 - val_loss: 0.6377 - val_acc: 0.7040 - val_f1_m: 0.8195 - val_precision_m: 0.8695 - val_recall_m: 0.7773\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6620 - acc: 0.8099 - f1_m: 0.8883 - precision_m: 0.8908 - recall_m: 0.8877 - val_loss: 0.6376 - val_acc: 0.6931 - val_f1_m: 0.8112 - val_precision_m: 0.8698 - val_recall_m: 0.7623\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6553 - acc: 0.7904 - f1_m: 0.8744 - precision_m: 0.9018 - recall_m: 0.8506 - val_loss: 0.6371 - val_acc: 0.6702 - val_f1_m: 0.7937 - val_precision_m: 0.8692 - val_recall_m: 0.7330\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6487 - acc: 0.7954 - f1_m: 0.8765 - precision_m: 0.9039 - recall_m: 0.8526 - val_loss: 0.6486 - val_acc: 0.6412 - val_f1_m: 0.7691 - val_precision_m: 0.8700 - val_recall_m: 0.6919\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6426 - acc: 0.7655 - f1_m: 0.8543 - precision_m: 0.9126 - recall_m: 0.8058 - val_loss: 0.6391 - val_acc: 0.6425 - val_f1_m: 0.7711 - val_precision_m: 0.8684 - val_recall_m: 0.6964\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6369 - acc: 0.7655 - f1_m: 0.8553 - precision_m: 0.9130 - recall_m: 0.8066 - val_loss: 0.6428 - val_acc: 0.6288 - val_f1_m: 0.7599 - val_precision_m: 0.8677 - val_recall_m: 0.6791\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6321 - acc: 0.7643 - f1_m: 0.8515 - precision_m: 0.9102 - recall_m: 0.8019 - val_loss: 0.6599 - val_acc: 0.6073 - val_f1_m: 0.7404 - val_precision_m: 0.8684 - val_recall_m: 0.6485\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6278 - acc: 0.7302 - f1_m: 0.8270 - precision_m: 0.9208 - recall_m: 0.7533 - val_loss: 0.6539 - val_acc: 0.6148 - val_f1_m: 0.7466 - val_precision_m: 0.8692 - val_recall_m: 0.6574\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6240 - acc: 0.7455 - f1_m: 0.8369 - precision_m: 0.9187 - recall_m: 0.7716 - val_loss: 0.6686 - val_acc: 0.5633 - val_f1_m: 0.7002 - val_precision_m: 0.8691 - val_recall_m: 0.5898\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6207 - acc: 0.7261 - f1_m: 0.8213 - precision_m: 0.9232 - recall_m: 0.7429 - val_loss: 0.6753 - val_acc: 0.5572 - val_f1_m: 0.6935 - val_precision_m: 0.8701 - val_recall_m: 0.5797\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6179 - acc: 0.7205 - f1_m: 0.8183 - precision_m: 0.9227 - recall_m: 0.7377 - val_loss: 0.6820 - val_acc: 0.5519 - val_f1_m: 0.6886 - val_precision_m: 0.8695 - val_recall_m: 0.5732\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6155 - acc: 0.7124 - f1_m: 0.8119 - precision_m: 0.9253 - recall_m: 0.7254 - val_loss: 0.6886 - val_acc: 0.5449 - val_f1_m: 0.6821 - val_precision_m: 0.8686 - val_recall_m: 0.5649\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6136 - acc: 0.7169 - f1_m: 0.8158 - precision_m: 0.9252 - recall_m: 0.7331 - val_loss: 0.6931 - val_acc: 0.5440 - val_f1_m: 0.6810 - val_precision_m: 0.8688 - val_recall_m: 0.5633\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6120 - acc: 0.7063 - f1_m: 0.8039 - precision_m: 0.9251 - recall_m: 0.7137 - val_loss: 0.6939 - val_acc: 0.5453 - val_f1_m: 0.6822 - val_precision_m: 0.8692 - val_recall_m: 0.5649\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6108 - acc: 0.7121 - f1_m: 0.8110 - precision_m: 0.9258 - recall_m: 0.7239 - val_loss: 0.6996 - val_acc: 0.5422 - val_f1_m: 0.6793 - val_precision_m: 0.8684 - val_recall_m: 0.5614\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6097 - acc: 0.7083 - f1_m: 0.8079 - precision_m: 0.9256 - recall_m: 0.7194 - val_loss: 0.7109 - val_acc: 0.5356 - val_f1_m: 0.6729 - val_precision_m: 0.8677 - val_recall_m: 0.5529\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6086 - acc: 0.7009 - f1_m: 0.8027 - precision_m: 0.9260 - recall_m: 0.7114 - val_loss: 0.6972 - val_acc: 0.5471 - val_f1_m: 0.6837 - val_precision_m: 0.8695 - val_recall_m: 0.5668\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6079 - acc: 0.7092 - f1_m: 0.8093 - precision_m: 0.9257 - recall_m: 0.7215 - val_loss: 0.7164 - val_acc: 0.5347 - val_f1_m: 0.6723 - val_precision_m: 0.8680 - val_recall_m: 0.5520\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6071 - acc: 0.7034 - f1_m: 0.8028 - precision_m: 0.9238 - recall_m: 0.7122 - val_loss: 0.7147 - val_acc: 0.5378 - val_f1_m: 0.6751 - val_precision_m: 0.8687 - val_recall_m: 0.5555\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6067 - acc: 0.6985 - f1_m: 0.7988 - precision_m: 0.9253 - recall_m: 0.7056 - val_loss: 0.7243 - val_acc: 0.5295 - val_f1_m: 0.6669 - val_precision_m: 0.8665 - val_recall_m: 0.5453\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6062 - acc: 0.6919 - f1_m: 0.7941 - precision_m: 0.9291 - recall_m: 0.6963 - val_loss: 0.7180 - val_acc: 0.5374 - val_f1_m: 0.6746 - val_precision_m: 0.8685 - val_recall_m: 0.5549\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6057 - acc: 0.7020 - f1_m: 0.8026 - precision_m: 0.9269 - recall_m: 0.7106 - val_loss: 0.7273 - val_acc: 0.5255 - val_f1_m: 0.6631 - val_precision_m: 0.8662 - val_recall_m: 0.5404\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6052 - acc: 0.6915 - f1_m: 0.7931 - precision_m: 0.9291 - recall_m: 0.6944 - val_loss: 0.7225 - val_acc: 0.5334 - val_f1_m: 0.6707 - val_precision_m: 0.8681 - val_recall_m: 0.5499\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6050 - acc: 0.6940 - f1_m: 0.7967 - precision_m: 0.9293 - recall_m: 0.7002 - val_loss: 0.7297 - val_acc: 0.5251 - val_f1_m: 0.6632 - val_precision_m: 0.8666 - val_recall_m: 0.5406\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6045 - acc: 0.6936 - f1_m: 0.7964 - precision_m: 0.9299 - recall_m: 0.6993 - val_loss: 0.7291 - val_acc: 0.5255 - val_f1_m: 0.6636 - val_precision_m: 0.8668 - val_recall_m: 0.5411\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6042 - acc: 0.6877 - f1_m: 0.7893 - precision_m: 0.9305 - recall_m: 0.6879 - val_loss: 0.7320 - val_acc: 0.5233 - val_f1_m: 0.6616 - val_precision_m: 0.8663 - val_recall_m: 0.5386\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6042 - acc: 0.6929 - f1_m: 0.7931 - precision_m: 0.9293 - recall_m: 0.6944 - val_loss: 0.7327 - val_acc: 0.5237 - val_f1_m: 0.6618 - val_precision_m: 0.8671 - val_recall_m: 0.5386\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_test_df,y_test_rapport), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4953197945845005, 0.4896370468253872, 0.4278434547657409, None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_test_df)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [1050 1224]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted against Throughput, WorkTime, PC Agreement & Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train weighted encodings\n",
    "pred_df_full_throughput, pred_df_throughput, pred_df_full_worktime, pred_df_worktime, pred_df_full_agreement, pred_df_agreement, pred_df_full_textlength, pred_df_textlength, pred_df_full_special, pred_df_special = metadata_options.construct_weighted_dataframe(indices_train, df_throughput, df_worktime, df_agreement, df_textlength, df_special, pred_df, pred_df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weighted encodings\n",
    "pred_df_full_throughput_test, pred_df_throughput_test, pred_df_full_worktime_test, pred_df_worktime_test, pred_df_full_agreement_test, pred_df_agreement_test, pred_df_full_textlength_test, pred_df_textlength_test, pred_df_full_special_test, pred_df_special_test = metadata_options.construct_weighted_dataframe(indices_test, df_throughput, df_worktime, df_agreement, df_textlength, df_special, pred_test_df, pred_test_df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model with one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting deception, weighted by throughput\n",
      "Model: \"functional_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6932 - acc: 0.9412 - f1_m: 0.9690 - precision_m: 0.9513 - recall_m: 0.9896 - val_loss: 0.6858 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.8912 - f1_m: 0.9393 - precision_m: 0.9522 - recall_m: 0.9342 - val_loss: 0.6837 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.9001 - f1_m: 0.9443 - precision_m: 0.9521 - recall_m: 0.9421 - val_loss: 0.6868 - val_acc: 0.7366 - val_f1_m: 0.8451 - val_precision_m: 0.9494 - val_recall_m: 0.7632\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6929 - acc: 0.7544 - f1_m: 0.8570 - precision_m: 0.9531 - recall_m: 0.7806 - val_loss: 0.6867 - val_acc: 0.7216 - val_f1_m: 0.8350 - val_precision_m: 0.9483 - val_recall_m: 0.7476\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6929 - acc: 0.6720 - f1_m: 0.7882 - precision_m: 0.9510 - recall_m: 0.6907 - val_loss: 0.6860 - val_acc: 0.7238 - val_f1_m: 0.8366 - val_precision_m: 0.9480 - val_recall_m: 0.7504\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6928 - acc: 0.6722 - f1_m: 0.7929 - precision_m: 0.9530 - recall_m: 0.6884 - val_loss: 0.6889 - val_acc: 0.6469 - val_f1_m: 0.7798 - val_precision_m: 0.9486 - val_recall_m: 0.6641\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6928 - acc: 0.4288 - f1_m: 0.5579 - precision_m: 0.9513 - recall_m: 0.4217 - val_loss: 0.6855 - val_acc: 0.7168 - val_f1_m: 0.8316 - val_precision_m: 0.9481 - val_recall_m: 0.7423\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6927 - acc: 0.6689 - f1_m: 0.7935 - precision_m: 0.9538 - recall_m: 0.6857 - val_loss: 0.6853 - val_acc: 0.7067 - val_f1_m: 0.8242 - val_precision_m: 0.9487 - val_recall_m: 0.7303\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6926 - acc: 0.7339 - f1_m: 0.8419 - precision_m: 0.9516 - recall_m: 0.7569 - val_loss: 0.6863 - val_acc: 0.6790 - val_f1_m: 0.8043 - val_precision_m: 0.9473 - val_recall_m: 0.7004\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6927 - acc: 0.4049 - f1_m: 0.5195 - precision_m: 0.9535 - recall_m: 0.3984 - val_loss: 0.6871 - val_acc: 0.6574 - val_f1_m: 0.7878 - val_precision_m: 0.9485 - val_recall_m: 0.6758\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6925 - acc: 0.5142 - f1_m: 0.6597 - precision_m: 0.9552 - recall_m: 0.5146 - val_loss: 0.6886 - val_acc: 0.6099 - val_f1_m: 0.7502 - val_precision_m: 0.9494 - val_recall_m: 0.6221\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6924 - acc: 0.5701 - f1_m: 0.7097 - precision_m: 0.9527 - recall_m: 0.5778 - val_loss: 0.6873 - val_acc: 0.6376 - val_f1_m: 0.7729 - val_precision_m: 0.9489 - val_recall_m: 0.6538\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6924 - acc: 0.7173 - f1_m: 0.8311 - precision_m: 0.9536 - recall_m: 0.7391 - val_loss: 0.6850 - val_acc: 0.6658 - val_f1_m: 0.7943 - val_precision_m: 0.9486 - val_recall_m: 0.6849\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6924 - acc: 0.6975 - f1_m: 0.8155 - precision_m: 0.9545 - recall_m: 0.7159 - val_loss: 0.6865 - val_acc: 0.6469 - val_f1_m: 0.7797 - val_precision_m: 0.9495 - val_recall_m: 0.6634\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6923 - acc: 0.7058 - f1_m: 0.8190 - precision_m: 0.9536 - recall_m: 0.7234 - val_loss: 0.6830 - val_acc: 0.6777 - val_f1_m: 0.8035 - val_precision_m: 0.9474 - val_recall_m: 0.6991\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6923 - acc: 0.5825 - f1_m: 0.7153 - precision_m: 0.9547 - recall_m: 0.5927 - val_loss: 0.6826 - val_acc: 0.6759 - val_f1_m: 0.8021 - val_precision_m: 0.9479 - val_recall_m: 0.6968\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6922 - acc: 0.7099 - f1_m: 0.8269 - precision_m: 0.9535 - recall_m: 0.7334 - val_loss: 0.6846 - val_acc: 0.6557 - val_f1_m: 0.7865 - val_precision_m: 0.9483 - val_recall_m: 0.6735\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6922 - acc: 0.5632 - f1_m: 0.6973 - precision_m: 0.9524 - recall_m: 0.5679 - val_loss: 0.6900 - val_acc: 0.4459 - val_f1_m: 0.5989 - val_precision_m: 0.9478 - val_recall_m: 0.4407\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6921 - acc: 0.4075 - f1_m: 0.5507 - precision_m: 0.9509 - recall_m: 0.3923 - val_loss: 0.6960 - val_acc: 0.3751 - val_f1_m: 0.5185 - val_precision_m: 0.9512 - val_recall_m: 0.3592\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6920 - acc: 0.3323 - f1_m: 0.4640 - precision_m: 0.9596 - recall_m: 0.3111 - val_loss: 0.6886 - val_acc: 0.4507 - val_f1_m: 0.6043 - val_precision_m: 0.9450 - val_recall_m: 0.4470\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6918 - acc: 0.5285 - f1_m: 0.6765 - precision_m: 0.9546 - recall_m: 0.5310 - val_loss: 0.6890 - val_acc: 0.4472 - val_f1_m: 0.6011 - val_precision_m: 0.9447 - val_recall_m: 0.4434\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6920 - acc: 0.6322 - f1_m: 0.7581 - precision_m: 0.9535 - recall_m: 0.6448 - val_loss: 0.6921 - val_acc: 0.4182 - val_f1_m: 0.5687 - val_precision_m: 0.9493 - val_recall_m: 0.4085\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6917 - acc: 0.5207 - f1_m: 0.6695 - precision_m: 0.9544 - recall_m: 0.5221 - val_loss: 0.6879 - val_acc: 0.4661 - val_f1_m: 0.6205 - val_precision_m: 0.9457 - val_recall_m: 0.4644\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6918 - acc: 0.3715 - f1_m: 0.5115 - precision_m: 0.9587 - recall_m: 0.3563 - val_loss: 0.6908 - val_acc: 0.4323 - val_f1_m: 0.5836 - val_precision_m: 0.9496 - val_recall_m: 0.4239\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6916 - acc: 0.4222 - f1_m: 0.5707 - precision_m: 0.9585 - recall_m: 0.4109 - val_loss: 0.6849 - val_acc: 0.5167 - val_f1_m: 0.6687 - val_precision_m: 0.9474 - val_recall_m: 0.5193\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6915 - acc: 0.4347 - f1_m: 0.5846 - precision_m: 0.9568 - recall_m: 0.4290 - val_loss: 0.6786 - val_acc: 0.6104 - val_f1_m: 0.7504 - val_precision_m: 0.9490 - val_recall_m: 0.6221\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6914 - acc: 0.5285 - f1_m: 0.6782 - precision_m: 0.9560 - recall_m: 0.5314 - val_loss: 0.6840 - val_acc: 0.5295 - val_f1_m: 0.6805 - val_precision_m: 0.9482 - val_recall_m: 0.5330\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6914 - acc: 0.4667 - f1_m: 0.6129 - precision_m: 0.9581 - recall_m: 0.4583 - val_loss: 0.6805 - val_acc: 0.5778 - val_f1_m: 0.7238 - val_precision_m: 0.9474 - val_recall_m: 0.5874\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6913 - acc: 0.5785 - f1_m: 0.7212 - precision_m: 0.9545 - recall_m: 0.5848 - val_loss: 0.6856 - val_acc: 0.5018 - val_f1_m: 0.6538 - val_precision_m: 0.9497 - val_recall_m: 0.5010\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6912 - acc: 0.4423 - f1_m: 0.5896 - precision_m: 0.9576 - recall_m: 0.4333 - val_loss: 0.6808 - val_acc: 0.5550 - val_f1_m: 0.7036 - val_precision_m: 0.9488 - val_recall_m: 0.5614\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6914 - acc: 0.5972 - f1_m: 0.7304 - precision_m: 0.9556 - recall_m: 0.6041 - val_loss: 0.6871 - val_acc: 0.4872 - val_f1_m: 0.6401 - val_precision_m: 0.9482 - val_recall_m: 0.4856\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6913 - acc: 0.5015 - f1_m: 0.6520 - precision_m: 0.9564 - recall_m: 0.5006 - val_loss: 0.6898 - val_acc: 0.4560 - val_f1_m: 0.6094 - val_precision_m: 0.9482 - val_recall_m: 0.4513\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by throughput')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_throughput)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_throughput, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_throughput_test,y_test_deception), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49962910010314926, 0.49803835734108626, 0.35093799948360016, None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_full_throughput_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [1247 1027]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rapport = np.asarray(y_test_rapport)\n",
    "y_train_rapport = np.asarray(y_train_rapport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting rapport, weighted by throughput\n",
      "Model: \"functional_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6966 - acc: 0.6605 - f1_m: 0.7734 - precision_m: 0.8634 - recall_m: 0.7191 - val_loss: 0.6951 - val_acc: 0.3773 - val_f1_m: 0.5048 - val_precision_m: 0.8196 - val_recall_m: 0.3673\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6944 - acc: 0.3850 - f1_m: 0.4853 - precision_m: 0.8581 - recall_m: 0.3422 - val_loss: 0.6983 - val_acc: 0.3492 - val_f1_m: 0.4649 - val_precision_m: 0.8130 - val_recall_m: 0.3275\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6938 - acc: 0.4336 - f1_m: 0.5446 - precision_m: 0.8600 - recall_m: 0.4024 - val_loss: 0.6998 - val_acc: 0.3307 - val_f1_m: 0.4373 - val_precision_m: 0.8101 - val_recall_m: 0.3017\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.3744 - f1_m: 0.4648 - precision_m: 0.8597 - recall_m: 0.3245 - val_loss: 0.6952 - val_acc: 0.3580 - val_f1_m: 0.4798 - val_precision_m: 0.8106 - val_recall_m: 0.3430\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6929 - acc: 0.3650 - f1_m: 0.4535 - precision_m: 0.8653 - recall_m: 0.3105 - val_loss: 0.6979 - val_acc: 0.3289 - val_f1_m: 0.4379 - val_precision_m: 0.8022 - val_recall_m: 0.3036\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6925 - acc: 0.4133 - f1_m: 0.5201 - precision_m: 0.8703 - recall_m: 0.3746 - val_loss: 0.6999 - val_acc: 0.3215 - val_f1_m: 0.4272 - val_precision_m: 0.8001 - val_recall_m: 0.2936\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6923 - acc: 0.4133 - f1_m: 0.5189 - precision_m: 0.8712 - recall_m: 0.3747 - val_loss: 0.7015 - val_acc: 0.3083 - val_f1_m: 0.4064 - val_precision_m: 0.7988 - val_recall_m: 0.2747\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6920 - acc: 0.3923 - f1_m: 0.4876 - precision_m: 0.8660 - recall_m: 0.3429 - val_loss: 0.7030 - val_acc: 0.3052 - val_f1_m: 0.4023 - val_precision_m: 0.7984 - val_recall_m: 0.2713\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6918 - acc: 0.4236 - f1_m: 0.5306 - precision_m: 0.8720 - recall_m: 0.3854 - val_loss: 0.6949 - val_acc: 0.3615 - val_f1_m: 0.4853 - val_precision_m: 0.8106 - val_recall_m: 0.3491\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6916 - acc: 0.4693 - f1_m: 0.5829 - precision_m: 0.8692 - recall_m: 0.4439 - val_loss: 0.7028 - val_acc: 0.3100 - val_f1_m: 0.4086 - val_precision_m: 0.7997 - val_recall_m: 0.2766\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6916 - acc: 0.3746 - f1_m: 0.4547 - precision_m: 0.8775 - recall_m: 0.3177 - val_loss: 0.6966 - val_acc: 0.3606 - val_f1_m: 0.4844 - val_precision_m: 0.8100 - val_recall_m: 0.3486\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6913 - acc: 0.4222 - f1_m: 0.5218 - precision_m: 0.8738 - recall_m: 0.3779 - val_loss: 0.7076 - val_acc: 0.2850 - val_f1_m: 0.3716 - val_precision_m: 0.7890 - val_recall_m: 0.2449\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6911 - acc: 0.4244 - f1_m: 0.5289 - precision_m: 0.8841 - recall_m: 0.3820 - val_loss: 0.7023 - val_acc: 0.3303 - val_f1_m: 0.4415 - val_precision_m: 0.8003 - val_recall_m: 0.3075\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6909 - acc: 0.4445 - f1_m: 0.5582 - precision_m: 0.8837 - recall_m: 0.4131 - val_loss: 0.7024 - val_acc: 0.3276 - val_f1_m: 0.4390 - val_precision_m: 0.7986 - val_recall_m: 0.3056\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6907 - acc: 0.4207 - f1_m: 0.5243 - precision_m: 0.8870 - recall_m: 0.3764 - val_loss: 0.7069 - val_acc: 0.3131 - val_f1_m: 0.4163 - val_precision_m: 0.7950 - val_recall_m: 0.2842\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6905 - acc: 0.4546 - f1_m: 0.5629 - precision_m: 0.8788 - recall_m: 0.4183 - val_loss: 0.7020 - val_acc: 0.3443 - val_f1_m: 0.4608 - val_precision_m: 0.8086 - val_recall_m: 0.3252\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6903 - acc: 0.4471 - f1_m: 0.5543 - precision_m: 0.8778 - recall_m: 0.4085 - val_loss: 0.7013 - val_acc: 0.3514 - val_f1_m: 0.4708 - val_precision_m: 0.8104 - val_recall_m: 0.3347\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6902 - acc: 0.4659 - f1_m: 0.5792 - precision_m: 0.8869 - recall_m: 0.4350 - val_loss: 0.7038 - val_acc: 0.3430 - val_f1_m: 0.4584 - val_precision_m: 0.8094 - val_recall_m: 0.3227\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6901 - acc: 0.4594 - f1_m: 0.5738 - precision_m: 0.8870 - recall_m: 0.4274 - val_loss: 0.7030 - val_acc: 0.3527 - val_f1_m: 0.4715 - val_precision_m: 0.8121 - val_recall_m: 0.3351\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6900 - acc: 0.4792 - f1_m: 0.5982 - precision_m: 0.8886 - recall_m: 0.4548 - val_loss: 0.7015 - val_acc: 0.3624 - val_f1_m: 0.4848 - val_precision_m: 0.8115 - val_recall_m: 0.3483\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6899 - acc: 0.4516 - f1_m: 0.5628 - precision_m: 0.8858 - recall_m: 0.4157 - val_loss: 0.6994 - val_acc: 0.3826 - val_f1_m: 0.5103 - val_precision_m: 0.8172 - val_recall_m: 0.3743\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6896 - acc: 0.5068 - f1_m: 0.6262 - precision_m: 0.8861 - recall_m: 0.4882 - val_loss: 0.7027 - val_acc: 0.3637 - val_f1_m: 0.4858 - val_precision_m: 0.8129 - val_recall_m: 0.3494\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6895 - acc: 0.4835 - f1_m: 0.5964 - precision_m: 0.8835 - recall_m: 0.4540 - val_loss: 0.7034 - val_acc: 0.3694 - val_f1_m: 0.4931 - val_precision_m: 0.8157 - val_recall_m: 0.3565\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6895 - acc: 0.4236 - f1_m: 0.5267 - precision_m: 0.8879 - recall_m: 0.3794 - val_loss: 0.6969 - val_acc: 0.4204 - val_f1_m: 0.5548 - val_precision_m: 0.8305 - val_recall_m: 0.4208\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6894 - acc: 0.5037 - f1_m: 0.6205 - precision_m: 0.8920 - recall_m: 0.4804 - val_loss: 0.6907 - val_acc: 0.4543 - val_f1_m: 0.5931 - val_precision_m: 0.8366 - val_recall_m: 0.4642\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6890 - acc: 0.5241 - f1_m: 0.6446 - precision_m: 0.8949 - recall_m: 0.5073 - val_loss: 0.6987 - val_acc: 0.4160 - val_f1_m: 0.5481 - val_precision_m: 0.8325 - val_recall_m: 0.4128\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6888 - acc: 0.5355 - f1_m: 0.6552 - precision_m: 0.8963 - recall_m: 0.5207 - val_loss: 0.7058 - val_acc: 0.3804 - val_f1_m: 0.5054 - val_precision_m: 0.8204 - val_recall_m: 0.3689\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6885 - acc: 0.4904 - f1_m: 0.6062 - precision_m: 0.8949 - recall_m: 0.4631 - val_loss: 0.7006 - val_acc: 0.4151 - val_f1_m: 0.5469 - val_precision_m: 0.8309 - val_recall_m: 0.4118\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6881 - acc: 0.5152 - f1_m: 0.6355 - precision_m: 0.8947 - recall_m: 0.4964 - val_loss: 0.7016 - val_acc: 0.4134 - val_f1_m: 0.5449 - val_precision_m: 0.8302 - val_recall_m: 0.4098\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6880 - acc: 0.5077 - f1_m: 0.6254 - precision_m: 0.8898 - recall_m: 0.4864 - val_loss: 0.6979 - val_acc: 0.4261 - val_f1_m: 0.5600 - val_precision_m: 0.8335 - val_recall_m: 0.4264\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6876 - acc: 0.5197 - f1_m: 0.6386 - precision_m: 0.8944 - recall_m: 0.5009 - val_loss: 0.7056 - val_acc: 0.3980 - val_f1_m: 0.5273 - val_precision_m: 0.8251 - val_recall_m: 0.3921\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6875 - acc: 0.5046 - f1_m: 0.6214 - precision_m: 0.8941 - recall_m: 0.4807 - val_loss: 0.7058 - val_acc: 0.4028 - val_f1_m: 0.5321 - val_precision_m: 0.8282 - val_recall_m: 0.3966\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by throughput')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_throughput)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_throughput, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64,\n",
    "                               validation_data=(pred_df_throughput_test,y_test_rapport), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4670821196884661, 0.42895217880444425, 0.35074195705100975, None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_throughput_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [1337  937]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WorkTime only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model with one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting deception, weighted by worktime\n",
      "Model: \"functional_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6933 - acc: 0.9399 - f1_m: 0.9650 - precision_m: 0.9475 - recall_m: 0.9882 - val_loss: 0.6921 - val_acc: 0.9433 - val_f1_m: 0.9706 - val_precision_m: 0.9503 - val_recall_m: 0.9923\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.1282 - f1_m: 0.0996 - precision_m: 0.5341 - recall_m: 0.0897 - val_loss: 0.6933 - val_acc: 0.0805 - val_f1_m: 0.0652 - val_precision_m: 0.7685 - val_recall_m: 0.0347\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.4610 - f1_m: 0.4603 - precision_m: 0.6909 - recall_m: 0.4654 - val_loss: 0.6926 - val_acc: 0.9472 - val_f1_m: 0.9727 - val_precision_m: 0.9504 - val_recall_m: 0.9963\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.5660 - f1_m: 0.5771 - precision_m: 0.9135 - recall_m: 0.5789 - val_loss: 0.6924 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.7343 - f1_m: 0.7638 - precision_m: 0.9419 - recall_m: 0.7639 - val_loss: 0.6816 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.8688 - f1_m: 0.9020 - precision_m: 0.9524 - recall_m: 0.9122 - val_loss: 0.6884 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.6866 - f1_m: 0.6999 - precision_m: 0.8664 - recall_m: 0.7109 - val_loss: 0.6864 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6929 - acc: 0.6087 - f1_m: 0.6216 - precision_m: 0.8583 - recall_m: 0.6230 - val_loss: 0.6864 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6934 - acc: 0.3656 - f1_m: 0.3715 - precision_m: 0.8151 - recall_m: 0.3582 - val_loss: 0.6728 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6927 - acc: 0.3225 - f1_m: 0.3221 - precision_m: 0.8566 - recall_m: 0.3048 - val_loss: 0.6952 - val_acc: 0.0805 - val_f1_m: 0.0647 - val_precision_m: 0.7722 - val_recall_m: 0.0343\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6926 - acc: 0.0958 - f1_m: 0.0967 - precision_m: 0.9063 - recall_m: 0.0521 - val_loss: 0.6944 - val_acc: 0.0862 - val_f1_m: 0.0754 - val_precision_m: 0.8611 - val_recall_m: 0.0402\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6925 - acc: 0.4179 - f1_m: 0.4336 - precision_m: 0.9337 - recall_m: 0.4102 - val_loss: 0.6921 - val_acc: 0.1284 - val_f1_m: 0.1578 - val_precision_m: 0.9487 - val_recall_m: 0.0871\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6925 - acc: 0.1278 - f1_m: 0.1574 - precision_m: 0.9398 - recall_m: 0.0888 - val_loss: 0.6927 - val_acc: 0.0954 - val_f1_m: 0.0949 - val_precision_m: 0.8988 - val_recall_m: 0.0510\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6924 - acc: 0.2955 - f1_m: 0.3271 - precision_m: 0.8784 - recall_m: 0.2758 - val_loss: 0.6933 - val_acc: 0.0822 - val_f1_m: 0.0679 - val_precision_m: 0.7722 - val_recall_m: 0.0361\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6924 - acc: 0.4787 - f1_m: 0.4926 - precision_m: 0.8919 - recall_m: 0.4790 - val_loss: 0.6927 - val_acc: 0.0818 - val_f1_m: 0.0671 - val_precision_m: 0.7722 - val_recall_m: 0.0356\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6923 - acc: 0.0880 - f1_m: 0.0799 - precision_m: 0.8323 - recall_m: 0.0428 - val_loss: 0.6931 - val_acc: 0.0862 - val_f1_m: 0.0755 - val_precision_m: 0.8333 - val_recall_m: 0.0402\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6923 - acc: 0.4311 - f1_m: 0.4765 - precision_m: 0.9558 - recall_m: 0.4251 - val_loss: 0.6918 - val_acc: 0.0858 - val_f1_m: 0.0745 - val_precision_m: 0.8294 - val_recall_m: 0.0397\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6921 - acc: 0.0882 - f1_m: 0.0764 - precision_m: 0.8938 - recall_m: 0.0432 - val_loss: 0.6913 - val_acc: 0.1275 - val_f1_m: 0.1560 - val_precision_m: 0.9516 - val_recall_m: 0.0862\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6924 - acc: 0.3768 - f1_m: 0.4092 - precision_m: 0.9556 - recall_m: 0.3694 - val_loss: 0.6887 - val_acc: 0.9072 - val_f1_m: 0.9508 - val_precision_m: 0.9509 - val_recall_m: 0.9514\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6923 - acc: 0.1402 - f1_m: 0.1609 - precision_m: 0.9271 - recall_m: 0.1002 - val_loss: 0.6943 - val_acc: 0.0888 - val_f1_m: 0.0801 - val_precision_m: 0.8345 - val_recall_m: 0.0429\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6920 - acc: 0.0822 - f1_m: 0.0687 - precision_m: 0.8737 - recall_m: 0.0361 - val_loss: 0.6904 - val_acc: 0.1020 - val_f1_m: 0.1072 - val_precision_m: 0.9265 - val_recall_m: 0.0578\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6920 - acc: 0.4210 - f1_m: 0.4261 - precision_m: 0.9021 - recall_m: 0.4104 - val_loss: 0.6915 - val_acc: 0.0893 - val_f1_m: 0.0821 - val_precision_m: 0.9104 - val_recall_m: 0.0438\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6921 - acc: 0.0808 - f1_m: 0.0651 - precision_m: 0.8520 - recall_m: 0.0342 - val_loss: 0.6969 - val_acc: 0.0778 - val_f1_m: 0.0597 - val_precision_m: 0.7444 - val_recall_m: 0.0315\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6919 - acc: 0.0851 - f1_m: 0.0740 - precision_m: 0.8772 - recall_m: 0.0391 - val_loss: 0.6942 - val_acc: 0.0783 - val_f1_m: 0.0605 - val_precision_m: 0.7444 - val_recall_m: 0.0320\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6919 - acc: 0.0769 - f1_m: 0.0569 - precision_m: 0.8112 - recall_m: 0.0298 - val_loss: 0.6925 - val_acc: 0.0901 - val_f1_m: 0.0826 - val_precision_m: 0.8623 - val_recall_m: 0.0443\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6919 - acc: 0.0977 - f1_m: 0.0985 - precision_m: 0.9098 - recall_m: 0.0531 - val_loss: 0.6897 - val_acc: 0.1249 - val_f1_m: 0.1498 - val_precision_m: 0.9612 - val_recall_m: 0.0826\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6918 - acc: 0.7350 - f1_m: 0.7653 - precision_m: 0.9477 - recall_m: 0.7574 - val_loss: 0.6908 - val_acc: 0.0888 - val_f1_m: 0.0813 - val_precision_m: 0.9104 - val_recall_m: 0.0434\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6917 - acc: 0.6500 - f1_m: 0.6868 - precision_m: 0.9261 - recall_m: 0.6697 - val_loss: 0.6892 - val_acc: 0.8967 - val_f1_m: 0.9450 - val_precision_m: 0.9503 - val_recall_m: 0.9404\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6917 - acc: 0.4452 - f1_m: 0.4740 - precision_m: 0.9312 - recall_m: 0.4419 - val_loss: 0.6899 - val_acc: 0.1218 - val_f1_m: 0.1450 - val_precision_m: 0.9600 - val_recall_m: 0.0794\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6916 - acc: 0.5288 - f1_m: 0.5453 - precision_m: 0.9294 - recall_m: 0.5367 - val_loss: 0.6901 - val_acc: 0.1179 - val_f1_m: 0.1377 - val_precision_m: 0.9521 - val_recall_m: 0.0752\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6917 - acc: 0.0950 - f1_m: 0.0937 - precision_m: 0.8989 - recall_m: 0.0502 - val_loss: 0.6945 - val_acc: 0.0792 - val_f1_m: 0.0622 - val_precision_m: 0.7722 - val_recall_m: 0.0329\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6915 - acc: 0.0970 - f1_m: 0.0976 - precision_m: 0.9224 - recall_m: 0.0523 - val_loss: 0.6943 - val_acc: 0.0906 - val_f1_m: 0.0851 - val_precision_m: 0.9250 - val_recall_m: 0.0451\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by worktime')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_worktime)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_worktime, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_worktime_test,y_test_deception), \n",
    "#                                callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5052540353157838, 0.5048070569578432, 0.09057168317934564, None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_full_worktime_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [2172  102]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting rapport, weighted by worktime\n",
      "Model: \"functional_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6933 - acc: 0.5630 - f1_m: 0.5805 - precision_m: 0.5612 - recall_m: 0.6036 - val_loss: 0.6935 - val_acc: 0.8426 - val_f1_m: 0.9126 - val_precision_m: 0.8707 - val_recall_m: 0.9603\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8316 - f1_m: 0.9053 - precision_m: 0.8582 - recall_m: 0.9603 - val_loss: 0.6924 - val_acc: 0.8478 - val_f1_m: 0.9158 - val_precision_m: 0.8704 - val_recall_m: 0.9677\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.2044 - f1_m: 0.0829 - precision_m: 0.0791 - recall_m: 0.0870 - val_loss: 0.6944 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.4733 - f1_m: 0.4339 - precision_m: 0.4079 - recall_m: 0.4641 - val_loss: 0.6945 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6229 - f1_m: 0.6326 - precision_m: 0.5955 - recall_m: 0.6760 - val_loss: 0.6931 - val_acc: 0.8558 - val_f1_m: 0.9205 - val_precision_m: 0.8703 - val_recall_m: 0.9781\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.8462 - f1_m: 0.9163 - precision_m: 0.8618 - recall_m: 0.9799 - val_loss: 0.6925 - val_acc: 0.8553 - val_f1_m: 0.9203 - val_precision_m: 0.8703 - val_recall_m: 0.9776\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.8456 - f1_m: 0.9160 - precision_m: 0.8616 - recall_m: 0.9791 - val_loss: 0.6920 - val_acc: 0.8549 - val_f1_m: 0.9201 - val_precision_m: 0.8699 - val_recall_m: 0.9776\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.7162 - f1_m: 0.7519 - precision_m: 0.7056 - recall_m: 0.8072 - val_loss: 0.6909 - val_acc: 0.8558 - val_f1_m: 0.9206 - val_precision_m: 0.8697 - val_recall_m: 0.9791\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.5388 - f1_m: 0.5286 - precision_m: 0.4971 - recall_m: 0.5651 - val_loss: 0.6932 - val_acc: 0.8544 - val_f1_m: 0.9197 - val_precision_m: 0.8705 - val_recall_m: 0.9761\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.4093 - f1_m: 0.3568 - precision_m: 0.3348 - recall_m: 0.3826 - val_loss: 0.6934 - val_acc: 0.8540 - val_f1_m: 0.9195 - val_precision_m: 0.8705 - val_recall_m: 0.9756\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.7582 - f1_m: 0.8000 - precision_m: 0.7507 - recall_m: 0.8574 - val_loss: 0.6941 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6931 - acc: 0.1396 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6939 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.7901 - f1_m: 0.8448 - precision_m: 0.7962 - recall_m: 0.9011 - val_loss: 0.6931 - val_acc: 0.8500 - val_f1_m: 0.9170 - val_precision_m: 0.8720 - val_recall_m: 0.9682\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6145 - f1_m: 0.6276 - precision_m: 0.7121 - recall_m: 0.6649 - val_loss: 0.6910 - val_acc: 0.8571 - val_f1_m: 0.9213 - val_precision_m: 0.8702 - val_recall_m: 0.9800\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6930 - acc: 0.4414 - f1_m: 0.3899 - precision_m: 0.3668 - recall_m: 0.4168 - val_loss: 0.6938 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.6907 - f1_m: 0.7247 - precision_m: 0.7174 - recall_m: 0.7686 - val_loss: 0.6924 - val_acc: 0.8527 - val_f1_m: 0.9186 - val_precision_m: 0.8719 - val_recall_m: 0.9716\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.8422 - f1_m: 0.9138 - precision_m: 0.8627 - recall_m: 0.9728 - val_loss: 0.6930 - val_acc: 0.8443 - val_f1_m: 0.9137 - val_precision_m: 0.8721 - val_recall_m: 0.9607\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.7780 - f1_m: 0.8434 - precision_m: 0.8582 - recall_m: 0.8889 - val_loss: 0.6935 - val_acc: 0.8298 - val_f1_m: 0.9047 - val_precision_m: 0.8719 - val_recall_m: 0.9413\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6930 - acc: 0.1826 - f1_m: 0.0632 - precision_m: 0.0614 - recall_m: 0.0652 - val_loss: 0.6932 - val_acc: 0.8153 - val_f1_m: 0.8954 - val_precision_m: 0.8715 - val_recall_m: 0.9217\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.7091 - f1_m: 0.7578 - precision_m: 0.8542 - recall_m: 0.7917 - val_loss: 0.6926 - val_acc: 0.8237 - val_f1_m: 0.9010 - val_precision_m: 0.8714 - val_recall_m: 0.9338\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.6366 - f1_m: 0.6840 - precision_m: 0.8703 - recall_m: 0.6850 - val_loss: 0.6954 - val_acc: 0.2023 - val_f1_m: 0.1696 - val_precision_m: 0.8920 - val_recall_m: 0.0948\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.5818 - f1_m: 0.5976 - precision_m: 0.8486 - recall_m: 0.6170 - val_loss: 0.6918 - val_acc: 0.8487 - val_f1_m: 0.9164 - val_precision_m: 0.8721 - val_recall_m: 0.9667\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6930 - acc: 0.5810 - f1_m: 0.6066 - precision_m: 0.7464 - recall_m: 0.6093 - val_loss: 0.6961 - val_acc: 0.1974 - val_f1_m: 0.1603 - val_precision_m: 0.8771 - val_recall_m: 0.0893\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6928 - acc: 0.6819 - f1_m: 0.7110 - precision_m: 0.8242 - recall_m: 0.7426 - val_loss: 0.7072 - val_acc: 0.1495 - val_f1_m: 0.0483 - val_precision_m: 0.6704 - val_recall_m: 0.0254\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6929 - acc: 0.3205 - f1_m: 0.2871 - precision_m: 0.7864 - recall_m: 0.2529 - val_loss: 0.6915 - val_acc: 0.8325 - val_f1_m: 0.9063 - val_precision_m: 0.8720 - val_recall_m: 0.9448\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6928 - acc: 0.4924 - f1_m: 0.4936 - precision_m: 0.8397 - recall_m: 0.4846 - val_loss: 0.6967 - val_acc: 0.2080 - val_f1_m: 0.1819 - val_precision_m: 0.8824 - val_recall_m: 0.1027\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6928 - acc: 0.4762 - f1_m: 0.4903 - precision_m: 0.8546 - recall_m: 0.4610 - val_loss: 0.6953 - val_acc: 0.2410 - val_f1_m: 0.2455 - val_precision_m: 0.8862 - val_recall_m: 0.1443\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6928 - acc: 0.7591 - f1_m: 0.8321 - precision_m: 0.8707 - recall_m: 0.8553 - val_loss: 0.6921 - val_acc: 0.5998 - val_f1_m: 0.7254 - val_precision_m: 0.8929 - val_recall_m: 0.6137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6927 - acc: 0.6723 - f1_m: 0.7405 - precision_m: 0.8715 - recall_m: 0.7251 - val_loss: 0.6982 - val_acc: 0.2089 - val_f1_m: 0.1820 - val_precision_m: 0.8886 - val_recall_m: 0.1027\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6926 - acc: 0.3132 - f1_m: 0.3552 - precision_m: 0.8952 - recall_m: 0.2290 - val_loss: 0.6942 - val_acc: 0.2916 - val_f1_m: 0.3386 - val_precision_m: 0.8905 - val_recall_m: 0.2115\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6925 - acc: 0.6194 - f1_m: 0.6562 - precision_m: 0.8616 - recall_m: 0.6575 - val_loss: 0.6982 - val_acc: 0.2208 - val_f1_m: 0.2089 - val_precision_m: 0.8783 - val_recall_m: 0.1199\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6925 - acc: 0.5339 - f1_m: 0.5631 - precision_m: 0.8627 - recall_m: 0.5396 - val_loss: 0.7026 - val_acc: 0.1979 - val_f1_m: 0.1605 - val_precision_m: 0.8938 - val_recall_m: 0.0894\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by worktime')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_worktime)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_worktime, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64,\n",
    "                               validation_data=(pred_df_worktime_test,y_test_rapport), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5127783495792214, 0.5090906271697164, 0.19651624101165385, None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_worktime_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [2075  199]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC Agreement only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model with one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting deception, weighted by PC Agreement\n",
      "Model: \"functional_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6932 - acc: 0.2498 - f1_m: 0.3079 - precision_m: 0.9460 - recall_m: 0.2302 - val_loss: 0.6917 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6869 - f1_m: 0.7434 - precision_m: 0.9545 - recall_m: 0.7050 - val_loss: 0.6921 - val_acc: 0.2806 - val_f1_m: 0.3973 - val_precision_m: 0.9585 - val_recall_m: 0.2536\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.9384 - f1_m: 0.9640 - precision_m: 0.9492 - recall_m: 0.9863 - val_loss: 0.6908 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.1282 - f1_m: 0.1344 - precision_m: 0.9180 - recall_m: 0.0859 - val_loss: 0.6958 - val_acc: 0.1236 - val_f1_m: 0.1501 - val_precision_m: 0.9493 - val_recall_m: 0.0824\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.0950 - f1_m: 0.0971 - precision_m: 0.8729 - recall_m: 0.0523 - val_loss: 0.6973 - val_acc: 0.1310 - val_f1_m: 0.1629 - val_precision_m: 0.9545 - val_recall_m: 0.0901\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.1395 - f1_m: 0.1740 - precision_m: 0.9298 - recall_m: 0.0993 - val_loss: 0.6945 - val_acc: 0.1966 - val_f1_m: 0.2745 - val_precision_m: 0.9590 - val_recall_m: 0.1619\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.2070 - f1_m: 0.2909 - precision_m: 0.9476 - recall_m: 0.1740 - val_loss: 0.6935 - val_acc: 0.1961 - val_f1_m: 0.2740 - val_precision_m: 0.9589 - val_recall_m: 0.1614\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.6403 - f1_m: 0.6869 - precision_m: 0.9448 - recall_m: 0.6538 - val_loss: 0.6933 - val_acc: 0.1970 - val_f1_m: 0.2753 - val_precision_m: 0.9591 - val_recall_m: 0.1623\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.2359 - f1_m: 0.3278 - precision_m: 0.9527 - recall_m: 0.2136 - val_loss: 0.6911 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6931 - acc: 0.6491 - f1_m: 0.7053 - precision_m: 0.9512 - recall_m: 0.6677 - val_loss: 0.6912 - val_acc: 0.3325 - val_f1_m: 0.4658 - val_precision_m: 0.9561 - val_recall_m: 0.3109\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.5582 - f1_m: 0.6222 - precision_m: 0.9444 - recall_m: 0.5637 - val_loss: 0.6922 - val_acc: 0.2718 - val_f1_m: 0.3857 - val_precision_m: 0.9567 - val_recall_m: 0.2445\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.5574 - f1_m: 0.6321 - precision_m: 0.9553 - recall_m: 0.5681 - val_loss: 0.6913 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.6066 - f1_m: 0.6743 - precision_m: 0.9510 - recall_m: 0.6283 - val_loss: 0.6913 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.4930 - f1_m: 0.5501 - precision_m: 0.9541 - recall_m: 0.4987 - val_loss: 0.6903 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.9509 - f1_m: 0.9748 - precision_m: 0.9513 - recall_m: 1.0000 - val_loss: 0.6894 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.7137 - f1_m: 0.7848 - precision_m: 0.9501 - recall_m: 0.7410 - val_loss: 0.6899 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.5574 - f1_m: 0.6390 - precision_m: 0.9514 - recall_m: 0.5706 - val_loss: 0.6895 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.9509 - f1_m: 0.9748 - precision_m: 0.9513 - recall_m: 1.0000 - val_loss: 0.6891 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.9245 - f1_m: 0.9526 - precision_m: 0.9519 - recall_m: 0.9726 - val_loss: 0.6905 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.8810 - f1_m: 0.9179 - precision_m: 0.9509 - recall_m: 0.9252 - val_loss: 0.6903 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.4931 - f1_m: 0.5577 - precision_m: 0.9534 - recall_m: 0.4974 - val_loss: 0.6893 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.9509 - f1_m: 0.9739 - precision_m: 0.9496 - recall_m: 1.0000 - val_loss: 0.6894 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.1652 - f1_m: 0.2063 - precision_m: 0.9487 - recall_m: 0.1307 - val_loss: 0.6915 - val_acc: 0.2106 - val_f1_m: 0.2969 - val_precision_m: 0.9541 - val_recall_m: 0.1778\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.6270 - f1_m: 0.6831 - precision_m: 0.9527 - recall_m: 0.6458 - val_loss: 0.6902 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.7608 - f1_m: 0.8099 - precision_m: 0.9520 - recall_m: 0.7895 - val_loss: 0.6911 - val_acc: 0.2546 - val_f1_m: 0.3625 - val_precision_m: 0.9530 - val_recall_m: 0.2265\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.3469 - f1_m: 0.4441 - precision_m: 0.9592 - recall_m: 0.3306 - val_loss: 0.6903 - val_acc: 0.3034 - val_f1_m: 0.4272 - val_precision_m: 0.9581 - val_recall_m: 0.2782\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.9249 - f1_m: 0.9544 - precision_m: 0.9515 - recall_m: 0.9720 - val_loss: 0.6901 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.3935 - f1_m: 0.4781 - precision_m: 0.9558 - recall_m: 0.3833 - val_loss: 0.6912 - val_acc: 0.2467 - val_f1_m: 0.3512 - val_precision_m: 0.9533 - val_recall_m: 0.2178\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6931 - acc: 0.5976 - f1_m: 0.6635 - precision_m: 0.9530 - recall_m: 0.6062 - val_loss: 0.6910 - val_acc: 0.2463 - val_f1_m: 0.3506 - val_precision_m: 0.9533 - val_recall_m: 0.2174\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.9321 - f1_m: 0.9597 - precision_m: 0.9513 - recall_m: 0.9802 - val_loss: 0.6887 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.8700 - f1_m: 0.9063 - precision_m: 0.9502 - recall_m: 0.9108 - val_loss: 0.6904 - val_acc: 0.2524 - val_f1_m: 0.3596 - val_precision_m: 0.9527 - val_recall_m: 0.2242\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.3288 - f1_m: 0.4280 - precision_m: 0.9473 - recall_m: 0.3091 - val_loss: 0.6909 - val_acc: 0.2507 - val_f1_m: 0.3570 - val_precision_m: 0.9523 - val_recall_m: 0.2224\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by PC Agreement')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_agreement)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_agreement, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_agreement_test,y_test_deception), \n",
    "#                                callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4470111172558746, 0.4404074199778441, 0.44355477770485424, None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_full_throughput_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [ 337 1937]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rapport = np.asarray(y_test_rapport)\n",
    "y_train_rapport = np.asarray(y_train_rapport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting rapport, weighted by PC Agreement\n",
      "Model: \"functional_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6932 - acc: 0.8557 - f1_m: 0.9191 - precision_m: 0.8590 - recall_m: 0.9939 - val_loss: 0.6926 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 1s 3ms/step - loss: 0.6932 - acc: 0.3181 - f1_m: 0.2329 - precision_m: 0.2562 - recall_m: 0.2525 - val_loss: 0.6932 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6932 - acc: 0.8123 - f1_m: 0.8660 - precision_m: 0.8059 - recall_m: 0.9371 - val_loss: 0.6923 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8604 - f1_m: 0.9249 - precision_m: 0.8613 - recall_m: 1.0000 - val_loss: 0.6907 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6932 - acc: 0.6501 - f1_m: 0.6597 - precision_m: 0.6274 - recall_m: 0.7135 - val_loss: 0.6924 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8604 - f1_m: 0.9249 - precision_m: 0.8613 - recall_m: 1.0000 - val_loss: 0.6912 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.5961 - f1_m: 0.5938 - precision_m: 0.5587 - recall_m: 0.6435 - val_loss: 0.6909 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6693 - f1_m: 0.6887 - precision_m: 0.6389 - recall_m: 0.7483 - val_loss: 0.6908 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8604 - f1_m: 0.9240 - precision_m: 0.8597 - recall_m: 1.0000 - val_loss: 0.6923 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6419 - f1_m: 0.6452 - precision_m: 0.6061 - recall_m: 0.6994 - val_loss: 0.6932 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.7036 - f1_m: 0.7290 - precision_m: 0.6839 - recall_m: 0.7903 - val_loss: 0.6918 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6696 - f1_m: 0.6845 - precision_m: 0.6564 - recall_m: 0.7416 - val_loss: 0.6921 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.3996 - f1_m: 0.3407 - precision_m: 0.3353 - recall_m: 0.3710 - val_loss: 0.6940 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.7044 - f1_m: 0.7232 - precision_m: 0.6791 - recall_m: 0.7833 - val_loss: 0.6931 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8604 - f1_m: 0.9249 - precision_m: 0.8613 - recall_m: 1.0000 - val_loss: 0.6921 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6018 - f1_m: 0.5985 - precision_m: 0.5687 - recall_m: 0.6506 - val_loss: 0.6924 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.1598 - f1_m: 0.0267 - precision_m: 0.0518 - recall_m: 0.0285 - val_loss: 0.6940 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.1398 - f1_m: 7.8376e-04 - precision_m: 0.0210 - recall_m: 3.9936e-04 - val_loss: 0.6951 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6540 - f1_m: 0.6646 - precision_m: 0.6242 - recall_m: 0.7204 - val_loss: 0.6929 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.4954 - f1_m: 0.4615 - precision_m: 0.4458 - recall_m: 0.4991 - val_loss: 0.6933 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.1400 - f1_m: 0.0023 - precision_m: 0.0629 - recall_m: 0.0012 - val_loss: 0.6948 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.3104 - f1_m: 0.2267 - precision_m: 0.2244 - recall_m: 0.2450 - val_loss: 0.6923 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.7097 - f1_m: 0.7342 - precision_m: 0.6818 - recall_m: 0.7972 - val_loss: 0.6923 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.1527 - f1_m: 0.0197 - precision_m: 0.0448 - recall_m: 0.0215 - val_loss: 0.6951 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.1482 - f1_m: 0.0136 - precision_m: 0.0460 - recall_m: 0.0146 - val_loss: 0.6940 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.1400 - f1_m: 0.0012 - precision_m: 0.0350 - recall_m: 6.3203e-04 - val_loss: 0.6946 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.3175 - f1_m: 0.2327 - precision_m: 0.2298 - recall_m: 0.2520 - val_loss: 0.6918 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.7622 - f1_m: 0.7947 - precision_m: 0.7394 - recall_m: 0.8601 - val_loss: 0.6937 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.4713 - f1_m: 0.4262 - precision_m: 0.4094 - recall_m: 0.4618 - val_loss: 0.6955 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.1401 - f1_m: 0.0015 - precision_m: 0.0420 - recall_m: 7.7544e-04 - val_loss: 0.6943 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6932 - acc: 0.7253 - f1_m: 0.7552 - precision_m: 0.7086 - recall_m: 0.8183 - val_loss: 0.6925 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6298 - f1_m: 0.6320 - precision_m: 0.6001 - recall_m: 0.6856 - val_loss: 0.6935 - val_acc: 0.1288 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by PC Agreement')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_agreement)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_agreement, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_agreement_test,y_test_rapport), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4983476761275305, 0.4996649053379115, 0.13637846349528185, None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_throughput_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [2221   53]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Length only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model with one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting deception, weighted by text length\n",
      "Model: \"functional_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6931 - acc: 0.9415 - f1_m: 0.9677 - precision_m: 0.9519 - recall_m: 0.9892 - val_loss: 0.6889 - val_acc: 0.9384 - val_f1_m: 0.9678 - val_precision_m: 0.9500 - val_recall_m: 0.9868\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.9100 - f1_m: 0.9429 - precision_m: 0.9469 - recall_m: 0.9505 - val_loss: 0.6945 - val_acc: 0.2023 - val_f1_m: 0.2883 - val_precision_m: 0.9400 - val_recall_m: 0.1730\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6932 - acc: 0.0668 - f1_m: 0.0373 - precision_m: 0.6119 - recall_m: 0.0195 - val_loss: 0.6968 - val_acc: 0.0778 - val_f1_m: 0.0598 - val_precision_m: 0.8264 - val_recall_m: 0.0313\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6932 - acc: 0.0615 - f1_m: 0.0264 - precision_m: 0.5822 - recall_m: 0.0136 - val_loss: 0.6988 - val_acc: 0.0814 - val_f1_m: 0.0666 - val_precision_m: 0.8542 - val_recall_m: 0.0349\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6932 - acc: 0.2232 - f1_m: 0.2917 - precision_m: 0.9201 - recall_m: 0.1959 - val_loss: 0.6946 - val_acc: 0.2806 - val_f1_m: 0.3997 - val_precision_m: 0.9544 - val_recall_m: 0.2559\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6929 - acc: 0.6939 - f1_m: 0.8014 - precision_m: 0.9525 - recall_m: 0.7148 - val_loss: 0.6881 - val_acc: 0.7894 - val_f1_m: 0.8804 - val_precision_m: 0.9512 - val_recall_m: 0.8212\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6929 - acc: 0.6817 - f1_m: 0.7935 - precision_m: 0.9553 - recall_m: 0.6999 - val_loss: 0.6893 - val_acc: 0.7313 - val_f1_m: 0.8417 - val_precision_m: 0.9508 - val_recall_m: 0.7571\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6929 - acc: 0.8187 - f1_m: 0.8975 - precision_m: 0.9537 - recall_m: 0.8511 - val_loss: 0.6903 - val_acc: 0.6719 - val_f1_m: 0.7992 - val_precision_m: 0.9505 - val_recall_m: 0.6915\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6928 - acc: 0.5230 - f1_m: 0.6491 - precision_m: 0.9589 - recall_m: 0.5255 - val_loss: 0.6840 - val_acc: 0.8320 - val_f1_m: 0.9071 - val_precision_m: 0.9512 - val_recall_m: 0.8681\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6928 - acc: 0.7030 - f1_m: 0.8110 - precision_m: 0.9538 - recall_m: 0.7262 - val_loss: 0.6808 - val_acc: 0.8602 - val_f1_m: 0.9239 - val_precision_m: 0.9519 - val_recall_m: 0.8987\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6927 - acc: 0.8848 - f1_m: 0.9373 - precision_m: 0.9521 - recall_m: 0.9248 - val_loss: 0.6849 - val_acc: 0.7858 - val_f1_m: 0.8784 - val_precision_m: 0.9511 - val_recall_m: 0.8176\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6926 - acc: 0.7986 - f1_m: 0.8846 - precision_m: 0.9528 - recall_m: 0.8311 - val_loss: 0.6913 - val_acc: 0.6196 - val_f1_m: 0.7585 - val_precision_m: 0.9512 - val_recall_m: 0.6326\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6926 - acc: 0.7160 - f1_m: 0.8212 - precision_m: 0.9526 - recall_m: 0.7371 - val_loss: 0.6851 - val_acc: 0.7599 - val_f1_m: 0.8608 - val_precision_m: 0.9505 - val_recall_m: 0.7886\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6926 - acc: 0.3296 - f1_m: 0.4293 - precision_m: 0.9292 - recall_m: 0.3096 - val_loss: 0.6943 - val_acc: 0.5356 - val_f1_m: 0.6852 - val_precision_m: 0.9518 - val_recall_m: 0.5372\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6925 - acc: 0.6099 - f1_m: 0.7451 - precision_m: 0.9564 - recall_m: 0.6183 - val_loss: 0.6883 - val_acc: 0.6900 - val_f1_m: 0.8120 - val_precision_m: 0.9518 - val_recall_m: 0.7103\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6925 - acc: 0.8126 - f1_m: 0.8942 - precision_m: 0.9533 - recall_m: 0.8440 - val_loss: 0.6837 - val_acc: 0.7586 - val_f1_m: 0.8601 - val_precision_m: 0.9498 - val_recall_m: 0.7877\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6926 - acc: 0.6846 - f1_m: 0.8050 - precision_m: 0.9547 - recall_m: 0.7053 - val_loss: 0.6891 - val_acc: 0.6636 - val_f1_m: 0.7929 - val_precision_m: 0.9518 - val_recall_m: 0.6814\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6924 - acc: 0.6391 - f1_m: 0.7703 - precision_m: 0.9552 - recall_m: 0.6542 - val_loss: 0.6814 - val_acc: 0.7718 - val_f1_m: 0.8687 - val_precision_m: 0.9505 - val_recall_m: 0.8013\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6926 - acc: 0.6456 - f1_m: 0.7652 - precision_m: 0.9552 - recall_m: 0.6623 - val_loss: 0.6811 - val_acc: 0.7748 - val_f1_m: 0.8707 - val_precision_m: 0.9511 - val_recall_m: 0.8043\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6924 - acc: 0.7493 - f1_m: 0.8496 - precision_m: 0.9515 - recall_m: 0.7710 - val_loss: 0.6834 - val_acc: 0.7383 - val_f1_m: 0.8459 - val_precision_m: 0.9511 - val_recall_m: 0.7636\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6922 - acc: 0.5885 - f1_m: 0.7227 - precision_m: 0.9576 - recall_m: 0.5971 - val_loss: 0.6878 - val_acc: 0.6781 - val_f1_m: 0.8036 - val_precision_m: 0.9523 - val_recall_m: 0.6970\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6922 - acc: 0.6780 - f1_m: 0.7988 - precision_m: 0.9558 - recall_m: 0.6913 - val_loss: 0.6860 - val_acc: 0.6961 - val_f1_m: 0.8164 - val_precision_m: 0.9522 - val_recall_m: 0.7167\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6923 - acc: 0.6681 - f1_m: 0.7922 - precision_m: 0.9540 - recall_m: 0.6871 - val_loss: 0.6884 - val_acc: 0.6653 - val_f1_m: 0.7941 - val_precision_m: 0.9518 - val_recall_m: 0.6832\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6922 - acc: 0.6214 - f1_m: 0.7548 - precision_m: 0.9550 - recall_m: 0.6296 - val_loss: 0.6819 - val_acc: 0.7326 - val_f1_m: 0.8420 - val_precision_m: 0.9510 - val_recall_m: 0.7570\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6920 - acc: 0.7757 - f1_m: 0.8691 - precision_m: 0.9542 - recall_m: 0.8009 - val_loss: 0.6892 - val_acc: 0.6491 - val_f1_m: 0.7815 - val_precision_m: 0.9524 - val_recall_m: 0.6646\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6921 - acc: 0.6919 - f1_m: 0.8087 - precision_m: 0.9558 - recall_m: 0.7083 - val_loss: 0.6930 - val_acc: 0.5884 - val_f1_m: 0.7306 - val_precision_m: 0.9536 - val_recall_m: 0.5943\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6920 - acc: 0.6400 - f1_m: 0.7732 - precision_m: 0.9562 - recall_m: 0.6546 - val_loss: 0.6914 - val_acc: 0.6082 - val_f1_m: 0.7485 - val_precision_m: 0.9510 - val_recall_m: 0.6190\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6920 - acc: 0.6764 - f1_m: 0.7984 - precision_m: 0.9551 - recall_m: 0.6921 - val_loss: 0.6906 - val_acc: 0.6201 - val_f1_m: 0.7585 - val_precision_m: 0.9500 - val_recall_m: 0.6332\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6919 - acc: 0.7084 - f1_m: 0.8241 - precision_m: 0.9548 - recall_m: 0.7285 - val_loss: 0.6892 - val_acc: 0.6372 - val_f1_m: 0.7719 - val_precision_m: 0.9517 - val_recall_m: 0.6513\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6919 - acc: 0.6524 - f1_m: 0.7807 - precision_m: 0.9549 - recall_m: 0.6662 - val_loss: 0.6896 - val_acc: 0.6310 - val_f1_m: 0.7666 - val_precision_m: 0.9518 - val_recall_m: 0.6437\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6920 - acc: 0.6676 - f1_m: 0.7919 - precision_m: 0.9548 - recall_m: 0.6860 - val_loss: 0.6833 - val_acc: 0.6909 - val_f1_m: 0.8128 - val_precision_m: 0.9518 - val_recall_m: 0.7112\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6918 - acc: 0.5635 - f1_m: 0.7064 - precision_m: 0.9563 - recall_m: 0.5703 - val_loss: 0.6781 - val_acc: 0.7313 - val_f1_m: 0.8412 - val_precision_m: 0.9509 - val_recall_m: 0.7558\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by text length')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_textlength)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_textlength, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_textlength_test,y_test_deception), \n",
    "#                                callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5007924154521013, 0.5031221091581868, 0.4632621849343301, None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_full_textlength_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [ 555 1719]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting rapport, weighted by text length\n",
      "Model: \"functional_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6938 - acc: 0.2958 - f1_m: 0.3063 - precision_m: 0.5434 - recall_m: 0.2219 - val_loss: 0.6964 - val_acc: 0.4270 - val_f1_m: 0.5280 - val_precision_m: 0.9157 - val_recall_m: 0.3745\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6936 - acc: 0.4487 - f1_m: 0.5673 - precision_m: 0.8560 - recall_m: 0.4355 - val_loss: 0.6924 - val_acc: 0.6438 - val_f1_m: 0.7626 - val_precision_m: 0.8920 - val_recall_m: 0.6681\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6935 - acc: 0.5451 - f1_m: 0.6752 - precision_m: 0.8541 - recall_m: 0.5713 - val_loss: 0.6915 - val_acc: 0.6961 - val_f1_m: 0.8060 - val_precision_m: 0.8919 - val_recall_m: 0.7370\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6934 - acc: 0.6596 - f1_m: 0.7727 - precision_m: 0.8561 - recall_m: 0.7264 - val_loss: 0.6934 - val_acc: 0.5840 - val_f1_m: 0.7067 - val_precision_m: 0.8954 - val_recall_m: 0.5863\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6934 - acc: 0.6620 - f1_m: 0.7843 - precision_m: 0.8563 - recall_m: 0.7327 - val_loss: 0.6932 - val_acc: 0.6007 - val_f1_m: 0.7231 - val_precision_m: 0.8915 - val_recall_m: 0.6104\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.7359 - f1_m: 0.8436 - precision_m: 0.8587 - recall_m: 0.8341 - val_loss: 0.6918 - val_acc: 0.7287 - val_f1_m: 0.8322 - val_precision_m: 0.8896 - val_recall_m: 0.7833\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.7755 - f1_m: 0.8714 - precision_m: 0.8581 - recall_m: 0.8874 - val_loss: 0.6905 - val_acc: 0.8188 - val_f1_m: 0.8965 - val_precision_m: 0.8816 - val_recall_m: 0.9137\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.6895 - f1_m: 0.8061 - precision_m: 0.8580 - recall_m: 0.7707 - val_loss: 0.6916 - val_acc: 0.7832 - val_f1_m: 0.8726 - val_precision_m: 0.8863 - val_recall_m: 0.8608\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.5923 - f1_m: 0.6799 - precision_m: 0.7673 - recall_m: 0.6365 - val_loss: 0.6913 - val_acc: 0.8175 - val_f1_m: 0.8957 - val_precision_m: 0.8814 - val_recall_m: 0.9123\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.8339 - f1_m: 0.9091 - precision_m: 0.8603 - recall_m: 0.9658 - val_loss: 0.6909 - val_acc: 0.8448 - val_f1_m: 0.9138 - val_precision_m: 0.8758 - val_recall_m: 0.9570\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8425 - f1_m: 0.9119 - precision_m: 0.8595 - recall_m: 0.9732 - val_loss: 0.6910 - val_acc: 0.8558 - val_f1_m: 0.9206 - val_precision_m: 0.8731 - val_recall_m: 0.9754\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.7911 - f1_m: 0.8806 - precision_m: 0.8601 - recall_m: 0.9079 - val_loss: 0.6922 - val_acc: 0.8347 - val_f1_m: 0.9072 - val_precision_m: 0.8776 - val_recall_m: 0.9406\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8457 - f1_m: 0.9150 - precision_m: 0.8592 - recall_m: 0.9811 - val_loss: 0.6921 - val_acc: 0.8575 - val_f1_m: 0.9217 - val_precision_m: 0.8730 - val_recall_m: 0.9778\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6653 - f1_m: 0.7210 - precision_m: 0.7340 - recall_m: 0.7432 - val_loss: 0.6921 - val_acc: 0.8707 - val_f1_m: 0.9296 - val_precision_m: 0.8700 - val_recall_m: 0.9990\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8570 - f1_m: 0.9229 - precision_m: 0.8614 - recall_m: 0.9954 - val_loss: 0.6912 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8604 - f1_m: 0.9249 - precision_m: 0.8613 - recall_m: 1.0000 - val_loss: 0.6924 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8604 - f1_m: 0.9239 - precision_m: 0.8597 - recall_m: 1.0000 - val_loss: 0.6912 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8604 - f1_m: 0.9249 - precision_m: 0.8613 - recall_m: 1.0000 - val_loss: 0.6923 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8489 - f1_m: 0.9152 - precision_m: 0.8580 - recall_m: 0.9845 - val_loss: 0.6926 - val_acc: 0.6957 - val_f1_m: 0.8171 - val_precision_m: 0.8549 - val_recall_m: 0.7850\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.2184 - f1_m: 0.1731 - precision_m: 0.8413 - recall_m: 0.1067 - val_loss: 0.6954 - val_acc: 0.1557 - val_f1_m: 0.0871 - val_precision_m: 0.7511 - val_recall_m: 0.0470\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.2446 - f1_m: 0.2218 - precision_m: 0.8217 - recall_m: 0.1434 - val_loss: 0.6960 - val_acc: 0.1640 - val_f1_m: 0.1070 - val_precision_m: 0.7782 - val_recall_m: 0.0584\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.2234 - f1_m: 0.1893 - precision_m: 0.8418 - recall_m: 0.1142 - val_loss: 0.6942 - val_acc: 0.2801 - val_f1_m: 0.3564 - val_precision_m: 0.7997 - val_recall_m: 0.2310\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.5004 - f1_m: 0.6159 - precision_m: 0.8679 - recall_m: 0.5012 - val_loss: 0.6945 - val_acc: 0.2713 - val_f1_m: 0.3382 - val_precision_m: 0.8053 - val_recall_m: 0.2156\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.3303 - f1_m: 0.3882 - precision_m: 0.8785 - recall_m: 0.2544 - val_loss: 0.6950 - val_acc: 0.2731 - val_f1_m: 0.3407 - val_precision_m: 0.8050 - val_recall_m: 0.2175\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6929 - acc: 0.6729 - f1_m: 0.7692 - precision_m: 0.8648 - recall_m: 0.7384 - val_loss: 0.6941 - val_acc: 0.3039 - val_f1_m: 0.3936 - val_precision_m: 0.8150 - val_recall_m: 0.2614\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6928 - acc: 0.3787 - f1_m: 0.4727 - precision_m: 0.8773 - recall_m: 0.3282 - val_loss: 0.6920 - val_acc: 0.4380 - val_f1_m: 0.5783 - val_precision_m: 0.8339 - val_recall_m: 0.4460\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6927 - acc: 0.6301 - f1_m: 0.7444 - precision_m: 0.8683 - recall_m: 0.6751 - val_loss: 0.6915 - val_acc: 0.4459 - val_f1_m: 0.5879 - val_precision_m: 0.8334 - val_recall_m: 0.4580\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6925 - acc: 0.5216 - f1_m: 0.6286 - precision_m: 0.8655 - recall_m: 0.5229 - val_loss: 0.6891 - val_acc: 0.5158 - val_f1_m: 0.6622 - val_precision_m: 0.8382 - val_recall_m: 0.5509\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6924 - acc: 0.4128 - f1_m: 0.5115 - precision_m: 0.8815 - recall_m: 0.3693 - val_loss: 0.6966 - val_acc: 0.3039 - val_f1_m: 0.3911 - val_precision_m: 0.8161 - val_recall_m: 0.2592\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6922 - acc: 0.4608 - f1_m: 0.5733 - precision_m: 0.8762 - recall_m: 0.4344 - val_loss: 0.6948 - val_acc: 0.3465 - val_f1_m: 0.4572 - val_precision_m: 0.8259 - val_recall_m: 0.3189\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6920 - acc: 0.4825 - f1_m: 0.6022 - precision_m: 0.8798 - recall_m: 0.4629 - val_loss: 0.6932 - val_acc: 0.3799 - val_f1_m: 0.5030 - val_precision_m: 0.8320 - val_recall_m: 0.3641\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6918 - acc: 0.4143 - f1_m: 0.5187 - precision_m: 0.8851 - recall_m: 0.3707 - val_loss: 0.6922 - val_acc: 0.4059 - val_f1_m: 0.5384 - val_precision_m: 0.8305 - val_recall_m: 0.4023\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by text length')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_textlength)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_textlength, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_textlength_test,y_test_rapport), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.465365533505384, 0.42490261580578637, 0.35142348360019804, None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_textlength_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [1322  952]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other special options (either SP1, SP2, SP3, RAND_UNI, or RAND_NORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model with one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting deception, weighted by special option\n",
      "Model: \"functional_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6938 - acc: 0.8394 - f1_m: 0.8698 - precision_m: 0.9423 - recall_m: 0.8724 - val_loss: 0.6939 - val_acc: 0.1341 - val_f1_m: 0.1673 - val_precision_m: 0.9685 - val_recall_m: 0.0928\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.3021 - f1_m: 0.3398 - precision_m: 0.9520 - recall_m: 0.2854 - val_loss: 0.6917 - val_acc: 0.8703 - val_f1_m: 0.9299 - val_precision_m: 0.9516 - val_recall_m: 0.9102\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.7970 - f1_m: 0.8137 - precision_m: 0.9176 - recall_m: 0.8260 - val_loss: 0.6938 - val_acc: 0.0761 - val_f1_m: 0.0565 - val_precision_m: 0.8819 - val_recall_m: 0.0295\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.7207 - f1_m: 0.7457 - precision_m: 0.9205 - recall_m: 0.7424 - val_loss: 0.6928 - val_acc: 0.0783 - val_f1_m: 0.0608 - val_precision_m: 0.8819 - val_recall_m: 0.0318\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.5189 - f1_m: 0.5357 - precision_m: 0.9149 - recall_m: 0.5182 - val_loss: 0.6925 - val_acc: 0.0849 - val_f1_m: 0.0732 - val_precision_m: 0.8972 - val_recall_m: 0.0386\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.2875 - f1_m: 0.2968 - precision_m: 0.8948 - recall_m: 0.2694 - val_loss: 0.6895 - val_acc: 0.9507 - val_f1_m: 0.9745 - val_precision_m: 0.9506 - val_recall_m: 1.0000\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.2656 - f1_m: 0.2721 - precision_m: 0.8612 - recall_m: 0.2490 - val_loss: 0.6899 - val_acc: 0.9437 - val_f1_m: 0.9708 - val_precision_m: 0.9506 - val_recall_m: 0.9922\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.9441 - f1_m: 0.9712 - precision_m: 0.9514 - recall_m: 0.9925 - val_loss: 0.6897 - val_acc: 0.9380 - val_f1_m: 0.9678 - val_precision_m: 0.9508 - val_recall_m: 0.9859\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.8422 - f1_m: 0.8874 - precision_m: 0.9398 - recall_m: 0.8805 - val_loss: 0.6912 - val_acc: 0.8883 - val_f1_m: 0.9404 - val_precision_m: 0.9508 - val_recall_m: 0.9311\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.9440 - f1_m: 0.9711 - precision_m: 0.9518 - recall_m: 0.9919 - val_loss: 0.6908 - val_acc: 0.8945 - val_f1_m: 0.9438 - val_precision_m: 0.9511 - val_recall_m: 0.9375\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.9289 - f1_m: 0.9629 - precision_m: 0.9517 - recall_m: 0.9757 - val_loss: 0.6912 - val_acc: 0.8883 - val_f1_m: 0.9403 - val_precision_m: 0.9511 - val_recall_m: 0.9306\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.9393 - f1_m: 0.9675 - precision_m: 0.9518 - recall_m: 0.9851 - val_loss: 0.6924 - val_acc: 0.7995 - val_f1_m: 0.8868 - val_precision_m: 0.9507 - val_recall_m: 0.8321\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.3403 - f1_m: 0.3766 - precision_m: 0.8686 - recall_m: 0.3257 - val_loss: 0.6921 - val_acc: 0.8021 - val_f1_m: 0.8884 - val_precision_m: 0.9501 - val_recall_m: 0.8352\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6935 - acc: 0.9219 - f1_m: 0.9589 - precision_m: 0.9519 - recall_m: 0.9676 - val_loss: 0.6871 - val_acc: 0.9433 - val_f1_m: 0.9704 - val_precision_m: 0.9503 - val_recall_m: 0.9919\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6935 - acc: 0.3985 - f1_m: 0.3914 - precision_m: 0.7591 - recall_m: 0.3855 - val_loss: 0.6956 - val_acc: 0.0690 - val_f1_m: 0.0419 - val_precision_m: 0.6458 - val_recall_m: 0.0218\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.3813 - f1_m: 0.4145 - precision_m: 0.8253 - recall_m: 0.3759 - val_loss: 0.6920 - val_acc: 0.8259 - val_f1_m: 0.9031 - val_precision_m: 0.9503 - val_recall_m: 0.8614\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8573 - f1_m: 0.9190 - precision_m: 0.9510 - recall_m: 0.8958 - val_loss: 0.6923 - val_acc: 0.8017 - val_f1_m: 0.8883 - val_precision_m: 0.9509 - val_recall_m: 0.8345\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6764 - f1_m: 0.7597 - precision_m: 0.9467 - recall_m: 0.6989 - val_loss: 0.6918 - val_acc: 0.8650 - val_f1_m: 0.9268 - val_precision_m: 0.9513 - val_recall_m: 0.9044\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.5086 - f1_m: 0.5312 - precision_m: 0.8127 - recall_m: 0.5145 - val_loss: 0.6900 - val_acc: 0.9230 - val_f1_m: 0.9598 - val_precision_m: 0.9513 - val_recall_m: 0.9690\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.5605 - f1_m: 0.5781 - precision_m: 0.8215 - recall_m: 0.5735 - val_loss: 0.6909 - val_acc: 0.8958 - val_f1_m: 0.9447 - val_precision_m: 0.9500 - val_recall_m: 0.9404\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.9413 - f1_m: 0.9686 - precision_m: 0.9517 - recall_m: 0.9874 - val_loss: 0.6898 - val_acc: 0.9164 - val_f1_m: 0.9562 - val_precision_m: 0.9510 - val_recall_m: 0.9622\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.7585 - f1_m: 0.8189 - precision_m: 0.9545 - recall_m: 0.7895 - val_loss: 0.6900 - val_acc: 0.9204 - val_f1_m: 0.9582 - val_precision_m: 0.9503 - val_recall_m: 0.9668\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8529 - f1_m: 0.9192 - precision_m: 0.9519 - recall_m: 0.8917 - val_loss: 0.6903 - val_acc: 0.8975 - val_f1_m: 0.9457 - val_precision_m: 0.9500 - val_recall_m: 0.9422\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.8920 - f1_m: 0.9403 - precision_m: 0.9507 - recall_m: 0.9362 - val_loss: 0.6895 - val_acc: 0.9318 - val_f1_m: 0.9644 - val_precision_m: 0.9501 - val_recall_m: 0.9796\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6934 - acc: 0.9161 - f1_m: 0.9522 - precision_m: 0.9510 - recall_m: 0.9591 - val_loss: 0.6923 - val_acc: 0.8157 - val_f1_m: 0.8970 - val_precision_m: 0.9501 - val_recall_m: 0.8504\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6998 - f1_m: 0.7516 - precision_m: 0.9465 - recall_m: 0.7246 - val_loss: 0.6897 - val_acc: 0.9156 - val_f1_m: 0.9556 - val_precision_m: 0.9505 - val_recall_m: 0.9613\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.9414 - f1_m: 0.9697 - precision_m: 0.9512 - recall_m: 0.9896 - val_loss: 0.6906 - val_acc: 0.8646 - val_f1_m: 0.9267 - val_precision_m: 0.9508 - val_recall_m: 0.9047\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8004 - f1_m: 0.8858 - precision_m: 0.9521 - recall_m: 0.8343 - val_loss: 0.6898 - val_acc: 0.9134 - val_f1_m: 0.9545 - val_precision_m: 0.9508 - val_recall_m: 0.9590\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.6588 - f1_m: 0.7111 - precision_m: 0.8632 - recall_m: 0.6801 - val_loss: 0.6905 - val_acc: 0.9085 - val_f1_m: 0.9519 - val_precision_m: 0.9506 - val_recall_m: 0.9539\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8534 - f1_m: 0.9189 - precision_m: 0.9511 - recall_m: 0.8931 - val_loss: 0.6899 - val_acc: 0.9464 - val_f1_m: 0.9722 - val_precision_m: 0.9508 - val_recall_m: 0.9950\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.9485 - f1_m: 0.9726 - precision_m: 0.9495 - recall_m: 0.9975 - val_loss: 0.6900 - val_acc: 0.9081 - val_f1_m: 0.9517 - val_precision_m: 0.9505 - val_recall_m: 0.9535\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.8559 - f1_m: 0.9135 - precision_m: 0.9452 - recall_m: 0.8885 - val_loss: 0.6932 - val_acc: 0.0704 - val_f1_m: 0.0451 - val_precision_m: 0.6171 - val_recall_m: 0.0236\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by special option')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_special)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_special, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_special_test,y_test_deception), \n",
    "#                                callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4967717717717718, 0.4984017774547377, 0.0697554352874154, None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_full_special_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [2220   54]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rapport = np.asarray(y_test_rapport)\n",
    "y_train_rapport = np.asarray(y_train_rapport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint full model with one hot encoding, predicting rapport, weighted by special option\n",
      "Model: \"functional_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6934 - acc: 0.5258 - f1_m: 0.5416 - precision_m: 0.8522 - recall_m: 0.5419 - val_loss: 0.6890 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 2/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.6807 - f1_m: 0.7398 - precision_m: 0.8665 - recall_m: 0.7576 - val_loss: 0.6900 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 3/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.4899 - f1_m: 0.5496 - precision_m: 0.8743 - recall_m: 0.4875 - val_loss: 0.6912 - val_acc: 0.8272 - val_f1_m: 0.9036 - val_precision_m: 0.8669 - val_recall_m: 0.9446\n",
      "Epoch 4/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.8569 - f1_m: 0.9192 - precision_m: 0.8616 - recall_m: 0.9907 - val_loss: 0.6924 - val_acc: 0.4164 - val_f1_m: 0.5400 - val_precision_m: 0.8517 - val_recall_m: 0.3983\n",
      "Epoch 5/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.2738 - f1_m: 0.2791 - precision_m: 0.8710 - recall_m: 0.1890 - val_loss: 0.6916 - val_acc: 0.6007 - val_f1_m: 0.7361 - val_precision_m: 0.8632 - val_recall_m: 0.6441\n",
      "Epoch 6/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6932 - acc: 0.7226 - f1_m: 0.7851 - precision_m: 0.8560 - recall_m: 0.8020 - val_loss: 0.6945 - val_acc: 0.2713 - val_f1_m: 0.3202 - val_precision_m: 0.8428 - val_recall_m: 0.1992\n",
      "Epoch 7/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.6248 - f1_m: 0.6827 - precision_m: 0.8583 - recall_m: 0.6681 - val_loss: 0.6968 - val_acc: 0.1873 - val_f1_m: 0.1488 - val_precision_m: 0.8560 - val_recall_m: 0.0830\n",
      "Epoch 8/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.2822 - f1_m: 0.3056 - precision_m: 0.8746 - recall_m: 0.1929 - val_loss: 0.6926 - val_acc: 0.4011 - val_f1_m: 0.5174 - val_precision_m: 0.8532 - val_recall_m: 0.3738\n",
      "Epoch 9/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.4542 - f1_m: 0.5166 - precision_m: 0.8752 - recall_m: 0.4304 - val_loss: 0.6983 - val_acc: 0.1974 - val_f1_m: 0.1714 - val_precision_m: 0.8524 - val_recall_m: 0.0970\n",
      "Epoch 10/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.3178 - f1_m: 0.2916 - precision_m: 0.8144 - recall_m: 0.2477 - val_loss: 0.6914 - val_acc: 0.5422 - val_f1_m: 0.6806 - val_precision_m: 0.8598 - val_recall_m: 0.5656\n",
      "Epoch 11/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6931 - acc: 0.2585 - f1_m: 0.2547 - precision_m: 0.8781 - recall_m: 0.1637 - val_loss: 0.6914 - val_acc: 0.5493 - val_f1_m: 0.6881 - val_precision_m: 0.8588 - val_recall_m: 0.5766\n",
      "Epoch 12/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.4618 - f1_m: 0.5544 - precision_m: 0.8708 - recall_m: 0.4485 - val_loss: 0.6899 - val_acc: 0.8712 - val_f1_m: 0.9299 - val_precision_m: 0.8698 - val_recall_m: 1.0000\n",
      "Epoch 13/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.5332 - f1_m: 0.5892 - precision_m: 0.8685 - recall_m: 0.5444 - val_loss: 0.6967 - val_acc: 0.2379 - val_f1_m: 0.2614 - val_precision_m: 0.8298 - val_recall_m: 0.1570\n",
      "Epoch 14/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.5376 - f1_m: 0.6100 - precision_m: 0.8708 - recall_m: 0.5516 - val_loss: 0.6947 - val_acc: 0.2889 - val_f1_m: 0.3540 - val_precision_m: 0.8368 - val_recall_m: 0.2260\n",
      "Epoch 15/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.5577 - f1_m: 0.6418 - precision_m: 0.8636 - recall_m: 0.5780 - val_loss: 0.6959 - val_acc: 0.2612 - val_f1_m: 0.3051 - val_precision_m: 0.8323 - val_recall_m: 0.1882\n",
      "Epoch 16/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6929 - acc: 0.4673 - f1_m: 0.5280 - precision_m: 0.8746 - recall_m: 0.4510 - val_loss: 0.6966 - val_acc: 0.2511 - val_f1_m: 0.2861 - val_precision_m: 0.8328 - val_recall_m: 0.1744\n",
      "Epoch 17/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6929 - acc: 0.5832 - f1_m: 0.6891 - precision_m: 0.8731 - recall_m: 0.6089 - val_loss: 0.6952 - val_acc: 0.2797 - val_f1_m: 0.3383 - val_precision_m: 0.8348 - val_recall_m: 0.2136\n",
      "Epoch 18/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6928 - acc: 0.2904 - f1_m: 0.3251 - precision_m: 0.8674 - recall_m: 0.2050 - val_loss: 0.6945 - val_acc: 0.3091 - val_f1_m: 0.3874 - val_precision_m: 0.8427 - val_recall_m: 0.2534\n",
      "Epoch 19/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6929 - acc: 0.3844 - f1_m: 0.4715 - precision_m: 0.8760 - recall_m: 0.3357 - val_loss: 0.6943 - val_acc: 0.3171 - val_f1_m: 0.4004 - val_precision_m: 0.8398 - val_recall_m: 0.2648\n",
      "Epoch 20/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6928 - acc: 0.3611 - f1_m: 0.4357 - precision_m: 0.8782 - recall_m: 0.3020 - val_loss: 0.6907 - val_acc: 0.4670 - val_f1_m: 0.6010 - val_precision_m: 0.8533 - val_recall_m: 0.4666\n",
      "Epoch 21/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6928 - acc: 0.8127 - f1_m: 0.8884 - precision_m: 0.8619 - recall_m: 0.9298 - val_loss: 0.6934 - val_acc: 0.3443 - val_f1_m: 0.4403 - val_precision_m: 0.8496 - val_recall_m: 0.2991\n",
      "Epoch 22/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6927 - acc: 0.3719 - f1_m: 0.4398 - precision_m: 0.8793 - recall_m: 0.3156 - val_loss: 0.6923 - val_acc: 0.3852 - val_f1_m: 0.4964 - val_precision_m: 0.8520 - val_recall_m: 0.3527\n",
      "Epoch 23/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6928 - acc: 0.4374 - f1_m: 0.5252 - precision_m: 0.8698 - recall_m: 0.4132 - val_loss: 0.6938 - val_acc: 0.3355 - val_f1_m: 0.4277 - val_precision_m: 0.8470 - val_recall_m: 0.2882\n",
      "Epoch 24/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6926 - acc: 0.5114 - f1_m: 0.6294 - precision_m: 0.8746 - recall_m: 0.5065 - val_loss: 0.6946 - val_acc: 0.3188 - val_f1_m: 0.4024 - val_precision_m: 0.8426 - val_recall_m: 0.2662\n",
      "Epoch 25/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6927 - acc: 0.3769 - f1_m: 0.4548 - precision_m: 0.8728 - recall_m: 0.3217 - val_loss: 0.6952 - val_acc: 0.3061 - val_f1_m: 0.3815 - val_precision_m: 0.8411 - val_recall_m: 0.2482\n",
      "Epoch 26/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6926 - acc: 0.4029 - f1_m: 0.4996 - precision_m: 0.8745 - recall_m: 0.3591 - val_loss: 0.6954 - val_acc: 0.3039 - val_f1_m: 0.3779 - val_precision_m: 0.8412 - val_recall_m: 0.2453\n",
      "Epoch 27/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6926 - acc: 0.4369 - f1_m: 0.5082 - precision_m: 0.8760 - recall_m: 0.4067 - val_loss: 0.6955 - val_acc: 0.3034 - val_f1_m: 0.3773 - val_precision_m: 0.8410 - val_recall_m: 0.2448\n",
      "Epoch 28/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6926 - acc: 0.3760 - f1_m: 0.4575 - precision_m: 0.8846 - recall_m: 0.3201 - val_loss: 0.6912 - val_acc: 0.4063 - val_f1_m: 0.5260 - val_precision_m: 0.8535 - val_recall_m: 0.3828\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6925 - acc: 0.4017 - f1_m: 0.4864 - precision_m: 0.8764 - recall_m: 0.3563 - val_loss: 0.6904 - val_acc: 0.4288 - val_f1_m: 0.5552 - val_precision_m: 0.8522 - val_recall_m: 0.4147\n",
      "Epoch 30/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6925 - acc: 0.4357 - f1_m: 0.5373 - precision_m: 0.8722 - recall_m: 0.3984 - val_loss: 0.6955 - val_acc: 0.3122 - val_f1_m: 0.3924 - val_precision_m: 0.8405 - val_recall_m: 0.2577\n",
      "Epoch 31/32\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.6924 - acc: 0.3435 - f1_m: 0.4107 - precision_m: 0.8840 - recall_m: 0.2757 - val_loss: 0.6875 - val_acc: 0.5356 - val_f1_m: 0.6750 - val_precision_m: 0.8584 - val_recall_m: 0.5594\n",
      "Epoch 32/32\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.6924 - acc: 0.6368 - f1_m: 0.7270 - precision_m: 0.8687 - recall_m: 0.6755 - val_loss: 0.6948 - val_acc: 0.3303 - val_f1_m: 0.4190 - val_precision_m: 0.8484 - val_recall_m: 0.2802\n"
     ]
    }
   ],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by special option')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_special)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_special, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64,\n",
    "                               validation_data=(pred_df_special_test,y_test_rapport), \n",
    "#                                callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.48526163778457354, 0.47309767018760135, 0.312939827466322, None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_predict = joint_full_model.predict(pred_df_special_test)\n",
    "# joint_predict_round = []\n",
    "# for a in joint_predict:\n",
    "#     joint_predict_round.append(np.argmax(a))\n",
    "joint_predict_round = joint_predict.round()\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [1620  654]\n"
     ]
    }
   ],
   "source": [
    "uni, cnt = np.unique(joint_predict_round, return_counts=True)\n",
    "print(uni, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
