{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Embedding, Flatten, Conv1D, MaxPooling1D, Attention\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df = pd.read_csv('data/affcon_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Full data \n",
    "# full_df = pd.read_csv('data/full_diplomacy_data.csv')\n",
    "# full_df = full_df.rename(columns={\"text\": \"Input.full_text\", \"meta.deception_quadrant\": \"Input.deception_quadrant\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df = full_df[full_df['meta.speaker_intention'] == 'Truth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11366,)\n",
      "(11366, 1)\n",
      "(9092,)\n",
      "(9092, 1)\n",
      "(2274,)\n",
      "(2274, 1)\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.read_csv('data/kokil dec 6 reprepare/conf_pc_worker_sem.csv')\n",
    "full_df = full_df.dropna() # dataset contains NaN values, dropping NaNs here\n",
    "\n",
    "X = full_df['Input.full_text']\n",
    "\n",
    "# full_df[\"Input.deception_quadrant\"] = full_df[\"Input.deception_quadrant\"].apply(lambda x : 1 if x == \"Straightforward\" else 0)\n",
    "y = full_df['Input.deception_quadrant']\n",
    "#y = full_df['Answer.3rapport.yes_label']\n",
    "#y = full_df['Answer.4shareinformation.yes_label']\n",
    "# y = full_df['Answer.2reasoning.yes_label']\n",
    "#y = full_df['Answer.1gamemove.yes_label']\n",
    "\n",
    "le = LabelEncoder() # this can convert our categories into labels, make sure you don't have NaNs or Nulls in your data first\n",
    "y = le.fit_transform(y)\n",
    "print(y.shape)\n",
    "\n",
    "# we reshape \n",
    "y = y.reshape(-1,1) # the -1 allows it to have whatever number went in there\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 100\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "X_train = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "# X_train = sequence.pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 50)           50000     \n",
      "_________________________________________________________________\n",
      "LSTM_01 (LSTM)               (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Dense_01 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 87,889\n",
      "Trainable params: 87,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Inp = Input(name='inputs',shape=[max_len])\n",
    "x = Embedding(max_words,50,input_length=max_len)(Inp)\n",
    "x = LSTM(64,name='LSTM_01')(x)\n",
    "x = Dropout(0.5,name='Dropout')(x)\n",
    "x = Dense(128,activation='relu',name='Dense_01')(x)\n",
    "# x = Dropout(0.5,name='Dropout')(x)\n",
    "out = Dense(1,activation='sigmoid', name='output')(x)\n",
    "\n",
    "model = Model(inputs=Inp,outputs=out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "74/74 [==============================] - 2s 26ms/step - loss: 0.2102 - acc: 0.9474 - f1_m: 0.9705 - precision_m: 0.9566 - recall_m: 0.9905 - val_loss: 0.1742 - val_acc: 0.9585 - val_f1_m: 0.9791 - val_precision_m: 0.9592 - val_recall_m: 1.0000\n",
      "Epoch 2/15\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 0.1824 - acc: 0.9567 - f1_m: 0.9778 - precision_m: 0.9568 - recall_m: 1.0000 - val_loss: 0.1712 - val_acc: 0.9585 - val_f1_m: 0.9791 - val_precision_m: 0.9592 - val_recall_m: 1.0000\n",
      "Epoch 3/15\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 0.1743 - acc: 0.9567 - f1_m: 0.9778 - precision_m: 0.9567 - recall_m: 1.0000 - val_loss: 0.1766 - val_acc: 0.9585 - val_f1_m: 0.9791 - val_precision_m: 0.9592 - val_recall_m: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e7a6952908>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',min_delta=0.00001)\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "          batch_size=128,\n",
    "          epochs=15,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 1s 7ms/step - loss: 0.2039 - acc: 0.9486 - f1_m: 0.9734 - precision_m: 0.9489 - recall_m: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20389054715633392,\n",
       " 0.9485617876052856,\n",
       " 0.9733641147613525,\n",
       " 0.948924720287323,\n",
       " 1.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences_LSTM = tok.texts_to_sequences(X_test)\n",
    "X_test_LSTM = sequence.pad_sequences(test_sequences_LSTM,maxlen=max_len)\n",
    "model.evaluate(X_test_LSTM,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lynne\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4742808798646362, 0.5, 0.4868009725599166, None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.predict(X_test_LSTM).round()\n",
    "precision_recall_fscore_support(y_test, a, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_ATTENTION_VECTOR = False\n",
    "APPLY_ATTENTION_BEFORE_LSTM = False\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    # output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = multiply([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attention_applied_after_lstm():\n",
    "    #inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    lstm_units = 64\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(layer)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model([inputs], output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import merge\n",
    "from keras.layers import multiply\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 50)      50000       inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 100, 64)      29440       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 64, 100)      0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 64, 100)      0           permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64, 100)      10100       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 100, 64)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 100, 64)      0           lstm[0][0]                       \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 6400)         0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            6401        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 95,941\n",
      "Trainable params: 95,941\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = 50\n",
    "TIME_STEPS = max_len\n",
    "m = model_attention_applied_after_lstm()\n",
    "m.summary()\n",
    "m.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.3215 - acc: 0.9293 - f1_m: 0.9633 - precision_m: 0.9293 - recall_m: 1.0000 - val_loss: 0.2269 - val_acc: 0.9401 - val_f1_m: 0.9675 - val_precision_m: 0.9374 - val_recall_m: 1.0000\n",
      "Epoch 2/15\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.2561 - acc: 0.9293 - f1_m: 0.9632 - precision_m: 0.9293 - recall_m: 1.0000 - val_loss: 0.2278 - val_acc: 0.9401 - val_f1_m: 0.9675 - val_precision_m: 0.9374 - val_recall_m: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f10442ab710>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',min_delta=0.00001)\n",
    "\n",
    "m.fit(X_train,y_train,\n",
    "      batch_size=128,\n",
    "      epochs=15,\n",
    "      validation_split=0.2,\n",
    "      callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2591 - acc: 0.9279 - f1_m: 0.9624 - precision_m: 0.9288 - recall_m: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2591043710708618,\n",
       " 0.9278804063796997,\n",
       " 0.9624230861663818,\n",
       " 0.9288194179534912,\n",
       " 1.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences_LSTM = tok.texts_to_sequences(X_test)\n",
    "X_test_LSTM = sequence.pad_sequences(test_sequences_LSTM,maxlen=max_len)\n",
    "m.evaluate(X_test_LSTM,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiyuan/anaconda3/envs/TF2/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.46394019349164467, 0.5, 0.48129562043795615, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = m.predict(X_test_LSTM).round()\n",
    "precision_recall_fscore_support(y_test, a, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_with_embeddings\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 100, 50)           50000     \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 93, 32)            12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 46, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1472)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1472)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                14730     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 77,573\n",
      "Trainable params: 77,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_CNN = Sequential(name=\"CNN_with_embeddings\")\n",
    "model_CNN.add(Embedding(max_words, 50, input_length=max_len))\n",
    "model_CNN.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model_CNN.add(MaxPooling1D(pool_size=2))\n",
    "model_CNN.add(Flatten())\n",
    "model_CNN.add(Dropout(0.5))\n",
    "model_CNN.add(Dense(10, activation='relu'))\n",
    "model_CNN.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_CNN.compile(loss='binary_crossentropy', \n",
    "              optimizer= 'adam',\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "model_CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.2823 - acc: 0.9372 - f1_m: 0.9621 - precision_m: 0.9534 - recall_m: 0.9830 - val_loss: 0.1963 - val_acc: 0.9505 - val_f1_m: 0.9738 - val_precision_m: 0.9492 - val_recall_m: 1.0000\n",
      "Epoch 2/15\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 0.1926 - acc: 0.9524 - f1_m: 0.9755 - precision_m: 0.9523 - recall_m: 1.0000 - val_loss: 0.2001 - val_acc: 0.9505 - val_f1_m: 0.9738 - val_precision_m: 0.9492 - val_recall_m: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19087302988>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',min_delta=0.00001)\n",
    "\n",
    "model_CNN.fit(X_train,y_train,\n",
    "          batch_size=128,\n",
    "          epochs=15,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4295 - acc: 0.8465 - f1_m: 0.9166 - precision_m: 0.8485 - recall_m: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42947834730148315,\n",
       " 0.8465259671211243,\n",
       " 0.9165847301483154,\n",
       " 0.8485243320465088,\n",
       " 1.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences_CNN = tok.texts_to_sequences(X_test)\n",
    "X_test_CNN = sequence.pad_sequences(test_sequences_CNN,maxlen=max_len)\n",
    "\n",
    "model_CNN.evaluate(X_test_CNN,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lynne\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4731750219876869, 0.5, 0.48621780388612745, None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_cnn = model_CNN.predict(X_test_CNN).round()\n",
    "precision_recall_fscore_support(y_test, a_cnn, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
