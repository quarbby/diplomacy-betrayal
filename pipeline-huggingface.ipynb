{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "## Own code \n",
    "import import_ipynb\n",
    "import metadata_options\n",
    "import models_nn\n",
    "import models_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('data/kokil dec 6 reprepare/conf_pc_worker_sem.csv')\n",
    "full_df = full_df.dropna() # dataset contains NaN values, dropping NaNs here\n",
    "\n",
    "y_variables = ['Answer.1gamemove.yes_label', 'Answer.2reasoning.yes_label', \n",
    "               'Answer.3rapport.yes_label', 'Answer.4shareinformation.yes_label',\n",
    "               'Input.deception_quadrant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "WT1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "PC1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "TL1: weighted by 1 normalised number of characters per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "SP1: weighted by average of TP1 and TP2 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "## Metadata Options ##\n",
    "######################################\n",
    "\n",
    "throughput_option = 'TP1'\n",
    "worktime_option = 'WT1'\n",
    "pc_agreement_option = 'PC1'\n",
    "textlength_option = 'TL1'\n",
    "special_option = 'SP1'\n",
    "k_option_for_tp = 1\n",
    "\n",
    "df_throughput, df_worktime, df_agreement, df_textlength, df_special = metadata_options.set_OHE_pipeline_options(full_df, throughput_option, worktime_option, pc_agreement_option, textlength_option, special_option, k_option_for_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "## Model Options ##\n",
    "# From 0 to 3\n",
    "######################################\n",
    "model_option = 1\n",
    "\n",
    "pre_trained_model_selection = ['bert-base-uncased', 'distilbert-base-uncased', \n",
    "                               'albert-base-v2', 'roberta-base']\n",
    "\n",
    "models_huggingface.PRE_TRAINED_MODEL_NAME = pre_trained_model_selection[model_option]\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "full_df['Answer.1gamemove.yes_label'] = le.fit_transform(full_df['Answer.1gamemove.yes_label'])\n",
    "full_df['Answer.2reasoning.yes_label'] = le.fit_transform(full_df['Answer.2reasoning.yes_label'])\n",
    "full_df['Answer.3rapport.yes_label'] = le.fit_transform(full_df['Answer.3rapport.yes_label'])\n",
    "full_df['Answer.4shareinformation.yes_label'] = le.fit_transform(full_df['Answer.4shareinformation.yes_label'])\n",
    "full_df['Input.deception_quadrant'] = le.fit_transform(full_df['Input.deception_quadrant'])\n",
    "\n",
    "df_train, df_test = train_test_split(full_df, test_size=0.2)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.2)\n",
    "\n",
    "y_train_deception = df_train['Input.deception_quadrant'].tolist()\n",
    "y_test_deception = df_test['Input.deception_quadrant'].tolist()\n",
    "\n",
    "y_train_rapport = df_train['Answer.3rapport.yes_label'].tolist()\n",
    "y_test_rapport = df_test['Answer.3rapport.yes_label'].tolist()\n",
    "\n",
    "y_test_deception = np.asarray(y_test_deception)\n",
    "y_train_deception = np.asarray(y_train_deception)\n",
    "\n",
    "y_test_rapport = np.asarray(y_test_rapport)\n",
    "y_train_rapport = np.asarray(y_train_rapport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 100\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "tokenizer = models_huggingface.tokenizer\n",
    "model = models_huggingface.model\n",
    "optimizer = models_huggingface.optimizer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\.conda\\envs\\tfgpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2173: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer.1gamemove.yes_label\n",
      "sanity checks\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c8a7ce886627>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Sainty checks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sanity checks'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Lynnette\\diplomacy-betrayal\\models_huggingface.ipynb\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, item)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2434\u001b[0m         \u001b[1;31m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2435\u001b[1;33m         padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n\u001b[0m\u001b[0;32m   2436\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2437\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_get_padding_truncation_strategies\u001b[1;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2250\u001b[0m         \u001b[1;31m# Test if we have a padding token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2252\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   2253\u001b[0m                 \u001b[1;34m\"Asking to pad but the tokenizer does not have a padding token. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2254\u001b[0m                 \u001b[1;34m\"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "models_dict = {}\n",
    "models_train_pred_dict = {}\n",
    "models_test_pred_dict = {}\n",
    "\n",
    "for y_var in y_variables:\n",
    "    print()\n",
    "    print(y_var)\n",
    "    \n",
    "    # Create data splits\n",
    "    train_data_loader = models_huggingface.create_data_loader(df_train, tokenizer, y_var, MAX_LEN, BATCH_SIZE)\n",
    "    test_data_loader = models_huggingface.create_data_loader(df_test, tokenizer, y_var, MAX_LEN, BATCH_SIZE)\n",
    "    val_data_loader = models_huggingface.create_data_loader(df_val, tokenizer, y_var, MAX_LEN, BATCH_SIZE)\n",
    "    \n",
    "    # Sainty checks\n",
    "    print('sanity checks')\n",
    "    data = next(iter(train_data_loader))\n",
    "    print(data.keys())\n",
    "    \n",
    "    class_names = df_train[y_var].unique()\n",
    "    \n",
    "    # Create classifier model\n",
    "    new_model = models_huggingface.QuadrantClassifier(len(class_names))\n",
    "    new_model = new_model.to(device)\n",
    "    \n",
    "    input_ids = data['input_ids'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "    nn.functional.softmax(new_model(input_ids, attention_mask), dim=1)\n",
    "\n",
    "    # Compile model for number of epochs\n",
    "    total_steps = len(train_data_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps=0,\n",
    "      num_training_steps=total_steps\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "        print('-' * 10)\n",
    "        train_acc, train_loss = train_epoch(\n",
    "            new_model,\n",
    "            train_data_loader,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            device,\n",
    "            scheduler,\n",
    "            len(df_train)\n",
    "        )\n",
    "\n",
    "        print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "        val_acc, val_loss = eval_model(\n",
    "            new_model,\n",
    "            val_data_loader,\n",
    "            loss_fn,\n",
    "            device,\n",
    "            len(df_val)\n",
    "          )\n",
    "\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            torch.save(model.state_dict(), y_var + '.bin')\n",
    "            best_accuracy = val_acc\n",
    "    \n",
    "    # Get predictions\n",
    "    test_acc, _ = eval_model(\n",
    "      new_model,\n",
    "      test_data_loader,\n",
    "      loss_fn,\n",
    "      device,\n",
    "      len(df_test)\n",
    "    )\n",
    "        \n",
    "    y_full_texts, y_pred_train, y_pred_probs, y_test = models_huggingface.get_predictions(\n",
    "      new_model,\n",
    "      train_data_loader\n",
    "    )\n",
    "    y_pred_train = y_pred_train.numpy()\n",
    "    y_test = y_test.numpy()\n",
    "\n",
    "    # Classification report and confusion matrix\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    print(pd.DataFrame(report).transpose())\n",
    "    \n",
    "    y_full_texts, y_pred_test, y_pred_probs, y_test = models_huggingface.get_predictions(\n",
    "      new_model,\n",
    "      test_data_loader\n",
    "    )\n",
    "    y_pred_test = y_pred_test.numpy()\n",
    "    y_test = y_test.numpy()\n",
    "        \n",
    "    # Add to model\n",
    "    model_dict[y_var] = new_model\n",
    "    models_train_pred_dict[y_var] = y_pred_train\n",
    "    models_test_pred_dict[y_var] = y_pred_test\n",
    "\n",
    "    # Clear GPU memory\n",
    "    del input_ids\n",
    "    del attention_mask\n",
    "    del new_model\n",
    "    del loss_fn\n",
    "    \n",
    "    print(torch.cuda.empty_cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train encodings\n",
    "pred_df_arr_full = []\n",
    "pred_df_arr = []\n",
    "\n",
    "num_preds = len(models_train_pred_dict[y_variables[0]])\n",
    "\n",
    "for i in range(0, len(num_preds)):\n",
    "    pred_obj_1 = {}\n",
    "    pred_obj_1['gamemove'] = models_train_pred_dict['Answer.1gamemove.yes_label'][i]\n",
    "    pred_obj_1['reasoning'] = models_train_pred_dict['Answer.2reasoning.yes_label'][i]\n",
    "    pred_obj_1['shareinfo'] = models_train_pred_dict['Answer.4shareinformation.yes_label'][i]\n",
    "    pred_df_arr.append(pred_obj_1)\n",
    "    \n",
    "    pred_obj_2 = pred_obj_1.copy()\n",
    "    pred_obj_2['rapport'] = models_train_pred_dict['Answer.3rapport.yes_label'][i]\n",
    "    pred_df_arr_full.append(pred_obj_2)\n",
    "    \n",
    "pred_df_full = pd.DataFrame(pred_df_arr_full)\n",
    "pred_df = pd.DataFrame(pred_df_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train encodings\n",
    "pred_test_df_arr_full = []\n",
    "pred_test_df_arr = []\n",
    "\n",
    "num_preds = len(models_test_pred_dict[y_variables[0]])\n",
    "\n",
    "for i in range(0, len(num_preds)):\n",
    "    pred_obj_1 = {}\n",
    "    pred_obj_1['gamemove'] = models_test_pred_dict['Answer.1gamemove.yes_label'][i]\n",
    "    pred_obj_1['reasoning'] = models_test_pred_dict['Answer.2reasoning.yes_label'][i]\n",
    "    pred_obj_1['shareinfo'] = models_test_pred_dict['Answer.4shareinformation.yes_label'][i]\n",
    "    pred_test_df_arr.append(pred_obj_1)\n",
    "    \n",
    "    pred_obj_2 = pred_obj_1.copy()\n",
    "    pred_obj_2['rapport'] = models_test_pred_dict['Answer.3rapport.yes_label'][i]\n",
    "    pred_test_df_arr_full.append(pred_obj_2)\n",
    "    \n",
    "pred_test_df_full = pd.DataFrame(pred_test_df_arr_full)\n",
    "pred_test_df = pd.DataFrame(pred_test_df_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct metadata frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train weighted encodings\n",
    "pred_df_full_throughput, pred_df_throughput, pred_df_full_worktime, pred_df_worktime, pred_df_full_agreement, pred_df_agreement, pred_df_full_textlength, pred_df_textlength, pred_df_full_special, pred_df_special = metadata_options.construct_train_weighted_dataframe(indices_train, df_throughput, df_worktime, df_agreement, df_textlength, df_special, pred_df, pred_df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weighted encodings\n",
    "pred_df_full_throughput_test, pred_df_throughput_test, pred_df_full_worktime_test, pred_df_worktime_test, pred_df_full_agreement_test, pred_df_agreement_test, pred_df_full_textlength_test, pred_df_textlength_test, pred_df_full_special_test, pred_df_special_test = metadata_options.construct_train_weighted_dataframe(indices_test, df_throughput, df_worktime, df_agreement, df_textlength, df_special, pred_test_df, pred_test_df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the models with One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_test_df_full,y_test_deception), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=deception_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_test_df_full)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_test_df,y_test_rapport), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_test_df)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by throughput')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_throughput)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_throughput, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_throughput_test,y_test_deception), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=deception_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_full_throughput_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by throughput')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_throughput)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_throughput, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64,\n",
    "                               validation_data=(pred_df_throughput_test,y_test_rapport), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_throughput_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PC Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by PC Agreement')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_agreement)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_agreement, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_agreement_test,y_test_deception), \n",
    "                               callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_full_throughput_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by PC Agreement')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_agreement)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_agreement, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_agreement_test,y_test_rapport), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_throughput_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WorkTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by worktime')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_worktime)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_worktime, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_worktime_test,y_test_deception), \n",
    "                               callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_full_worktime_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by worktime')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_worktime)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_worktime, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64,\n",
    "                               validation_data=(pred_df_worktime_test,y_test_rapport), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_worktime_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by text length')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_textlength)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_textlength, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_textlength_test,y_test_deception), \n",
    "                               callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_full_textlength_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by text length')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_textlength)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_textlength, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_textlength_test,y_test_rapport), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_textlength_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by special option')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_special)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_special, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_special_test,y_test_deception), \n",
    "                               callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_full_special_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by special option')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_special)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_special, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64,\n",
    "                               validation_data=(pred_df_special_test,y_test_rapport), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_special_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
