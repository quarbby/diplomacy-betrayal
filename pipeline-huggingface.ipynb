{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from metadata_options.ipynb\n",
      "importing Jupyter notebook from models_nn.ipynb\n",
      "importing Jupyter notebook from models_huggingface.ipynb\n",
      "bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "## Own code \n",
    "import import_ipynb\n",
    "import metadata_options\n",
    "import models_nn\n",
    "import models_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('data/kokil dec 6 reprepare/conf_pc_worker_sem.csv')\n",
    "full_df = full_df.dropna() # dataset contains NaN values, dropping NaNs here\n",
    "\n",
    "y_variables = ['Answer.1gamemove.yes_label', 'Answer.2reasoning.yes_label', \n",
    "               'Answer.3rapport.yes_label', 'Answer.4shareinformation.yes_label',\n",
    "               'Input.deception_quadrant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "WT1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "PC1: weighted by 1 average per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "TL1: weighted by 1 normalised number of characters per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n",
      "SP1: weighted by average of TP1 and TP2 per set of OHE, i.e. (a, b, c, d) -> (w*a, w*b, w*c, w*d)\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "## Metadata Options ##\n",
    "######################################\n",
    "\n",
    "throughput_option = 'TP1'\n",
    "worktime_option = 'WT1'\n",
    "pc_agreement_option = 'PC1'\n",
    "textlength_option = 'TL1'\n",
    "special_option = 'SP1'\n",
    "k_option_for_tp = 1\n",
    "\n",
    "df_throughput, df_worktime, df_agreement, df_textlength, df_special = metadata_options.set_OHE_pipeline_options(full_df, throughput_option, worktime_option, pc_agreement_option, textlength_option, special_option, k_option_for_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "## Model Options ##\n",
    "# From 0 to 3\n",
    "######################################\n",
    "model_option = 1\n",
    "\n",
    "pre_trained_model_selection = ['bert-base-uncased', 'distilbert-base-uncased', \n",
    "                               'albert-base-v2', 'roberta-base']\n",
    "\n",
    "models_huggingface.PRE_TRAINED_MODEL_NAME = pre_trained_model_selection[model_option]\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "full_df['Answer.1gamemove.yes_label'] = le.fit_transform(full_df['Answer.1gamemove.yes_label'])\n",
    "full_df['Answer.2reasoning.yes_label'] = le.fit_transform(full_df['Answer.2reasoning.yes_label'])\n",
    "full_df['Answer.3rapport.yes_label'] = le.fit_transform(full_df['Answer.3rapport.yes_label'])\n",
    "full_df['Answer.4shareinformation.yes_label'] = le.fit_transform(full_df['Answer.4shareinformation.yes_label'])\n",
    "full_df['Input.deception_quadrant'] = le.fit_transform(full_df['Input.deception_quadrant'])\n",
    "\n",
    "df_train, df_test = train_test_split(full_df, test_size=0.2)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.2)\n",
    "\n",
    "y_train_deception = df_train['Input.deception_quadrant'].tolist()\n",
    "y_test_deception = df_test['Input.deception_quadrant'].tolist()\n",
    "\n",
    "y_train_rapport = df_train['Answer.3rapport.yes_label'].tolist()\n",
    "y_test_rapport = df_test['Answer.3rapport.yes_label'].tolist()\n",
    "\n",
    "y_test_deception = np.asarray(y_test_deception)\n",
    "y_train_deception = np.asarray(y_train_deception)\n",
    "\n",
    "y_test_rapport = np.asarray(y_test_rapport)\n",
    "y_train_rapport = np.asarray(y_train_rapport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 100\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "tokenizer = models_huggingface.tokenizer\n",
    "model = models_huggingface.model\n",
    "optimizer = models_huggingface.optimizer\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer.1gamemove.yes_label\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c8a7ce886627>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Create data splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain_data_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels_huggingface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_data_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtest_data_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels_huggingface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_data_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mval_data_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels_huggingface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_data_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "models_dict = {}\n",
    "models_train_pred_dict = {}\n",
    "models_test_pred_dict = {}\n",
    "\n",
    "for y_var in y_variables:\n",
    "    print()\n",
    "    print(y_var)\n",
    "    \n",
    "    # Create data splits\n",
    "    train_data_loader = models_huggingface.create_data_loader(df_train, tokenizer, y_var, MAX_LEN, BATCH_SIZE)\n",
    "    test_data_loader = models_huggingface.create_data_loader(df_test, tokenizer, y_var, MAX_LEN, BATCH_SIZE)\n",
    "    val_data_loader = models_huggingface.create_data_loader(df_val, tokenizer, y_var, MAX_LEN, BATCH_SIZE)\n",
    "    \n",
    "    # Sainty checks\n",
    "    print('sanity checks')\n",
    "    data = next(iter(train_data_loader))\n",
    "    print(data.keys())\n",
    "    \n",
    "    class_names = df_train[y_var].unique()\n",
    "    \n",
    "    # Create classifier model\n",
    "    new_model = models_huggingface.QuadrantClassifier(len(class_names))\n",
    "    new_model = new_model.to(device)\n",
    "    \n",
    "    input_ids = data['input_ids'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "    nn.functional.softmax(new_model(input_ids, attention_mask), dim=1)\n",
    "\n",
    "    # Compile model for number of epochs\n",
    "    total_steps = len(train_data_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps=0,\n",
    "      num_training_steps=total_steps\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "        print('-' * 10)\n",
    "        train_acc, train_loss = train_epoch(\n",
    "            new_model,\n",
    "            train_data_loader,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            device,\n",
    "            scheduler,\n",
    "            len(df_train)\n",
    "        )\n",
    "\n",
    "        print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "        val_acc, val_loss = eval_model(\n",
    "            new_model,\n",
    "            val_data_loader,\n",
    "            loss_fn,\n",
    "            device,\n",
    "            len(df_val)\n",
    "          )\n",
    "\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            torch.save(model.state_dict(), y_var + '.bin')\n",
    "            best_accuracy = val_acc\n",
    "    \n",
    "    # Get predictions\n",
    "    test_acc, _ = eval_model(\n",
    "      new_model,\n",
    "      test_data_loader,\n",
    "      loss_fn,\n",
    "      device,\n",
    "      len(df_test)\n",
    "    )\n",
    "        \n",
    "    y_full_texts, y_pred_train, y_pred_probs, y_test = models_huggingface.get_predictions(\n",
    "      new_model,\n",
    "      train_data_loader\n",
    "    )\n",
    "    y_pred_train = y_pred_train.numpy()\n",
    "    y_test = y_test.numpy()\n",
    "\n",
    "    # Classification report and confusion matrix\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    print(pd.DataFrame(report).transpose())\n",
    "    \n",
    "    y_full_texts, y_pred_test, y_pred_probs, y_test = models_huggingface.get_predictions(\n",
    "      new_model,\n",
    "      test_data_loader\n",
    "    )\n",
    "    y_pred_test = y_pred_test.numpy()\n",
    "    y_test = y_test.numpy()\n",
    "        \n",
    "    # Add to model\n",
    "    model_dict[y_var] = new_model\n",
    "    models_train_pred_dict[y_var] = y_pred_train\n",
    "    models_test_pred_dict[y_var] = y_pred_test\n",
    "\n",
    "    # Clear GPU memory\n",
    "    del input_ids\n",
    "    del attention_mask\n",
    "    del new_model\n",
    "    del loss_fn\n",
    "    \n",
    "    print(torch.cuda.empty_cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train encodings\n",
    "pred_df_arr_full = []\n",
    "pred_df_arr = []\n",
    "\n",
    "num_preds = len(models_train_pred_dict[y_variables[0]])\n",
    "\n",
    "for i in range(0, len(num_preds)):\n",
    "    pred_obj_1 = {}\n",
    "    pred_obj_1['gamemove'] = models_train_pred_dict['Answer.1gamemove.yes_label'][i]\n",
    "    pred_obj_1['reasoning'] = models_train_pred_dict['Answer.2reasoning.yes_label'][i]\n",
    "    pred_obj_1['shareinfo'] = models_train_pred_dict['Answer.4shareinformation.yes_label'][i]\n",
    "    pred_df_arr.append(pred_obj_1)\n",
    "    \n",
    "    pred_obj_2 = pred_obj_1.copy()\n",
    "    pred_obj_2['rapport'] = models_train_pred_dict['Answer.3rapport.yes_label'][i]\n",
    "    pred_df_arr_full.append(pred_obj_2)\n",
    "    \n",
    "pred_df_full = pd.DataFrame(pred_df_arr_full)\n",
    "pred_df = pd.DataFrame(pred_df_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train encodings\n",
    "pred_test_df_arr_full = []\n",
    "pred_test_df_arr = []\n",
    "\n",
    "num_preds = len(models_test_pred_dict[y_variables[0]])\n",
    "\n",
    "for i in range(0, len(num_preds)):\n",
    "    pred_obj_1 = {}\n",
    "    pred_obj_1['gamemove'] = models_test_pred_dict['Answer.1gamemove.yes_label'][i]\n",
    "    pred_obj_1['reasoning'] = models_test_pred_dict['Answer.2reasoning.yes_label'][i]\n",
    "    pred_obj_1['shareinfo'] = models_test_pred_dict['Answer.4shareinformation.yes_label'][i]\n",
    "    pred_test_df_arr.append(pred_obj_1)\n",
    "    \n",
    "    pred_obj_2 = pred_obj_1.copy()\n",
    "    pred_obj_2['rapport'] = models_test_pred_dict['Answer.3rapport.yes_label'][i]\n",
    "    pred_test_df_arr_full.append(pred_obj_2)\n",
    "    \n",
    "pred_test_df_full = pd.DataFrame(pred_test_df_arr_full)\n",
    "pred_test_df = pd.DataFrame(pred_test_df_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct metadata frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train weighted encodings\n",
    "pred_df_full_throughput, pred_df_throughput, pred_df_full_worktime, pred_df_worktime, pred_df_full_agreement, pred_df_agreement, pred_df_full_textlength, pred_df_textlength, pred_df_full_special, pred_df_special = metadata_options.construct_train_weighted_dataframe(indices_train, df_throughput, df_worktime, df_agreement, df_textlength, df_special, pred_df, pred_df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weighted encodings\n",
    "pred_df_full_throughput_test, pred_df_throughput_test, pred_df_full_worktime_test, pred_df_worktime_test, pred_df_full_agreement_test, pred_df_agreement_test, pred_df_full_textlength_test, pred_df_textlength_test, pred_df_full_special_test, pred_df_special_test = metadata_options.construct_train_weighted_dataframe(indices_test, df_throughput, df_worktime, df_agreement, df_textlength, df_special, pred_test_df, pred_test_df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the models with One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_test_df_full,y_test_deception), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=deception_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_test_df_full)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_test_df,y_test_rapport), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_test_df)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by throughput')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_throughput)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_throughput, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_throughput_test,y_test_deception), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=deception_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_full_throughput_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by throughput')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_throughput)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_throughput, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64,\n",
    "                               validation_data=(pred_df_throughput_test,y_test_rapport), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_throughput_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PC Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by PC Agreement')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_agreement)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_agreement, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_agreement_test,y_test_deception), \n",
    "                               callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_full_throughput_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by PC Agreement')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_agreement)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_agreement, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_agreement_test,y_test_rapport), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_throughput_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WorkTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by worktime')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_worktime)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_worktime, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_worktime_test,y_test_deception), \n",
    "                               callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_full_worktime_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by worktime')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_worktime)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_worktime, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64,\n",
    "                               validation_data=(pred_df_worktime_test,y_test_rapport), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_worktime_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by text length')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_textlength)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_textlength, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_textlength_test,y_test_deception), \n",
    "                               callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_full_textlength_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by text length')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_textlength)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_textlength, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_textlength_test,y_test_rapport), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_textlength_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting deception, weighted by special option')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_full_special)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_full_special, \n",
    "                               y=y_train_deception, \n",
    "                               epochs=32, \n",
    "                               batch_size=64, \n",
    "                               validation_data=(pred_df_full_special_test,y_test_deception), \n",
    "                               callbacks=[models_nn.callback],\n",
    "                               class_weight=deception_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_full_special_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_deception, np.array(joint_predict_round), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Joint full model with one hot encoding, predicting rapport, weighted by special option')\n",
    "joint_full_model = models_nn.create_joint_model(pred_df_special)\n",
    "joint_full_model.summary()\n",
    "history = joint_full_model.fit(x=pred_df_special, \n",
    "                               y=y_train_rapport, \n",
    "                               epochs=32, \n",
    "                               batch_size=64,\n",
    "                               validation_data=(pred_df_special_test,y_test_rapport), \n",
    "                               callbacks=[models_nn.callback], \n",
    "                               class_weight=rapport_class_weight_dict)\n",
    "\n",
    "joint_predict = joint_full_model.predict(pred_df_special_test)\n",
    "joint_predict_round = []\n",
    "for a in joint_predict:\n",
    "    joint_predict_round.append(np.argmax(a))\n",
    "precision_recall_fscore_support(y_test_rapport, np.array(joint_predict_round), average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
